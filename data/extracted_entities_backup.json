{
  "entities": [
    {
      "type": "Architecture",
      "name": "Transformer",
      "description": "A model architecture that has become the standard for natural language processing tasks."
    },
    {
      "type": "Technique",
      "name": "Attention Mechanism",
      "description": "A technique used in conjunction with convolutional networks or as a replacement for certain components."
    },
    {
      "type": "Model",
      "name": "Convolutional Neural Networks (CNNs)",
      "description": "A class of deep neural networks commonly used for processing structured grid data such as images."
    },
    {
      "type": "Concept",
      "name": "Image Patches",
      "description": "Segments of images that can be processed by the transformer model."
    },
    {
      "type": "Model",
      "name": "Vision Transformer (ViT)",
      "description": "A pure transformer model applied directly to sequences of image patches for image classification tasks."
    },
    {
      "type": "Dataset",
      "name": "ImageNet",
      "description": "A large dataset used for image recognition tasks."
    },
    {
      "type": "Dataset",
      "name": "CIFAR-100",
      "description": "A dataset containing 100 classes of images, used for image classification benchmarks."
    },
    {
      "type": "Dataset",
      "name": "VTAB",
      "description": "A collection of vision tasks used for evaluating image recognition models."
    },
    {
      "type": "Technique",
      "name": "Self-attention",
      "description": "A mechanism that allows models to weigh the importance of different parts of the input data."
    },
    {
      "type": "Metric",
      "name": "Computational Efficiency",
      "description": "A measure of the resources required to train a model."
    },
    {
      "type": "Architecture",
      "name": "CNN",
      "description": "Convolutional Neural Networks, a dominant architecture in computer vision."
    },
    {
      "type": "Architecture",
      "name": "ResNet",
      "description": "A type of CNN architecture that uses residual connections to improve training."
    },
    {
      "type": "Technique",
      "name": "Self-Attention",
      "description": "A mechanism that allows models to weigh the importance of different parts of the input data."
    },
    {
      "type": "Dataset",
      "name": "Large-scale image datasets",
      "description": "Datasets used for training models in image recognition tasks."
    },
    {
      "type": "Metric",
      "name": "Performance",
      "description": "A measure of how well a model performs on a given task."
    },
    {
      "type": "Dataset",
      "name": "Image Net",
      "description": "A large dataset used for image classification tasks."
    },
    {
      "type": "Architecture",
      "name": "Res Nets",
      "description": "A type of neural network architecture known for its deep residual learning framework."
    },
    {
      "type": "Technique",
      "name": "Image Patching",
      "description": "The process of splitting an image into smaller patches for input into the Transformer model."
    },
    {
      "type": "Metric",
      "name": "Accuracy",
      "description": "A measure of the model's performance, indicating the percentage of correct predictions."
    },
    {
      "type": "Model",
      "name": "Vision Transformer",
      "description": "A model that applies transformer architecture to image recognition tasks."
    },
    {
      "type": "Technique",
      "name": "Fine-tuning",
      "description": "A technique used to adapt a pre-trained model to a specific task."
    },
    {
      "type": "Dataset",
      "name": "ImageNet-21k",
      "description": "A public dataset used for training the Vision Transformer model."
    },
    {
      "type": "Dataset",
      "name": "JFT-300M",
      "description": "An in-house dataset used for training the Vision Transformer model."
    },
    {
      "type": "Benchmark",
      "name": "ImageNet-ReaL",
      "description": "A benchmark dataset where the best model achieved an accuracy of 90.72%."
    },
    {
      "type": "Technique",
      "name": "Large Scale Training",
      "description": "A training approach that improves model performance when trained on large datasets."
    },
    {
      "type": "Technique",
      "name": "Inductive Bias",
      "description": "A concept that refers to the assumptions made by a model that can affect its generalization."
    },
    {
      "type": "Model",
      "name": "BERT",
      "description": "A Transformer-based model that uses a denoising self-supervised pre-training task."
    },
    {
      "type": "Model",
      "name": "GPT",
      "description": "A line of work that uses language modeling as its pre-training task."
    },
    {
      "type": "Technique",
      "name": "self-attention",
      "description": "A mechanism that allows each element of a sequence to attend to every other element."
    },
    {
      "type": "Technique",
      "name": "local neighborhoods self-attention",
      "description": "An approximation of self-attention that only considers local neighborhoods for each query."
    },
    {
      "type": "Metric",
      "name": "77.63%",
      "description": "The performance metric achieved on the VTAB suite."
    },
    {
      "type": "Architecture",
      "name": "Sparse Transformers",
      "description": "Employ scalable approximations to global self-attention for image applications."
    },
    {
      "type": "Model",
      "name": "Cordonnier et al. (2020)",
      "description": "A model that extracts patches of size 2x2 for image processing."
    },
    {
      "type": "Technique",
      "name": "Multi-Head Dot-Product Self Attention",
      "description": "A variant of self-attention that processes multiple attention heads simultaneously."
    },
    {
      "type": "Technique",
      "name": "Local Multi-Head Attention",
      "description": "Applies self-attention only in local neighborhoods for each query pixel."
    },
    {
      "type": "Technique",
      "name": "Attention in Blocks",
      "description": "Applies attention in blocks of varying sizes to scale the attention mechanism."
    },
    {
      "type": "Model",
      "name": "ViT",
      "description": "A model that extracts patches from images and applies self-attention for image recognition."
    },
    {
      "type": "Technique",
      "name": "Image Classification",
      "description": "The task of identifying and categorizing objects within an image."
    },
    {
      "type": "Technique",
      "name": "Object Detection",
      "description": "The process of identifying and locating objects within an image."
    },
    {
      "type": "Model",
      "name": "Bello et al. (2019)",
      "description": "A model that combines CNNs with self-attention for image classification."
    },
    {
      "type": "Model",
      "name": "Hu et al. (2018)",
      "description": "A model that processes CNN outputs using self-attention for object detection."
    },
    {
      "type": "Model",
      "name": "Carion et al. (2020)",
      "description": "A model that utilizes self-attention in the context of object detection."
    },
    {
      "type": "Model",
      "name": "Wang et al. (2018)",
      "description": "A model that applies self-attention techniques in video processing."
    },
    {
      "type": "Model",
      "name": "Sun et al. (2019)",
      "description": "A model that utilizes self-attention for video processing tasks."
    },
    {
      "type": "Technique",
      "name": "Large Scale Pre-training",
      "description": "A training approach that enhances the performance of models by training on large datasets."
    },
    {
      "type": "Model",
      "name": "image GPT (i GPT)",
      "description": "A model that applies Transformers to image pixels after reducing image resolution and color space, trained in an unsupervised fashion as a generative model."
    },
    {
      "type": "Metric",
      "name": "accuracy",
      "description": "A measure of classification performance, with a reported maximal accuracy of 72% on Image Net for the image GPT model."
    },
    {
      "type": "Dataset",
      "name": "standard benchmarks",
      "description": "Benchmarks used to evaluate the performance of image recognition models."
    },
    {
      "type": "Concept",
      "name": "image recognition",
      "description": "The task of identifying and classifying objects within images."
    },
    {
      "type": "Concept",
      "name": "CNN transfer learning",
      "description": "A technique for transferring knowledge from one model trained on a large dataset to another model."
    },
    {
      "type": "Metric",
      "name": "performance",
      "description": "A measure of how well a model performs on standard benchmarks."
    },
    {
      "type": "Technique",
      "name": "Multi-Head Attention",
      "description": "A mechanism that allows the model to focus on different parts of the input sequence."
    },
    {
      "type": "Architecture",
      "name": "MLP",
      "description": "A multi-layer perceptron used in the model for processing features."
    },
    {
      "type": "Concept",
      "name": "Position Embedding",
      "description": "An embedding that provides information about the position of patches in the input."
    },
    {
      "type": "Concept",
      "name": "Patch",
      "description": "Segments of the input image that are processed by the model."
    },
    {
      "type": "Concept",
      "name": "Class Embedding",
      "description": "An additional learnable embedding that represents the class of the input."
    },
    {
      "type": "Technique",
      "name": "Position Embeddings",
      "description": "Additional embeddings added to input tokens to provide information about their position in the sequence."
    },
    {
      "type": "Technique",
      "name": "Classification Token",
      "description": "A learnable token added to the input sequence for the purpose of classifying the entire input."
    },
    {
      "type": "Architecture",
      "name": "NLP Transformer",
      "description": "Transformer architectures originally designed for natural language processing tasks."
    },
    {
      "type": "Paper",
      "name": "Attention is All You Need",
      "description": "A foundational paper by Vaswani et al. (2017) that introduced the Transformer architecture."
    },
    {
      "type": "Concept",
      "name": "Token Embeddings",
      "description": "A representation of input data as a sequence of vectors, used as input for the Transformer model."
    },
    {
      "type": "Technique",
      "name": "Flattening",
      "description": "The process of reshaping 2D images into a sequence of 1D patches for input into the Transformer."
    },
    {
      "type": "Concept",
      "name": "Patch Embeddings",
      "description": "The output of the linear projection applied to the flattened image patches, used as input to the Transformer."
    },
    {
      "type": "Metric",
      "name": "Latent Vector Size",
      "description": "A constant size D used throughout the layers of the Transformer."
    },
    {
      "type": "Technique",
      "name": "Multiheaded Self-Attention (MSA)",
      "description": "A technique used in the Transformer encoder that allows the model to focus on different parts of the input sequence."
    },
    {
      "type": "Architecture",
      "name": "Layernorm (LN)",
      "description": "A normalization technique applied before the attention and MLP blocks in the Transformer encoder."
    },
    {
      "type": "Concept",
      "name": "Image Representation",
      "description": "The output state of the Transformer encoder that serves as the representation of the input image."
    },
    {
      "type": "Architecture",
      "name": "Transformer encoder",
      "description": "A neural network architecture consisting of alternating layers of multiheaded self-attention and MLP blocks."
    },
    {
      "type": "Technique",
      "name": "MLP blocks",
      "description": "Multi-layer perceptron blocks used in the Transformer architecture."
    },
    {
      "type": "Technique",
      "name": "Residual connections",
      "description": "Connections added after every block to help with gradient flow during training."
    },
    {
      "type": "Technique",
      "name": "GELU",
      "description": "Gaussian Error Linear Unit, a non-linearity used in the MLP layers."
    },
    {
      "type": "Concept",
      "name": "Locality",
      "description": "The property of CNNs that allows them to focus on local features in images."
    },
    {
      "type": "Concept",
      "name": "Translation Equivariance",
      "description": "The property that allows a model to produce the same output when the input is translated."
    },
    {
      "type": "Architecture",
      "name": "Hybrid Architecture",
      "description": "An architecture that combines CNN feature maps with Transformer models for image processing."
    },
    {
      "type": "Dataset",
      "name": "Large datasets",
      "description": "Datasets used for pre-training the ViT model before fine-tuning."
    },
    {
      "type": "Metric",
      "name": "Classification Input Embedding",
      "description": "An embedding used as input for classification tasks in the model."
    },
    {
      "type": "Technique",
      "name": "Higher Resolution Training",
      "description": "Training the model with images of higher resolution than those used during pre-training."
    },
    {
      "type": "Concept",
      "name": "Patch Extraction",
      "description": "The method of dividing images into patches for processing by the Vision Transformer."
    },
    {
      "type": "Dataset",
      "name": "Large Datasets",
      "description": "Datasets used for pre-training the Vision Transformer model."
    },
    {
      "type": "Dataset",
      "name": "ILSVRC-2012 ImageNet",
      "description": "A large-scale dataset for image classification with 1,000 classes."
    },
    {
      "type": "Technique",
      "name": "Self-supervision",
      "description": "A learning paradigm where the model learns from unlabeled data by generating its own supervisory signals."
    },
    {
      "type": "Metric",
      "name": "State of the art",
      "description": "Refers to the highest level of performance achieved on recognition benchmarks."
    },
    {
      "type": "Dataset",
      "name": "JFT",
      "description": "A dataset containing 18,000 classes and 303 million high-resolution images."
    },
    {
      "type": "Dataset",
      "name": "CIFAR-10",
      "description": "A dataset with 10 classes and 60,000 images used for image classification."
    },
    {
      "type": "Dataset",
      "name": "Oxford-IIIT Pets",
      "description": "A dataset containing images of pets with annotations for 37 breeds."
    },
    {
      "type": "Dataset",
      "name": "Oxford Flowers-102",
      "description": "A dataset with 102 flower categories and 8,189 images."
    },
    {
      "type": "Technique",
      "name": "Self-supervised learning",
      "description": "A learning paradigm where the model learns from unlabeled data."
    },
    {
      "type": "Model",
      "name": "ViT-Base",
      "description": "A Vision Transformer model variant with 12 layers, hidden size of 768, and 86 million parameters."
    },
    {
      "type": "Model",
      "name": "ViT-Large",
      "description": "A Vision Transformer model variant with 24 layers, hidden size of 1024, and 307 million parameters."
    },
    {
      "type": "Model",
      "name": "ViT-Huge",
      "description": "A Vision Transformer model variant with 32 layers, hidden size of 1280, and 632 million parameters."
    },
    {
      "type": "Concept",
      "name": "Low-data transfer",
      "description": "The ability to transfer knowledge from a model trained on a small amount of data to various tasks."
    },
    {
      "type": "Technique",
      "name": "Geometric understanding",
      "description": "Tasks that require understanding of spatial relationships, such as localization."
    },
    {
      "type": "Model",
      "name": "ViT-L/16",
      "description": "The 'Large' variant of the Vision Transformer with a 16x16 input patch size."
    },
    {
      "type": "Technique",
      "name": "Group Normalization",
      "description": "A normalization technique that replaces Batch Normalization in the ResNet model."
    },
    {
      "type": "Technique",
      "name": "Standardized Convolutions",
      "description": "A modification applied to the convolutional layers in the ResNet model."
    },
    {
      "type": "Model",
      "name": "ResNet (BiT)",
      "description": "A modified version of ResNet that incorporates Group Normalization and standardized convolutions."
    },
    {
      "type": "Model",
      "name": "Res Net (Bi T)",
      "description": "A modified version of the ResNet model used in the experiments."
    },
    {
      "type": "Model",
      "name": "Vi T",
      "description": "Vision Transformer model that processes image patches."
    },
    {
      "type": "Technique",
      "name": "Adam",
      "description": "An optimization algorithm used for training models."
    },
    {
      "type": "Technique",
      "name": "SGD",
      "description": "Stochastic Gradient Descent, another optimization algorithm used for fine-tuning."
    },
    {
      "type": "Metric",
      "name": "Weight Decay",
      "description": "A regularization technique applied during training."
    },
    {
      "type": "Architecture",
      "name": "Res Net 50",
      "description": "A specific architecture of ResNet with 50 layers."
    },
    {
      "type": "Concept",
      "name": "Linear Learning Rate Warmup and Decay",
      "description": "A training strategy for adjusting the learning rate."
    },
    {
      "type": "Model",
      "name": "ViT-H/14",
      "description": "Another variant of the Vision Transformer model with a different configuration."
    },
    {
      "type": "Metric",
      "name": "Fine-tuning accuracy",
      "description": "Performance measure after fine-tuning a model on a dataset."
    },
    {
      "type": "Metric",
      "name": "Few-shot accuracy",
      "description": "Performance measure obtained by solving a regression problem with a subset of training images."
    },
    {
      "type": "Technique",
      "name": "Polyak & Juditsky averaging",
      "description": "A technique used for improving model performance through averaging."
    },
    {
      "type": "Model",
      "name": "Big Transfer (BiT)",
      "description": "A model that performs supervised transfer learning with large ResNets."
    },
    {
      "type": "Model",
      "name": "Noisy Student",
      "description": "A large EfficientNet trained using semi-supervised learning on ImageNet and JFT-300M."
    },
    {
      "type": "Metric",
      "name": "few-shot accuracy",
      "description": "A metric used to evaluate model performance with limited training examples."
    },
    {
      "type": "Model",
      "name": "Bi T-L",
      "description": "A model pre-trained on the same dataset as Vi T-L/16."
    },
    {
      "type": "Model",
      "name": "Vi T-L/16",
      "description": "A smaller model pre-trained on JFT-300 M that outperforms Bi T-L."
    },
    {
      "type": "Model",
      "name": "Vi T-H/14",
      "description": "A larger model that improves performance on challenging datasets."
    },
    {
      "type": "Dataset",
      "name": "VTAB suite",
      "description": "A suite of datasets used for evaluating image recognition models."
    },
    {
      "type": "Dataset",
      "name": "JFT-300 M",
      "description": "A dataset used for pre-training the Vi T-L/16 model."
    },
    {
      "type": "Hardware",
      "name": "TPUv3",
      "description": "Hardware used for training the models."
    },
    {
      "type": "Architecture",
      "name": "EfficientNet",
      "description": "A family of convolutional neural networks that optimize accuracy and efficiency."
    },
    {
      "type": "Dataset",
      "name": "Image Net-21 k",
      "description": "A smaller public dataset used for pre-training Vision Transformer models."
    },
    {
      "type": "Model",
      "name": "Bi T",
      "description": "A model co-trained on Image Net and Youtube, compared against Vi T models."
    },
    {
      "type": "Model",
      "name": "VIVI",
      "description": "A ResNet model co-trained on Image Net and Youtube."
    },
    {
      "type": "Model",
      "name": "S 4 L",
      "description": "A model that combines supervised and semi-supervised learning on Image Net."
    },
    {
      "type": "Dataset",
      "name": "Image Net-21k",
      "description": "A public dataset used for pre-training the Vi T-L/16 model."
    },
    {
      "type": "Metric",
      "name": "performance vs. compute",
      "description": "A measure used to evaluate the efficiency of different architectures."
    },
    {
      "type": "Technique",
      "name": "training schedule",
      "description": "A parameter that affects pre-training efficiency."
    },
    {
      "type": "Technique",
      "name": "optimizer",
      "description": "A parameter that affects pre-training efficiency."
    },
    {
      "type": "Technique",
      "name": "weight decay",
      "description": "A parameter that affects pre-training efficiency."
    },
    {
      "type": "Model",
      "name": "BiT-R152x4",
      "description": "A model that is compared against ViT-H/14 in terms of performance on various tasks."
    },
    {
      "type": "Technique",
      "name": "Regularization",
      "description": "Techniques such as weight decay, dropout, and label smoothing used to boost performance on smaller datasets."
    },
    {
      "type": "Metric",
      "name": "Top 1 Accuracy",
      "description": "A metric used to evaluate the performance of image classification models."
    },
    {
      "type": "Metric",
      "name": "Linear 5-shot Evaluation",
      "description": "A few-shot learning evaluation method on ImageNet."
    },
    {
      "type": "Model",
      "name": "Vi T-b",
      "description": "A variant of Vi T with all hidden dimensions halved."
    },
    {
      "type": "Model",
      "name": "Res Net",
      "description": "Residual Network model that is compared against Vision Transformers."
    },
    {
      "type": "Metric",
      "name": "Transfer accuracy",
      "description": "A metric used to evaluate the performance of models."
    },
    {
      "type": "Architecture",
      "name": "Hybrid",
      "description": "A model architecture that combines features of Transformers and Res Nets."
    },
    {
      "type": "Metric",
      "name": "validation accuracy",
      "description": "A measure of the model's performance on a validation dataset during training."
    },
    {
      "type": "Metric",
      "name": "few-shot linear accuracy",
      "description": "A performance metric that evaluates the model's accuracy with limited training examples."
    },
    {
      "type": "Architecture",
      "name": "ViT-B/32",
      "description": "A specific variant of the Vision Transformer model with a certain configuration."
    },
    {
      "type": "Architecture",
      "name": "ResNet 50",
      "description": "A specific variant of the ResNet model with 50 layers."
    },
    {
      "type": "Architecture",
      "name": "ResNet 152 x 2",
      "description": "A variant of the ResNet model with 152 layers and a specific configuration."
    },
    {
      "type": "Metric",
      "name": "few-shot results",
      "description": "Performance metrics that evaluate how well a model can learn from a limited number of examples."
    },
    {
      "type": "Concept",
      "name": "low-data transfer",
      "description": "The ability of a model to generalize and perform well on tasks with very few training examples."
    },
    {
      "type": "Metric",
      "name": "transfer performance",
      "description": "A measure of how well a model performs on a new task after being pre-trained on a different dataset."
    },
    {
      "type": "Architecture",
      "name": "Hybrid Model",
      "description": "A model that combines ResNet and Vision Transformer architectures."
    },
    {
      "type": "Metric",
      "name": "performance/compute trade-off",
      "description": "A measure comparing the performance of models relative to the computational resources they require."
    },
    {
      "type": "Dataset",
      "name": "5 datasets",
      "description": "A collection of datasets used to evaluate the performance of the models."
    },
    {
      "type": "Technique",
      "name": "hybrids",
      "description": "Hybrid models that combine different architectures to improve performance."
    },
    {
      "type": "Concept",
      "name": "Principal Components",
      "description": "The top components of the learned embedding filters that resemble basis functions for low-dimensional representation."
    },
    {
      "type": "Metric",
      "name": "Attention distance",
      "description": "A measure analogous to receptive field size in CNNs, indicating how far information is integrated in the image."
    },
    {
      "type": "Concept",
      "name": "Position embeddings",
      "description": "Embeddings that represent the 2D topology of images, showing spatial relationships between patches."
    },
    {
      "type": "Concept",
      "name": "Attention Distance",
      "description": "Analogous to receptive field size in CNNs, indicating how much of the image is attended to by the model."
    },
    {
      "type": "Technique",
      "name": "Self-Supervised Pre-Training",
      "description": "A method that enhances the performance of Transformers on NLP tasks through large scale training without labeled data."
    },
    {
      "type": "Metric",
      "name": "Attention Heads",
      "description": "Components of the Transformer that can exhibit varying attention distances in different layers."
    },
    {
      "type": "Model",
      "name": "ViT-L/32",
      "description": "Another variant of the Vision Transformer model with a different patch size."
    },
    {
      "type": "Technique",
      "name": "Cosine Similarity",
      "description": "A metric used to measure the similarity between two non-zero vectors."
    },
    {
      "type": "Metric",
      "name": "Mean Attention Distance",
      "description": "The average distance of attention across images for different heads in the model."
    },
    {
      "type": "Concept",
      "name": "RGB Embedding Filters",
      "description": "Filters used to embed RGB values in the initial layers of the model."
    },
    {
      "type": "Model",
      "name": "ViT-B/16",
      "description": "A smaller Vision Transformer model that achieves 79.9% accuracy on ImageNet."
    },
    {
      "type": "Technique",
      "name": "Masked Patch Prediction",
      "description": "A self-supervised learning technique mimicking masked language modeling used in BERT."
    },
    {
      "type": "Technique",
      "name": "Contrastive Pre-Training",
      "description": "A self-supervised learning technique that contrasts positive and negative samples."
    },
    {
      "type": "Dataset",
      "name": "large datasets",
      "description": "Datasets used for pre-training the Vision Transformer to improve its performance."
    },
    {
      "type": "Technique",
      "name": "self-supervised pre-training",
      "description": "A method of training models without labeled data that shows improvement in the performance of Vision Transformer."
    },
    {
      "type": "Task",
      "name": "image classification",
      "description": "A computer vision task where the model classifies images into predefined categories."
    },
    {
      "type": "Task",
      "name": "detection",
      "description": "A computer vision task that involves identifying objects within images."
    },
    {
      "type": "Task",
      "name": "segmentation",
      "description": "A computer vision task that involves partitioning an image into multiple segments or regions."
    },
    {
      "type": "Technique",
      "name": "Self-supervised pre-training",
      "description": "A method of training models using unlabeled data to improve performance."
    },
    {
      "type": "Technique",
      "name": "Large-scale supervised pre-training",
      "description": "Training models on large labeled datasets to enhance their performance."
    },
    {
      "type": "Location",
      "name": "Berlin",
      "description": "One of the locations where the research work was performed."
    },
    {
      "type": "Location",
      "name": "Z\u00fcrich",
      "description": "One of the locations where the research work was performed."
    },
    {
      "type": "Location",
      "name": "Amsterdam",
      "description": "One of the locations where the research work was performed."
    },
    {
      "type": "Person",
      "name": "Andreas Steiner",
      "description": "Provided crucial help with infrastructure and code release."
    },
    {
      "type": "Person",
      "name": "Joan Puigcerver",
      "description": "Assisted with large-scale training infrastructure."
    },
    {
      "type": "Person",
      "name": "Maxim Neumann",
      "description": "Assisted with large-scale training infrastructure."
    },
    {
      "type": "Person",
      "name": "Dmitry Lepikhin",
      "description": "Contributed to useful discussions."
    },
    {
      "type": "Person",
      "name": "Aravindh Mahendran",
      "description": "Contributed to useful discussions."
    },
    {
      "type": "Person",
      "name": "Daniel Keysers",
      "description": "Contributed to useful discussions."
    },
    {
      "type": "Person",
      "name": "Mario Lu\u010di\u0107",
      "description": "Contributed to useful discussions."
    },
    {
      "type": "Person",
      "name": "Noam Shazeer",
      "description": "Contributed to useful discussions."
    },
    {
      "type": "Person",
      "name": "Ashish Vaswani",
      "description": "Contributed to useful discussions."
    },
    {
      "type": "Person",
      "name": "Colin Raffel",
      "description": "Contributed to useful discussions."
    },
    {
      "type": "Metric",
      "name": "Mutual Information",
      "description": "A measure used to quantify the amount of information obtained about one random variable through another."
    },
    {
      "type": "Dataset",
      "name": "NeurIPS",
      "description": "A conference where various machine learning research papers are presented."
    },
    {
      "type": "Dataset",
      "name": "ACL",
      "description": "A conference focused on computational linguistics and natural language processing."
    },
    {
      "type": "Technique",
      "name": "Attention Augmented Convolutional Networks",
      "description": "A technique that combines convolutional networks with attention mechanisms to improve performance."
    },
    {
      "type": "Model",
      "name": "End-to-End Object Detection with Transformers",
      "description": "A model that applies transformer architectures for object detection tasks."
    },
    {
      "type": "Model",
      "name": "Generative Pretraining from Pixels",
      "description": "A model that focuses on unsupervised learning from image data using generative pretraining."
    },
    {
      "type": "Technique",
      "name": "Adaptive Input Representations",
      "description": "A technique for improving neural language modeling by adapting input representations."
    },
    {
      "type": "Metric",
      "name": "Few-Shot Learning",
      "description": "A metric that evaluates the ability of models to generalize from a small number of examples."
    },
    {
      "type": "Technique",
      "name": "Contrastive Learning",
      "description": "A framework for learning visual representations by contrasting positive and negative pairs."
    },
    {
      "type": "Model",
      "name": "UNITER",
      "description": "A model for universal image-text representation learning."
    },
    {
      "type": "Technique",
      "name": "Deep Residual Learning",
      "description": "A technique that allows for training very deep neural networks by using residual connections."
    },
    {
      "type": "Technique",
      "name": "Momentum Contrast",
      "description": "A technique for unsupervised visual representation learning that enhances contrastive learning."
    },
    {
      "type": "Model",
      "name": "Axial Attention",
      "description": "A method for applying attention mechanisms in multi-dimensional transformers."
    },
    {
      "type": "Dataset",
      "name": "Image Database",
      "description": "A collection of images used for training and evaluating image recognition models."
    },
    {
      "type": "Model",
      "name": "Convolutional Neural Networks",
      "description": "A class of deep neural networks commonly used for analyzing visual imagery."
    },
    {
      "type": "Metric",
      "name": "Robustness",
      "description": "The ability of a model to maintain performance despite variations in input."
    },
    {
      "type": "Metric",
      "name": "Transferability",
      "description": "The ability of a model to perform well on different but related tasks."
    },
    {
      "type": "Technique",
      "name": "Contrastive Predictive Coding",
      "description": "A technique for data-efficient image recognition."
    },
    {
      "type": "Model",
      "name": "Relation Networks",
      "description": "A model designed for object detection."
    },
    {
      "type": "Model",
      "name": "Local Relation Networks",
      "description": "A model for image recognition that focuses on local relations."
    },
    {
      "type": "Model",
      "name": "Ccnet",
      "description": "A model that employs criss-cross attention for semantic segmentation."
    },
    {
      "type": "Technique",
      "name": "Batch Normalization",
      "description": "A technique to accelerate deep network training by reducing internal covariate shift."
    },
    {
      "type": "Architecture",
      "name": "Deep Convolutional Neural Networks",
      "description": "A class of deep learning models that use convolutional layers for image processing."
    },
    {
      "type": "Technique",
      "name": "Conditional Computation",
      "description": "A technique used to scale models by activating only a subset of parameters during inference."
    },
    {
      "type": "Model",
      "name": "Gshard",
      "description": "A model that implements conditional computation and automatic sharding for scaling giant models."
    },
    {
      "type": "Model",
      "name": "Visual BERT",
      "description": "A model that integrates visual and textual information for vision-and-language tasks."
    },
    {
      "type": "Model",
      "name": "ViLBERT",
      "description": "A model that pretrains task-agnostic visiolinguistic representations for vision-and-language tasks."
    },
    {
      "type": "Technique",
      "name": "Object-centric Learning",
      "description": "A learning approach that focuses on understanding and representing objects in images."
    },
    {
      "type": "Model",
      "name": "Slot Attention",
      "description": "A model that facilitates object-centric learning by using attention mechanisms to focus on specific slots."
    },
    {
      "type": "Technique",
      "name": "Weakly Supervised Pretraining",
      "description": "A training approach that utilizes weak labels to improve model performance."
    },
    {
      "type": "Dataset",
      "name": "Cats and Dogs",
      "description": "A dataset used for image classification tasks, specifically distinguishing between cat and dog images."
    },
    {
      "type": "Technique",
      "name": "Weight Standardization",
      "description": "A technique aimed at improving the training of deep neural networks by standardizing the weights."
    },
    {
      "type": "Metric",
      "name": "Stochastic Approximation",
      "description": "A method used for optimization that involves iterative updates based on random samples."
    },
    {
      "type": "Architecture",
      "name": "Image Transformer",
      "description": "A variant of the Transformer model specifically designed for image processing tasks."
    },
    {
      "type": "Model",
      "name": "Language Models",
      "description": "Models designed to understand and generate human language."
    },
    {
      "type": "Technique",
      "name": "Unsupervised Learning",
      "description": "A type of machine learning that uses unlabeled data to learn patterns."
    },
    {
      "type": "Model",
      "name": "VideoBERT",
      "description": "A model for joint video and language representation learning."
    },
    {
      "type": "Metric",
      "name": "Data Effectiveness",
      "description": "The impact of data quality and quantity on the performance of deep learning models."
    },
    {
      "type": "Model",
      "name": "Axial-DeepLab",
      "description": "A model that employs axial attention for panoptic segmentation tasks."
    },
    {
      "type": "Conference",
      "name": "ICLR",
      "description": "International Conference on Learning Representations, where the paper was published."
    },
    {
      "type": "Conference",
      "name": "CVPR",
      "description": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, where related works were presented."
    },
    {
      "type": "Conference",
      "name": "NIPS",
      "description": "Neural Information Processing Systems, where foundational works on attention mechanisms were published."
    },
    {
      "type": "Model",
      "name": "Non-local Neural Networks",
      "description": "A model that captures long-range dependencies in images through non-local operations."
    },
    {
      "type": "Model",
      "name": "Visual Transformers",
      "description": "A model that utilizes token-based image representation and processing for computer vision tasks."
    },
    {
      "type": "Dataset",
      "name": "Panoptic Segmentation Dataset",
      "description": "A dataset used for training and evaluating panoptic segmentation models."
    },
    {
      "type": "Metric",
      "name": "Panoptic Quality",
      "description": "A metric used to evaluate the performance of panoptic segmentation models."
    },
    {
      "type": "Technique",
      "name": "Self-Supervised Learning",
      "description": "A learning paradigm where the model learns from unlabeled data by generating labels from the data itself."
    },
    {
      "type": "Technique",
      "name": "Visual Task Adaptation",
      "description": "A method for adapting visual representations to specific tasks."
    },
    {
      "type": "Model",
      "name": "ViT-B/{16,32}",
      "description": "Vision Transformer model with base architecture and input patch sizes of 16 and 32."
    },
    {
      "type": "Model",
      "name": "R50",
      "description": "ResNet model with 50 layers."
    },
    {
      "type": "Model",
      "name": "R101",
      "description": "ResNet model with 101 layers."
    },
    {
      "type": "Model",
      "name": "R152",
      "description": "ResNet model with 152 layers."
    },
    {
      "type": "Technique",
      "name": "Learning Rate Decay",
      "description": "A technique to adjust the learning rate during training."
    },
    {
      "type": "Technique",
      "name": "Gradient Clipping",
      "description": "A technique to prevent exploding gradients by capping them at a certain threshold."
    },
    {
      "type": "Technique",
      "name": "Multihead Self-Attention",
      "description": "An extension of self-attention that runs multiple self-attention operations in parallel."
    },
    {
      "type": "Architecture",
      "name": "Neural Architecture",
      "description": "A framework for building neural networks, which includes components like self-attention."
    },
    {
      "type": "Model",
      "name": "ViT (Vision Transformer)",
      "description": "A model architecture that applies transformer techniques to image recognition tasks."
    },
    {
      "type": "Technique",
      "name": "Dropout",
      "description": "A regularization technique used to prevent overfitting by randomly dropping units during training."
    },
    {
      "type": "Metric",
      "name": "SGD (Stochastic Gradient Descent)",
      "description": "An optimization algorithm used for training machine learning models."
    },
    {
      "type": "Architecture",
      "name": "MSA (Multi-Head Self-Attention)",
      "description": "A component of the transformer architecture that allows the model to focus on different parts of the input."
    },
    {
      "type": "Dataset",
      "name": "Pets",
      "description": "A dataset used for training and evaluation, consisting of images of pets."
    },
    {
      "type": "Dataset",
      "name": "Flowers",
      "description": "A dataset used for training and evaluation, consisting of images of flowers."
    },
    {
      "type": "Dataset",
      "name": "CIFAR",
      "description": "A dataset containing small images used for training and evaluation."
    },
    {
      "type": "Technique",
      "name": "Learning Rate Sweep",
      "description": "A technique used to find optimal learning rates for training models."
    },
    {
      "type": "Model",
      "name": "Hybrid Models",
      "description": "Models that combine different architectures or techniques for improved performance."
    },
    {
      "type": "Technique",
      "name": "Cosine Learning Rate Decay",
      "description": "A technique used to adjust the learning rate during training."
    },
    {
      "type": "Metric",
      "name": "Batch Size",
      "description": "The number of training examples utilized in one iteration."
    },
    {
      "type": "Technique",
      "name": "Self-Supervision",
      "description": "A learning paradigm where the model learns from unlabeled data."
    },
    {
      "type": "Metric",
      "name": "VTAB score",
      "description": "A performance metric used to evaluate the effectiveness of models on the VTAB dataset."
    },
    {
      "type": "Architecture",
      "name": "Linear Layer",
      "description": "A layer in neural networks that performs a linear transformation on the input."
    },
    {
      "type": "Metric",
      "name": "Mean Color",
      "description": "A prediction target representing the average color of corrupted patches."
    },
    {
      "type": "Metric",
      "name": "few-shot performance",
      "description": "A measure of how well the model performs with limited training examples."
    },
    {
      "type": "Model",
      "name": "masked patch prediction",
      "description": "A method for predicting missing parts of image patches."
    },
    {
      "type": "Technique",
      "name": "cosine learning rate decay",
      "description": "A technique for adjusting the learning rate during training."
    },
    {
      "type": "Technique",
      "name": "L2 regression",
      "description": "A regression technique used to minimize the squared differences between predicted and actual values."
    },
    {
      "type": "Concept",
      "name": "corruption rate",
      "description": "The percentage of data that is intentionally corrupted during training to improve robustness."
    },
    {
      "type": "Metric",
      "name": "Top-1 Accuracy",
      "description": "A metric used to evaluate the performance of classification models."
    },
    {
      "type": "Dataset",
      "name": "Oxford-IIIT-Pets",
      "description": "A dataset for image classification focused on pet breeds."
    },
    {
      "type": "Dataset",
      "name": "JFT 300 M",
      "description": "A large dataset containing 300 million images used for training deep learning models."
    },
    {
      "type": "Metric",
      "name": "Top 1 accuracy",
      "description": "A metric used to evaluate the performance of classification models, indicating the percentage of correct predictions."
    },
    {
      "type": "Model",
      "name": "ViT-H",
      "description": "Vision Transformer model with high performance metrics."
    },
    {
      "type": "Model",
      "name": "ResNet-50",
      "description": "A convolutional neural network architecture known for its deep residual learning framework."
    },
    {
      "type": "Model",
      "name": "ResNet-101",
      "description": "A deeper version of ResNet-50 with 101 layers."
    },
    {
      "type": "Model",
      "name": "ResNet-152",
      "description": "An even deeper version of ResNet with 152 layers."
    },
    {
      "type": "Metric",
      "name": "Transfer Accuracy",
      "description": "A measure of how well a model performs on a different dataset after training."
    },
    {
      "type": "Metric",
      "name": "FLOPs",
      "description": "Floating Point Operations per second, a measure of computational efficiency."
    },
    {
      "type": "Metric",
      "name": "transfer accuracy",
      "description": "A measure of how well a model performs on a new dataset after being trained on a different one."
    },
    {
      "type": "Metric",
      "name": "5-shot performance",
      "description": "A performance metric indicating the model's accuracy with only 5 examples."
    },
    {
      "type": "Metric",
      "name": "absolute numbers",
      "description": "Quantitative results reported in the context of model performance."
    },
    {
      "type": "Technique",
      "name": "Scaling",
      "description": "The process of adjusting the depth and width of the network to improve performance."
    },
    {
      "type": "Metric",
      "name": "Compute",
      "description": "A measure that may predict performance better than the number of parameters in the model."
    },
    {
      "type": "Architecture",
      "name": "DMLP",
      "description": "A specific architecture configuration with a depth of 2048 and a patch size of 32."
    },
    {
      "type": "Concept",
      "name": "Class Token",
      "description": "An additional token used to represent the image in the Transformer model."
    },
    {
      "type": "Concept",
      "name": "Patch Size",
      "description": "The size of the image patches used in the model, which affects the effective sequence length."
    },
    {
      "type": "Technique",
      "name": "Multi-layer Perceptron (MLP)",
      "description": "A type of neural network used to transform the output of the class token into class predictions."
    },
    {
      "type": "Technique",
      "name": "Global Average Pooling (GAP)",
      "description": "A pooling operation used to aggregate image-patch embeddings before classification."
    },
    {
      "type": "Metric",
      "name": "5-shot linear accuracy",
      "description": "A metric used to evaluate the performance of models on few-shot learning tasks."
    },
    {
      "type": "Technique",
      "name": "Positional Embedding",
      "description": "A method for encoding spatial information in the input data."
    },
    {
      "type": "Architecture",
      "name": "Class-Token",
      "description": "An architecture component used for classification tasks in the Vision Transformer."
    },
    {
      "type": "Technique",
      "name": "1-dimensional Positional Embedding",
      "description": "Considers inputs as a sequence of patches in raster order."
    },
    {
      "type": "Technique",
      "name": "2-dimensional Positional Embedding",
      "description": "Uses a grid of patches and learns two sets of embeddings for the X and Y axes."
    },
    {
      "type": "Technique",
      "name": "Relative Positional Embeddings",
      "description": "Encodes spatial information based on the relative distance between patches."
    },
    {
      "type": "Technique",
      "name": "Relative Attention",
      "description": "A method to encode spatial information by defining the relative distance between pairs of patches."
    },
    {
      "type": "Concept",
      "name": "Positional Embeddings",
      "description": "Embeddings used to encode the position of patches in the input data."
    },
    {
      "type": "Architecture",
      "name": "Transformer Encoder",
      "description": "A component of the transformer model that processes input data."
    },
    {
      "type": "Hyperparameter",
      "name": "Learning Rate (LR)",
      "description": "A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated."
    },
    {
      "type": "Hyperparameter",
      "name": "Weight Decay (WD)",
      "description": "A regularization technique used to prevent overfitting by penalizing large weights."
    },
    {
      "type": "Concept",
      "name": "Patch-Level Inputs",
      "description": "Inputs that are processed in patches rather than individual pixels, allowing for different spatial encoding."
    },
    {
      "type": "Metric",
      "name": "Mean attention distance",
      "description": "A metric computed by averaging the distance between the query pixel and all other pixels, weighted by the attention weight."
    },
    {
      "type": "Concept",
      "name": "Positional encoding",
      "description": "A technique used to provide information about the position of elements in the input data."
    },
    {
      "type": "Concept",
      "name": "Attention mechanism",
      "description": "A technique that allows the model to focus on different parts of the input data when making predictions."
    },
    {
      "type": "Concept",
      "name": "Empirical Computational Costs",
      "description": "Real-world performance metrics that consider hardware specifics like lane widths and cache sizes, rather than just theoretical predictions."
    },
    {
      "type": "Metric",
      "name": "inference speed",
      "description": "The speed at which a model can process images, measured in images per second per core."
    },
    {
      "type": "Metric",
      "name": "batch size",
      "description": "The number of images processed simultaneously by the model, larger sizes are better for scaling."
    },
    {
      "type": "Model",
      "name": "Axial Res Net",
      "description": "A model that replaces convolutions in Res Net 50 with axial self-attention, using row and column attention."
    },
    {
      "type": "Architecture",
      "name": "Axial Transformer blocks",
      "description": "A structure that incorporates row-self-attention and column-self-attention followed by MLPs."
    },
    {
      "type": "Model",
      "name": "Axial-Vi T-B/32",
      "description": "A variant of the Axial Vision Transformer model with a specific configuration."
    },
    {
      "type": "Model",
      "name": "Axial-Vi T-B/16",
      "description": "Another variant of the Axial Vision Transformer model with a different configuration."
    },
    {
      "type": "Metric",
      "name": "inference time",
      "description": "The time taken to make predictions using a trained model."
    },
    {
      "type": "Technique",
      "name": "column-self-attention",
      "description": "An attention mechanism used in the Axial models."
    },
    {
      "type": "Model",
      "name": "Axial ViT",
      "description": "A variant of Vision Transformer that uses axial attention mechanisms."
    },
    {
      "type": "Metric",
      "name": "top-1 accuracy",
      "description": "A metric used to evaluate the performance of classification models."
    },
    {
      "type": "Model",
      "name": "Axial Transformer",
      "description": "A model that replaces traditional self-attention with two Axial Transformer blocks, one for row and one for column self-attention."
    },
    {
      "type": "Dataset",
      "name": "Object Net",
      "description": "A benchmark dataset used to evaluate the performance of image recognition models."
    },
    {
      "type": "Metric",
      "name": "top-5 accuracy",
      "description": "A performance metric indicating the percentage of times the correct label is among the top 5 predictions."
    },
    {
      "type": "Technique",
      "name": "Attention Rollout",
      "description": "A method used to compute attention maps from the output token to the input space."
    },
    {
      "type": "Dataset",
      "name": "VTAB-1k",
      "description": "A collection of tasks used to evaluate the performance of vision models."
    },
    {
      "type": "Technique",
      "name": "Image Recognition",
      "description": "The task of identifying and classifying objects within images."
    },
    {
      "type": "Model",
      "name": "ViT-L/16 (I21k)",
      "description": "Vision Transformer model variant trained on ImageNet 21k dataset."
    },
    {
      "type": "Dataset",
      "name": "Caltech 101",
      "description": "A dataset used for image classification tasks."
    },
    {
      "type": "Dataset",
      "name": "Flowers 102",
      "description": "A dataset containing images of flowers for classification tasks."
    },
    {
      "type": "Metric",
      "name": "Mean",
      "description": "Average performance metric across various tasks."
    },
    {
      "type": "Model",
      "name": "Denoising Diffusion Probabilistic Models",
      "description": "A class of latent variable models inspired by nonequilibrium thermodynamics."
    },
    {
      "type": "Technique",
      "name": "Denoising Score Matching",
      "description": "A technique related to training diffusion probabilistic models."
    },
    {
      "type": "Technique",
      "name": "Langevin Dynamics",
      "description": "A method used in conjunction with denoising score matching."
    },
    {
      "type": "Dataset",
      "name": "CIFAR 10",
      "description": "An unconditional dataset used for evaluating image synthesis quality."
    },
    {
      "type": "Metric",
      "name": "Inception Score",
      "description": "A metric used to evaluate the quality of generated images."
    },
    {
      "type": "Metric",
      "name": "FID Score",
      "description": "Fr\u00e9chet Inception Distance, a metric for assessing the quality of generated images."
    },
    {
      "type": "Dataset",
      "name": "LSUN",
      "description": "A dataset used for evaluating sample quality in image synthesis."
    },
    {
      "type": "Model",
      "name": "Progressive GAN",
      "description": "A generative adversarial network model used for comparison in sample quality."
    },
    {
      "type": "Model",
      "name": "Generative Adversarial Networks (GANs)",
      "description": "A class of deep generative models that have been successful in generating realistic images."
    },
    {
      "type": "Model",
      "name": "Autoregressive Models",
      "description": "Models that generate data sequentially, predicting the next data point based on previous ones."
    },
    {
      "type": "Model",
      "name": "Flows",
      "description": "A type of generative model that transforms a simple distribution into a complex one."
    },
    {
      "type": "Model",
      "name": "Variational Autoencoders (VAEs)",
      "description": "A generative model that learns to encode data into a latent space and decode it back."
    },
    {
      "type": "Technique",
      "name": "Energy-based Modeling",
      "description": "A framework for modeling probability distributions using energy functions."
    },
    {
      "type": "Technique",
      "name": "Score Matching",
      "description": "A technique used to estimate the score function of a probability distribution."
    },
    {
      "type": "Dataset",
      "name": "Celeb A-HQ",
      "description": "A dataset used for generating high-quality celebrity images."
    },
    {
      "type": "Metric",
      "name": "Sample Quality",
      "description": "A measure of the realism and fidelity of generated samples."
    },
    {
      "type": "Concept",
      "name": "Diffusion Probabilistic Models",
      "description": "Models that utilize a diffusion process to generate samples from a learned distribution."
    },
    {
      "type": "Technique",
      "name": "Variational Inference",
      "description": "A method used to train the diffusion probabilistic models by approximating the posterior distribution."
    },
    {
      "type": "Model",
      "name": "Diffusion Model",
      "description": "A parameterized Markov chain that reverses a diffusion process to generate samples."
    },
    {
      "type": "Concept",
      "name": "Markov Chain",
      "description": "A stochastic process that undergoes transitions from one state to another on a state space."
    },
    {
      "type": "Concept",
      "name": "Gaussian Noise",
      "description": "A type of statistical noise with a probability density function equal to that of the normal distribution."
    },
    {
      "type": "Architecture",
      "name": "Neural Network",
      "description": "A computational model inspired by the way biological neural networks in the human brain process information."
    },
    {
      "type": "Model",
      "name": "Diffusion Models",
      "description": "Generative models that are straightforward to define and efficient to train, capable of generating high-quality samples."
    },
    {
      "type": "Technique",
      "name": "Annealed Langevin Dynamics",
      "description": "A sampling technique that is equivalent to a certain parameterization of diffusion models."
    },
    {
      "type": "Metric",
      "name": "Log Likelihood",
      "description": "A measure used to evaluate the quality of generative models, where diffusion models do not perform competitively compared to likelihood-based models."
    },
    {
      "type": "Technique",
      "name": "Annealed Importance Sampling",
      "description": "A technique used for estimating probabilities in energy-based models."
    },
    {
      "type": "Concept",
      "name": "Lossless Codelengths",
      "description": "The amount of information required to encode data without loss."
    },
    {
      "type": "Concept",
      "name": "Lossy Compression",
      "description": "A data encoding method that reduces a file by removing certain information."
    },
    {
      "type": "Technique",
      "name": "Progressive Decoding",
      "description": "A decoding method that reconstructs data incrementally."
    },
    {
      "type": "Concept",
      "name": "Latent Variable Models",
      "description": "Models that assume the existence of unobserved variables that influence observed data."
    },
    {
      "type": "Metric",
      "name": "Negative Log Likelihood",
      "description": "A measure used to evaluate the performance of probabilistic models."
    },
    {
      "type": "Distribution",
      "name": "Gaussian Distribution",
      "description": "A continuous probability distribution characterized by a bell-shaped curve, defined by its mean and variance."
    },
    {
      "type": "Process",
      "name": "Forward Process",
      "description": "The process that gradually adds Gaussian noise to the data."
    },
    {
      "type": "Process",
      "name": "Reverse Process",
      "description": "The process that reconstructs data from the noisy observations using learned transitions."
    },
    {
      "type": "Technique",
      "name": "Reparameterization",
      "description": "A method used to learn forward process variances in diffusion models."
    },
    {
      "type": "Model",
      "name": "Gaussian Conditionals",
      "description": "Statistical distributions used in the reverse process to ensure expressiveness."
    },
    {
      "type": "Metric",
      "name": "L",
      "description": "A loss function representing the expected negative log likelihood in the context of diffusion models."
    },
    {
      "type": "Equation",
      "name": "q(xt|x0)",
      "description": "The distribution used to sample xt at an arbitrary timestep t in closed form."
    },
    {
      "type": "Technique",
      "name": "Stochastic Gradient Descent",
      "description": "An optimization method used for training models by minimizing a loss function."
    },
    {
      "type": "Metric",
      "name": "KL Divergence",
      "description": "A measure of how one probability distribution diverges from a second expected probability distribution."
    },
    {
      "type": "Architecture",
      "name": "Denoising Autoencoders",
      "description": "A type of neural network used to learn efficient representations of data, typically for the purpose of denoising."
    },
    {
      "type": "Architecture",
      "name": "Model Architecture",
      "description": "The structure and design choices made for implementing the diffusion models."
    },
    {
      "type": "Metric",
      "name": "Weighted Variational Bound Objective",
      "description": "An objective derived from the connection between diffusion models and denoising score matching."
    },
    {
      "type": "Concept",
      "name": "Forward process",
      "description": "A process in diffusion models where variances are fixed to constants."
    },
    {
      "type": "Concept",
      "name": "Reverse process",
      "description": "A process in diffusion models that generates samples from the learned distribution."
    },
    {
      "type": "Metric",
      "name": "Entropy",
      "description": "A measure used to evaluate the uncertainty in the reverse process."
    },
    {
      "type": "Architecture",
      "name": "N(xt\u22121; \u00b5\u03b8(xt, t),\u03a3\u03b8(xt, t))",
      "description": "The Gaussian distribution used in the reverse process for sampling."
    },
    {
      "type": "Model",
      "name": "\u00b5\u03b8(xt, t)",
      "description": "A parameterization that predicts the mean of the denoised data at time t."
    },
    {
      "type": "Metric",
      "name": "Lt",
      "description": "A loss function used to evaluate the performance of the model."
    },
    {
      "type": "Technique",
      "name": "Forward Process Posterior",
      "description": "A method used to compute the posterior mean in the diffusion process."
    },
    {
      "type": "Variable",
      "name": "\u03c32t",
      "description": "Variance parameter in the model representing noise."
    },
    {
      "type": "Variable",
      "name": "\u03b1t",
      "description": "A parameter that controls the amount of noise in the diffusion process."
    },
    {
      "type": "Variable",
      "name": "\u03b2t",
      "description": "A parameter related to the noise schedule in the diffusion process."
    },
    {
      "type": "Model",
      "name": "\u03f5\u03b8",
      "description": "A function approximator intended to predict noise \u03f5 from the input data xt."
    },
    {
      "type": "Algorithm",
      "name": "Training Algorithm",
      "description": "An iterative process for training the model using gradient descent."
    },
    {
      "type": "Algorithm",
      "name": "Sampling Algorithm",
      "description": "A procedure for generating samples from the trained model."
    },
    {
      "type": "Distribution",
      "name": "N(0, I)",
      "description": "A standard normal distribution used in the sampling process."
    },
    {
      "type": "Variable",
      "name": "xt",
      "description": "The current state in the diffusion process."
    },
    {
      "type": "Variable",
      "name": "xt\u22121",
      "description": "The previous state in the diffusion process, computed from xt."
    },
    {
      "type": "Variable",
      "name": "\u03c3t",
      "description": "A variable representing noise scale at time t."
    },
    {
      "type": "Variable",
      "name": "\u00af\u03b1t",
      "description": "A variable representing the cumulative product of \u03b1t."
    },
    {
      "type": "Model",
      "name": "Reverse Process Mean Function Approximator",
      "description": "A model denoted as \u00b5\u03b8 that predicts the mean function in the reverse process."
    },
    {
      "type": "Concept",
      "name": "\u03f5-Prediction Parameterization",
      "description": "A parameterization that resembles Langevin dynamics and simplifies the diffusion process."
    },
    {
      "type": "Concept",
      "name": "Diffusion Process",
      "description": "A process that involves the gradual addition of noise to data and the subsequent removal of noise."
    },
    {
      "type": "Technique",
      "name": "\u03f5-prediction parameterization",
      "description": "A parameterization approach that resembles Langevin dynamics and simplifies the diffusion model's variational bound."
    },
    {
      "type": "Concept",
      "name": "Langevin dynamics",
      "description": "A stochastic process used in statistical mechanics that is related to the diffusion process."
    },
    {
      "type": "Concept",
      "name": "Denoising score matching",
      "description": "An objective function used in training generative models that focuses on estimating the score of the data distribution."
    },
    {
      "type": "Model",
      "name": "Neural network reverse process",
      "description": "A neural network architecture used to reverse the diffusion process in generating samples."
    },
    {
      "type": "Metric",
      "name": "Discrete log likelihoods",
      "description": "A measure used to evaluate the likelihood of discrete data given a model."
    },
    {
      "type": "Dataset",
      "name": "Image data",
      "description": "Data consisting of pixel values typically ranging from 0 to 255, used for training the model."
    },
    {
      "type": "Architecture",
      "name": "Gaussian N(x 0; \u00b5\u03b8(x 1,1),\u03c32 1 I)",
      "description": "A Gaussian distribution used in the reverse process decoder for generating samples."
    },
    {
      "type": "Concept",
      "name": "Variational Bound",
      "description": "A theoretical framework ensuring lossless codelength of discrete data."
    },
    {
      "type": "Architecture",
      "name": "Conditional Autoregressive Model",
      "description": "A model that predicts future data points based on past data points."
    },
    {
      "type": "Metric",
      "name": "Codelength",
      "description": "A measure of the length of the encoded data."
    },
    {
      "type": "Concept",
      "name": "Discrete Data",
      "description": "Data that can take on a finite number of values."
    },
    {
      "type": "Architecture",
      "name": "Decoder",
      "description": "A component of the model that reconstructs data from latent representations."
    },
    {
      "type": "Metric",
      "name": "IS",
      "description": "Inception Score, a metric used to evaluate the quality of generated images."
    },
    {
      "type": "Metric",
      "name": "FID",
      "description": "Fr\u00e9chet Inception Distance, a metric for assessing the quality of generated images by comparing feature distributions."
    },
    {
      "type": "Model",
      "name": "EBM",
      "description": "Energy-Based Model, a type of generative model that learns to assign low energy to data points."
    },
    {
      "type": "Model",
      "name": "JEM",
      "description": "Joint Energy Model, a generative model that combines energy-based modeling with joint distributions."
    },
    {
      "type": "Model",
      "name": "Big GAN",
      "description": "A large-scale generative adversarial network designed for high-quality image generation."
    },
    {
      "type": "Model",
      "name": "Style GAN 2 + ADA",
      "description": "An advanced version of Style GAN that incorporates Adaptive Discriminator Augmentation."
    },
    {
      "type": "Model",
      "name": "Gated Pixel CNN",
      "description": "A generative model that uses convolutional neural networks with gating mechanisms."
    },
    {
      "type": "Model",
      "name": "Sparse Transformer",
      "description": "A transformer model that utilizes sparsity for efficient processing."
    },
    {
      "type": "Model",
      "name": "Pixel IQN",
      "description": "A model that uses quantile regression for pixel-level predictions."
    },
    {
      "type": "Model",
      "name": "NCSNv 2",
      "description": "A type of noise-conditional score network for generative modeling."
    },
    {
      "type": "Model",
      "name": "NCSN",
      "description": "Noise Conditional Score Network, a generative model that learns to denoise data."
    },
    {
      "type": "Model",
      "name": "SNGAN",
      "description": "Spectral Normalization GAN, a type of GAN that uses spectral normalization for stability."
    },
    {
      "type": "Model",
      "name": "SNGAN-DDLS",
      "description": "A variant of SNGAN that incorporates additional techniques for improved performance."
    },
    {
      "type": "Loss Function",
      "name": "Lsimple",
      "description": "A simplified loss function used for training the denoising model."
    },
    {
      "type": "Concept",
      "name": "Weighted Variational Bound",
      "description": "An objective that emphasizes different aspects of reconstruction compared to the standard variational bound."
    },
    {
      "type": "Concept",
      "name": "Denoising",
      "description": "The process of removing noise from data."
    },
    {
      "type": "Parameter",
      "name": "T",
      "description": "The number of steps in the diffusion process, set to 1000 for experiments."
    },
    {
      "type": "Model",
      "name": "U-Net",
      "description": "A convolutional neural network architecture used for image segmentation, adapted here for the reverse process in diffusion models."
    },
    {
      "type": "Model",
      "name": "Pixel CNN++",
      "description": "An autoregressive model for image generation that the U-Net backbone is similar to."
    },
    {
      "type": "Metric",
      "name": "signal-to-noise ratio",
      "description": "A measure used to ensure the quality of the generated samples by controlling noise levels."
    },
    {
      "type": "Metric",
      "name": "DKL",
      "description": "Kullback-Leibler divergence, a measure of how one probability distribution diverges from a second expected probability distribution."
    },
    {
      "type": "Metric",
      "name": "FID score",
      "description": "A metric used to evaluate the quality of generated samples."
    },
    {
      "type": "Metric",
      "name": "Inception score",
      "description": "A metric used to assess the quality of generated images."
    },
    {
      "type": "Algorithm",
      "name": "Sending",
      "description": "An algorithm for sending data through the diffusion process."
    },
    {
      "type": "Algorithm",
      "name": "Receiving",
      "description": "An algorithm for receiving data through the diffusion process."
    },
    {
      "type": "Metric",
      "name": "Mean Squared Error",
      "description": "A common loss function used for regression tasks."
    },
    {
      "type": "Model",
      "name": "CIFAR 10 models",
      "description": "Models trained on the CIFAR 10 dataset to evaluate sample quality."
    },
    {
      "type": "Concept",
      "name": "Reverse Process Parameterizations",
      "description": "Techniques used to parameterize the reverse diffusion process in generative modeling."
    },
    {
      "type": "Concept",
      "name": "Fixed Variances",
      "description": "A training approach where variances are kept constant during the training of the model."
    },
    {
      "type": "Concept",
      "name": "Learning Reverse Process Variances",
      "description": "An approach that incorporates parameterized variances into the training process."
    },
    {
      "type": "Concept",
      "name": "Predicting \u03f5",
      "description": "A proposed method for improving sample quality in diffusion models."
    },
    {
      "type": "Concept",
      "name": "Codelengths",
      "description": "A measure of the efficiency of the model in terms of bits per dimension."
    },
    {
      "type": "Model",
      "name": "Denoising Diffusion Probabilistic Model",
      "description": "A generative model that uses diffusion processes for sampling."
    },
    {
      "type": "Metric",
      "name": "Rate",
      "description": "The number of bits per dimension, indicating the efficiency of the model."
    },
    {
      "type": "Metric",
      "name": "Distortion",
      "description": "A measure of the difference between the original and reconstructed data."
    },
    {
      "type": "Metric",
      "name": "Root Mean Squared Error (RMSE)",
      "description": "A measure of the differences between values predicted by a model and the values observed."
    },
    {
      "type": "Model",
      "name": "Energy Based Models",
      "description": "A class of probabilistic models that define a probability distribution through an energy function."
    },
    {
      "type": "Model",
      "name": "Likelihood-based Generative Models",
      "description": "Models that generate data by maximizing the likelihood of the observed data."
    },
    {
      "type": "Metric",
      "name": "root mean squared error",
      "description": "A measure of the differences between values predicted by a model and the values observed."
    },
    {
      "type": "Technique",
      "name": "progressive lossy compression",
      "description": "A method of compressing data that allows for progressive refinement of the quality of the output."
    },
    {
      "type": "Algorithm",
      "name": "Algorithm 3",
      "description": "An algorithm that transmits samples in sequence using a specific expected codelength."
    },
    {
      "type": "Algorithm",
      "name": "Algorithm 4",
      "description": "An algorithm that works in conjunction with Algorithm 3 for transmitting samples."
    },
    {
      "type": "Concept",
      "name": "rate-distortion behavior",
      "description": "The relationship between the rate of data compression and the distortion of the reconstructed data."
    },
    {
      "type": "Concept",
      "name": "minimal random coding",
      "description": "A procedure that can transmit samples using a specific number of bits on average."
    },
    {
      "type": "Metric",
      "name": "Rate-Distortion",
      "description": "A plot that shows the trade-off between the rate of data transmission and the distortion of the reconstructed data."
    },
    {
      "type": "Metric",
      "name": "RMSE",
      "description": "Root Mean Squared Error, a metric used to measure distortion in the generated images."
    },
    {
      "type": "Technique",
      "name": "Progressive Generation",
      "description": "A method of generating images progressively from random bits, predicting results over the reverse process."
    },
    {
      "type": "Metric",
      "name": "Sample Quality Metrics",
      "description": "Metrics used to evaluate the quality of generated samples in generative models."
    },
    {
      "type": "Model",
      "name": "p\u03b8(xt\u22121|xt)",
      "description": "The learned model that predicts the previous state in the diffusion process given the current state."
    },
    {
      "type": "Concept",
      "name": "Blank Image",
      "description": "The target state of the diffusion process where all probability mass is placed at the final step."
    },
    {
      "type": "Concept",
      "name": "Gaussian diffusion model",
      "description": "A type of diffusion model that adds Gaussian noise to images, interpreted as an autoregressive model."
    },
    {
      "type": "Technique",
      "name": "Autoregressive model",
      "description": "A model that predicts future values based on past values, used in the context of training p\u03b8."
    },
    {
      "type": "Concept",
      "name": "Gaussian diffusion",
      "description": "A process that adds Gaussian noise to images, speculated to serve a purpose similar to other noise types."
    },
    {
      "type": "Technique",
      "name": "Interpolation",
      "description": "A method to combine source images in latent space and decode them into image space."
    },
    {
      "type": "Model",
      "name": "Stochastic encoder",
      "description": "A model used to encode images into a latent space probabilistically."
    },
    {
      "type": "Dataset",
      "name": "32x32x3 images",
      "description": "A dataset of images with dimensions 32x32 pixels and 3 color channels."
    },
    {
      "type": "Dataset",
      "name": "256x256x3 images",
      "description": "A dataset of images with dimensions 256x256 pixels and 3 color channels."
    },
    {
      "type": "Technique",
      "name": "\u03f5-prediction reverse process",
      "description": "A parameterization method used in the reverse process of the diffusion model."
    },
    {
      "type": "Technique",
      "name": "Infusion Training",
      "description": "A method for learning transition operators of Markov chains."
    },
    {
      "type": "Technique",
      "name": "Variational Walkback",
      "description": "A technique for learning transition operators in generative models."
    },
    {
      "type": "Model",
      "name": "Generative Stochastic Networks",
      "description": "A framework for generative modeling using stochastic processes."
    },
    {
      "type": "Concept",
      "name": "Energy-Based Modeling",
      "description": "A framework that connects to score matching and has implications for various models."
    },
    {
      "type": "Metric",
      "name": "Rate-Distortion Curves",
      "description": "Curves computed over time in the evaluation of the variational bound."
    },
    {
      "type": "Model",
      "name": "Convolutional DRAW",
      "description": "A model related to progressive decoding arguments."
    },
    {
      "type": "Technique",
      "name": "Progressive Lossy Compression",
      "description": "A technique that has connections with diffusion models."
    },
    {
      "type": "Concept",
      "name": "Inductive Biases",
      "description": "The inherent biases that diffusion models have for image data."
    },
    {
      "type": "Concept",
      "name": "Generative Models",
      "description": "A family of models that includes diffusion models, GANs, and others."
    },
    {
      "type": "Model",
      "name": "GANs",
      "description": "Generative Adversarial Networks, a type of deep generative model."
    },
    {
      "type": "Concept",
      "name": "Biases in Datasets",
      "description": "Systematic errors in data that can affect the performance of models trained on them."
    },
    {
      "type": "Technique",
      "name": "Data Compression",
      "description": "The process of reducing the size of data to save space or transmission time."
    },
    {
      "type": "Task",
      "name": "Representation Learning",
      "description": "Learning useful representations of data, often for downstream tasks."
    },
    {
      "type": "Task",
      "name": "Reinforcement Learning",
      "description": "A type of machine learning where agents learn to make decisions by receiving rewards or penalties."
    },
    {
      "type": "Application",
      "name": "Creative Uses",
      "description": "Applications of models in art, photography, and music."
    },
    {
      "type": "Funding Source",
      "name": "ONR PECASE",
      "description": "Office of Naval Research's Presidential Early Career Award for Scientists and Engineers."
    },
    {
      "type": "Funding Source",
      "name": "NSF Graduate Research Fellowship",
      "description": "A fellowship program that supports graduate students in NSF-supported science, technology, engineering, and mathematics disciplines."
    },
    {
      "type": "Infrastructure",
      "name": "Google\u2019s Tensor Flow Research Cloud (TFRC)",
      "description": "A cloud computing service that provides access to TensorFlow and Cloud TPUs for research purposes."
    },
    {
      "type": "Model",
      "name": "GAN",
      "description": "Generative Adversarial Networks, a framework for training generative models using adversarial training."
    },
    {
      "type": "Technique",
      "name": "Energy-based model",
      "description": "A probabilistic model that defines a distribution through an energy function."
    },
    {
      "type": "Model",
      "name": "Neural Ordinary Differential Equations",
      "description": "A framework that treats the evolution of hidden states in neural networks as a continuous-time dynamical system."
    },
    {
      "type": "Model",
      "name": "Pixel SNAIL",
      "description": "An autoregressive generative model that improves upon traditional SNAIL architectures."
    },
    {
      "type": "Model",
      "name": "Residual Energy-Based Models",
      "description": "Models that leverage residual structures for generating text."
    },
    {
      "type": "Technique",
      "name": "Non-linear Independent Components Estimation (NICE)",
      "description": "A technique for estimating independent components in a non-linear manner."
    },
    {
      "type": "Technique",
      "name": "Real NVP",
      "description": "A method for density estimation using invertible neural networks."
    },
    {
      "type": "Model",
      "name": "Energy-Based Models",
      "description": "Models that define a probability distribution through an energy function."
    },
    {
      "type": "Model",
      "name": "Generative Conv Nets",
      "description": "Convolutional networks designed for generative tasks."
    },
    {
      "type": "Model",
      "name": "Generative Adversarial Nets",
      "description": "A framework for training generative models using adversarial training."
    },
    {
      "type": "Technique",
      "name": "Flow Contrastive Estimation",
      "description": "A method for estimating energy-based models using flow-based techniques."
    },
    {
      "type": "Model",
      "name": "FFJORD",
      "description": "A model that provides free-form continuous dynamics for scalable reversible generative models."
    },
    {
      "type": "Conference",
      "name": "International Conference on Learning Representations",
      "description": "A venue for presenting research in machine learning."
    },
    {
      "type": "Conference",
      "name": "Advances In Neural Information Processing Systems",
      "description": "A prominent conference focusing on neural information processing."
    },
    {
      "type": "Conference",
      "name": "Twenty-Second Annual IEEE Conference on Computational Complexity",
      "description": "A conference that discusses computational complexity theory."
    },
    {
      "type": "Metric",
      "name": "Nash Equilibrium",
      "description": "A solution concept in game theory where no player can benefit by changing their strategy while the others keep theirs unchanged."
    },
    {
      "type": "Model",
      "name": "beta-VAE",
      "description": "A type of variational autoencoder that learns disentangled representations."
    },
    {
      "type": "Conference",
      "name": "Advances in Neural Information Processing Systems",
      "description": "A prominent conference in the field of machine learning and neural networks."
    },
    {
      "type": "Conference",
      "name": "International Conference on Machine Learning",
      "description": "A leading conference focused on machine learning research and applications."
    },
    {
      "type": "Model",
      "name": "Flow++",
      "description": "An improved flow-based generative model that incorporates variational dequantization."
    },
    {
      "type": "Model",
      "name": "Video Pixel Networks",
      "description": "A model designed for generating video data using pixel-level predictions."
    },
    {
      "type": "Model",
      "name": "Neural Audio Synthesis",
      "description": "A model for generating audio signals using neural networks."
    },
    {
      "type": "Technique",
      "name": "Progressive Growing of GANs",
      "description": "A technique that improves the quality and stability of GANs by progressively increasing the resolution of generated images."
    },
    {
      "type": "Architecture",
      "name": "Style-based Generator Architecture",
      "description": "An architecture for GANs that allows for more control over the generated images' styles."
    },
    {
      "type": "Technique",
      "name": "Generative Adversarial Networks",
      "description": "A framework for training generative models using adversarial training."
    },
    {
      "type": "Technique",
      "name": "Stochastic Optimization",
      "description": "An optimization method that uses randomness to find optimal solutions."
    },
    {
      "type": "Model",
      "name": "Glow",
      "description": "A generative model that uses invertible 1x1 convolutions."
    },
    {
      "type": "Model",
      "name": "Auto-encoding Variational Bayes",
      "description": "A probabilistic graphical model that combines variational inference with autoencoders."
    },
    {
      "type": "Metric",
      "name": "Image Quality",
      "description": "A measure of the visual quality of generated images."
    },
    {
      "type": "Technique",
      "name": "Hamiltonian Monte Carlo",
      "description": "A Markov Chain Monte Carlo method that uses Hamiltonian dynamics to sample from probability distributions."
    },
    {
      "type": "Model",
      "name": "Inverse Autoregressive Flow",
      "description": "A technique for improving variational inference by transforming simple distributions into complex ones."
    },
    {
      "type": "Model",
      "name": "Energy-inspired Models",
      "description": "Models that learn from sampler-induced distributions to improve sampling efficiency."
    },
    {
      "type": "Technique",
      "name": "Generative Modeling",
      "description": "A framework for modeling the distribution of data to generate new samples."
    },
    {
      "type": "Technique",
      "name": "Spectral Normalization",
      "description": "A technique used to stabilize the training of generative adversarial networks."
    },
    {
      "type": "Model",
      "name": "VQ-DRAW",
      "description": "A sequential discrete variational autoencoder model."
    },
    {
      "type": "Technique",
      "name": "Maximum Likelihood Learning",
      "description": "A statistical method for estimating the parameters of a model."
    },
    {
      "type": "Model",
      "name": "Autoregressive Quantile Networks",
      "description": "A generative modeling approach that uses autoregressive techniques to predict quantiles."
    },
    {
      "type": "Model",
      "name": "Wave Glow",
      "description": "A flow-based generative network designed for speech synthesis."
    },
    {
      "type": "Model",
      "name": "VQ-VAE",
      "description": "A model that combines vector quantization with variational autoencoders for generating high-fidelity images."
    },
    {
      "type": "Model",
      "name": "VQ-VAE-2",
      "description": "A generative model that produces diverse high-fidelity images."
    },
    {
      "type": "Technique",
      "name": "Stochastic Backpropagation",
      "description": "A technique used in deep generative models for approximate inference."
    },
    {
      "type": "Technique",
      "name": "Weight Normalization",
      "description": "A reparameterization technique to accelerate training in neural networks."
    },
    {
      "type": "Technique",
      "name": "Markov Chain Monte Carlo",
      "description": "A method for sampling from probability distributions based on constructing a Markov chain."
    },
    {
      "type": "Technique",
      "name": "Generative modeling",
      "description": "A method for estimating gradients of the data distribution."
    },
    {
      "type": "Model",
      "name": "A-NICE-MC",
      "description": "A model that employs adversarial training for Markov Chain Monte Carlo."
    },
    {
      "type": "Concept",
      "name": "Deep unsupervised learning",
      "description": "Learning without labeled data using techniques from thermodynamics."
    },
    {
      "type": "Model",
      "name": "Wave Net",
      "description": "A generative model for raw audio."
    },
    {
      "type": "Model",
      "name": "Pixel Recurrent Neural Networks",
      "description": "A model for generating images using recurrent neural networks."
    },
    {
      "type": "Model",
      "name": "Pixel CNN",
      "description": "A model for conditional image generation."
    },
    {
      "type": "Technique",
      "name": "Score-based generative models",
      "description": "Generative modeling technique that estimates gradients of the data distribution."
    },
    {
      "type": "Conference",
      "name": "IEEE Conference on Computer Vision and Pattern Recognition",
      "description": "A premier conference in the field of computer vision."
    },
    {
      "type": "Technique",
      "name": "Predictive sampling",
      "description": "A method used for generating samples based on predictive models."
    },
    {
      "type": "Technique",
      "name": "Stochastic normalizing flows",
      "description": "A technique for transforming probability distributions through invertible mappings."
    },
    {
      "type": "Technique",
      "name": "Group normalization",
      "description": "A normalization technique that improves training stability and performance."
    },
    {
      "type": "Model",
      "name": "Generative ConvNet",
      "description": "A convolutional neural network designed for generative tasks."
    },
    {
      "type": "Technique",
      "name": "Learning descriptor networks",
      "description": "A technique for 3D shape synthesis and analysis."
    },
    {
      "type": "Technique",
      "name": "Energy-based spatial-temporal generative convnets",
      "description": "A technique for modeling dynamic patterns."
    },
    {
      "type": "Architecture",
      "name": "Wide residual networks",
      "description": "A type of neural network architecture that improves training of deep networks."
    },
    {
      "type": "Model",
      "name": "Style GAN",
      "description": "A generative adversarial network that allows for fine control over the generated images' styles."
    },
    {
      "type": "Model",
      "name": "Style GAN 2",
      "description": "An improved version of Style GAN that enhances image quality and stability."
    },
    {
      "type": "Algorithm",
      "name": "Minimal Random Coding",
      "description": "A procedure referenced for its theoretical implications in high-dimensional data compression."
    },
    {
      "type": "Technique",
      "name": "Divergence KL",
      "description": "A measure of how one probability distribution diverges from a second, expected probability distribution."
    },
    {
      "type": "Architecture",
      "name": "Wide Res Net",
      "description": "A variant of the ResNet architecture that increases the width of the network to improve performance."
    },
    {
      "type": "Metric",
      "name": "H(x 0)",
      "description": "Entropy of the distribution of the initial data point x 0."
    },
    {
      "type": "Architecture",
      "name": "Convolutional Residual Blocks",
      "description": "A type of neural network architecture used in the models to improve learning."
    },
    {
      "type": "Architecture",
      "name": "Self-Attention Blocks",
      "description": "A mechanism used in the models to allow the model to focus on different parts of the input."
    },
    {
      "type": "Metric",
      "name": "Training Steps",
      "description": "The number of steps taken during the training process of the models."
    },
    {
      "type": "Metric",
      "name": "Parameters",
      "description": "The number of trainable parameters in the models, indicating model complexity."
    },
    {
      "type": "Hardware",
      "name": "TPU v3-8",
      "description": "A type of hardware used for training the models, comparable to 8 V100 GPUs."
    },
    {
      "type": "Dataset",
      "name": "LSUN Bedroom",
      "description": "A dataset containing images of bedrooms, used for training models."
    },
    {
      "type": "Dataset",
      "name": "LSUN Cat",
      "description": "A dataset containing images of cats, used for training models."
    },
    {
      "type": "Dataset",
      "name": "LSUN Church",
      "description": "A dataset containing images of churches, used for training models."
    },
    {
      "type": "Metric",
      "name": "CIFAR 10 sample quality",
      "description": "A metric used to evaluate the quality of generated samples from the model."
    },
    {
      "type": "Technique",
      "name": "\u03b2t schedule",
      "description": "A schedule for the noise variance in the diffusion process, chosen from constant, linear, and quadratic options."
    },
    {
      "type": "Technique",
      "name": "Random Horizontal Flips",
      "description": "A data augmentation technique that flips images horizontally to improve model robustness."
    },
    {
      "type": "Optimizer",
      "name": "RMSProp",
      "description": "An adaptive learning rate method designed to work well in non-stationary settings."
    },
    {
      "type": "Metric",
      "name": "Learning Rate",
      "description": "A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated."
    },
    {
      "type": "Technique",
      "name": "EMA",
      "description": "Exponential Moving Average, used on model parameters with a decay factor."
    },
    {
      "type": "Architecture",
      "name": "Re\ufb01ne Net",
      "description": "An architecture used in NCSN that employs dilated convolutions."
    },
    {
      "type": "Technique",
      "name": "Latent Variable Model",
      "description": "A model type that the proposed sampler is trained as, improving the overall generative process."
    },
    {
      "type": "Technique",
      "name": "Sinusoidal Position Embedding",
      "description": "A method used to condition layers in the U-Net architecture on time step t."
    },
    {
      "type": "Technique",
      "name": "Langevin-like sampler",
      "description": "A sampling technique derived from the forward process coefficients."
    },
    {
      "type": "Concept",
      "name": "Markov chain",
      "description": "A stochastic process that ensures reversibility in the forward process."
    },
    {
      "type": "Concept",
      "name": "Variational inference",
      "description": "A method used to train the sampler as a latent variable model."
    },
    {
      "type": "Metric",
      "name": "DKL(q(x T|x 0) \u2225N(0, I))",
      "description": "A measure of the divergence between the prior and aggregate posterior."
    },
    {
      "type": "Concept",
      "name": "Latent Structure",
      "description": "Refers to the underlying representation used in the sampling process of the diffusion models."
    },
    {
      "type": "Concept",
      "name": "Reverse Process Stochasticity",
      "description": "The stochastic nature of the reverse sampling process in diffusion models."
    },
    {
      "type": "Metric",
      "name": "Quality Metric",
      "description": "A measure that is not guaranteed to be optimized in the training procedure of NCSN."
    },
    {
      "type": "Dataset",
      "name": "Celeb A",
      "description": "A dataset of celebrity images used for training and evaluating models."
    },
    {
      "type": "Concept",
      "name": "Latent Variables",
      "description": "Intermediate representations in the diffusion process that encode high-level attributes."
    },
    {
      "type": "Metric",
      "name": "High-level Attributes",
      "description": "Characteristics such as gender, hair color, and pose that are encoded in the latent variables."
    },
    {
      "type": "Technique",
      "name": "Low-Rank Adaptation",
      "description": "A method that freezes pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture."
    },
    {
      "type": "Model",
      "name": "GPT-3",
      "description": "A large language model with 175 billion parameters used as an example in the paper."
    },
    {
      "type": "Technique",
      "name": "LoRA",
      "description": "Low-Rank Adaptation technique for large language models that reduces the number of trainable parameters."
    },
    {
      "type": "Model",
      "name": "RoBERTa",
      "description": "A robustly optimized BERT model used for evaluation of LoRA."
    },
    {
      "type": "Model",
      "name": "DeBERTa",
      "description": "A model that incorporates disentangled attention and enhanced mask decoder, evaluated in the study."
    },
    {
      "type": "Model",
      "name": "GPT-2",
      "description": "A predecessor to GPT-3, also evaluated in the context of LoRA."
    },
    {
      "type": "Metric",
      "name": "Training Throughput",
      "description": "The rate at which a model can be trained, which is improved by LoRA."
    },
    {
      "type": "Technique",
      "name": "Low-Rank Adaptation (LoRA)",
      "description": "A method to adapt large language models by training only a subset of parameters."
    },
    {
      "type": "Dataset",
      "name": "GLUE",
      "description": "A benchmark dataset for evaluating the performance of models on a variety of natural language understanding tasks."
    },
    {
      "type": "Metric",
      "name": "Adapter Latency",
      "description": "A measure of the time taken to load and use task-specific parameters in addition to the pre-trained model."
    },
    {
      "type": "Concept",
      "name": "Intrinsic Rank",
      "description": "A hypothesized low rank associated with the change in weights during model adaptation."
    },
    {
      "type": "Concept",
      "name": "Over-Parametrized Models",
      "description": "Models that have more parameters than necessary, which can reside on a low intrinsic dimension."
    },
    {
      "type": "Concept",
      "name": "Inference Latency",
      "description": "The delay introduced during model inference, often due to increased model depth or reduced sequence length."
    },
    {
      "type": "Metric",
      "name": "Rank",
      "description": "A measure of the dimensionality of the matrices used in LoRA adaptation."
    },
    {
      "type": "Architecture",
      "name": "Dense Layers",
      "description": "Layers in neural networks where LoRA optimizes rank decomposition matrices."
    },
    {
      "type": "Concept",
      "name": "Task Switching",
      "description": "The process of changing tasks using shared pre-trained models and LoRA modules."
    },
    {
      "type": "Concept",
      "name": "Adaptive Optimizers",
      "description": "Optimizers that adjust learning rates based on the parameters during training."
    },
    {
      "type": "Technique",
      "name": "Prefix-Tuning",
      "description": "A method that allows for tuning a model by adding trainable parameters to the input."
    },
    {
      "type": "Model",
      "name": "autoregressive language model",
      "description": "A type of language model that predicts the next word in a sequence based on previous words."
    },
    {
      "type": "Metric",
      "name": "conditional probabilities",
      "description": "Probabilities that represent the likelihood of a word given a specific context or prompt."
    },
    {
      "type": "Model",
      "name": "P\u03a6(y|x)",
      "description": "A pre-trained autoregressive language model parametrized by \u03a6."
    },
    {
      "type": "Dataset",
      "name": "Z",
      "description": "A training dataset of context-target pairs for downstream tasks."
    },
    {
      "type": "Task",
      "name": "summarization",
      "description": "A downstream conditional text generation task where the input is an article and the output is its summary."
    },
    {
      "type": "Task",
      "name": "machine reading comprehension (MRC)",
      "description": "A downstream conditional text generation task focused on understanding and answering questions based on a given text."
    },
    {
      "type": "Task",
      "name": "natural language to SQL (NL 2 SQL)",
      "description": "A downstream conditional text generation task where the input is a natural language query and the output is the corresponding SQL command."
    },
    {
      "type": "Technique",
      "name": "Full Fine-Tuning",
      "description": "A method where a model is initialized to pre-trained weights and updated to maximize the conditional language modeling objective."
    },
    {
      "type": "Parameter",
      "name": "\u03a60",
      "description": "The initial pre-trained weights of the model."
    },
    {
      "type": "Parameter",
      "name": "\u2206\u03a6",
      "description": "The parameter increment that is learned for each downstream task."
    },
    {
      "type": "Parameter",
      "name": "\u0398",
      "description": "A smaller set of parameters used to encode the task-specific parameter increment \u2206\u03a6."
    },
    {
      "type": "Objective",
      "name": "Conditional Language Modeling Objective",
      "description": "The objective function that the model aims to maximize during training."
    },
    {
      "type": "Technique",
      "name": "Transfer Learning",
      "description": "A machine learning technique where a model developed for a particular task is reused as the starting point for a model on a second task."
    },
    {
      "type": "Technique",
      "name": "Adapter Layers",
      "description": "A method for model adaptation that involves adding additional layers to a pre-trained model."
    },
    {
      "type": "Technique",
      "name": "Input Layer Activations Optimization",
      "description": "A strategy for efficient model adaptation that focuses on optimizing the activations of the input layer."
    },
    {
      "type": "Paper",
      "name": "LoRA: Low-Rank Adaptation of Large Language Models",
      "description": "The paper proposing the low-rank adaptation technique for large language models."
    },
    {
      "type": "Model",
      "name": "Shoeybi et al. (2020)",
      "description": "A reference to a work that discusses model sharding."
    },
    {
      "type": "Model",
      "name": "Lepikhin et al. (2020)",
      "description": "A reference to a work that discusses model sharding."
    },
    {
      "type": "Technique",
      "name": "Prefix Tuning",
      "description": "A method for optimizing prompts in language models"
    },
    {
      "type": "Model",
      "name": "Large Language Models",
      "description": "A class of models that are trained on vast amounts of text data to understand and generate human-like text"
    },
    {
      "type": "Technique",
      "name": "All Reduce",
      "description": "A synchronous GPU operation used for aggregating data across multiple devices"
    },
    {
      "type": "Technique",
      "name": "Broadcast",
      "description": "A synchronous GPU operation used for distributing data to multiple devices"
    },
    {
      "type": "Paper",
      "name": "Li & Liang (2021)",
      "description": "A referenced paper discussing prefix tuning"
    },
    {
      "type": "Concept",
      "name": "Adapter Tuning",
      "description": "A method of fine-tuning models using additional adapter layers."
    },
    {
      "type": "Model",
      "name": "NVIDIA Quadro RTX 8000",
      "description": "A GPU used for measuring inference latency in the experiments."
    },
    {
      "type": "Metric",
      "name": "Trainable Parameters",
      "description": "The number of parameters in adapter layers that can be trained."
    },
    {
      "type": "Concept",
      "name": "Intrinsic Dimension",
      "description": "A property of pre-trained language models indicating their ability to learn efficiently in a lower-dimensional space."
    },
    {
      "type": "Parameter",
      "name": "W0",
      "description": "The initial weight matrix that is frozen during training."
    },
    {
      "type": "Parameter",
      "name": "\u0394W",
      "description": "The low-rank update to the weight matrix, represented as the product of matrices B and A."
    },
    {
      "type": "Parameter",
      "name": "B",
      "description": "A trainable parameter matrix in the low-rank adaptation method."
    },
    {
      "type": "Parameter",
      "name": "A",
      "description": "Another trainable parameter matrix in the low-rank adaptation method."
    },
    {
      "type": "Constant",
      "name": "\u03b1",
      "description": "A scaling constant used in the adaptation process."
    },
    {
      "type": "Concept",
      "name": "Full Fine-tuning",
      "description": "A method of training all parameters of a pre-trained model."
    },
    {
      "type": "Concept",
      "name": "Adapter-based methods",
      "description": "Techniques that use additional modules to adapt pre-trained models without modifying the original parameters."
    },
    {
      "type": "Concept",
      "name": "Prefix-based methods",
      "description": "Techniques that prepend additional information to the input for model adaptation."
    },
    {
      "type": "Parameter",
      "name": "W",
      "description": "The weight matrix used in the model, which can be adapted for different tasks."
    },
    {
      "type": "Parameter",
      "name": "BA",
      "description": "A low-rank adaptation component added to the original weights."
    },
    {
      "type": "Parameter",
      "name": "B\u2032A\u2032",
      "description": "A different low-rank adaptation component for switching tasks."
    },
    {
      "type": "Architecture",
      "name": "Self-Attention Module",
      "description": "A component of the Transformer architecture that uses weight matrices Wq, Wk, Wv, and Wo."
    },
    {
      "type": "Architecture",
      "name": "MLP Module",
      "description": "A feed-forward neural network component in the Transformer architecture."
    },
    {
      "type": "Metric",
      "name": "Parameter Efficiency",
      "description": "A measure of how effectively a model uses its parameters, particularly in the context of training."
    },
    {
      "type": "Metric",
      "name": "VRAM usage",
      "description": "The amount of video RAM consumed during model training."
    },
    {
      "type": "Metric",
      "name": "Checkpoint size",
      "description": "The size of the model's saved state during training."
    },
    {
      "type": "Architecture",
      "name": "MLP layers",
      "description": "Multi-layer perceptron layers used in neural networks."
    },
    {
      "type": "Architecture",
      "name": "Layer Norm layers",
      "description": "Normalization layers that stabilize the learning process in neural networks."
    },
    {
      "type": "Metric",
      "name": "Speedup",
      "description": "A 25% speedup during training compared to full fine-tuning, indicating improved efficiency."
    },
    {
      "type": "Architecture",
      "name": "VRAM",
      "description": "Video RAM used for storing pre-trained weights, facilitating the swapping of LoRA weights."
    },
    {
      "type": "Dataset",
      "name": "WikiSQL",
      "description": "A dataset for natural language to SQL query generation."
    },
    {
      "type": "Dataset",
      "name": "SAMSum",
      "description": "A dataset for conversation summarization."
    },
    {
      "type": "Architecture",
      "name": "NVIDIA Tesla V100",
      "description": "A GPU used for running experiments in the paper."
    },
    {
      "type": "Technique",
      "name": "Fine-Tuning (FT)",
      "description": "A common approach for adapting pre-trained models by updating model parameters based on new data."
    },
    {
      "type": "Dataset",
      "name": "350 GB Model",
      "description": "A large language model that requires significant storage space for deployment."
    },
    {
      "type": "Architecture",
      "name": "V100 GPU",
      "description": "A type of GPU used for training large models, specifically NVIDIA's Volta architecture."
    },
    {
      "type": "Dataset",
      "name": "MNLI",
      "description": "Multi-Genre Natural Language Inference, a dataset for evaluating natural language understanding."
    },
    {
      "type": "Dataset",
      "name": "SST-2",
      "description": "Stanford Sentiment Treebank, a dataset for sentiment analysis."
    },
    {
      "type": "Dataset",
      "name": "MRPC",
      "description": "Microsoft Research Paraphrase Corpus, a dataset for paraphrase identification."
    },
    {
      "type": "Dataset",
      "name": "CoLA",
      "description": "Corpus of Linguistic Acceptability, a dataset for linguistic acceptability judgments."
    },
    {
      "type": "Dataset",
      "name": "QNLI",
      "description": "Question Natural Language Inference, a dataset for question answering."
    },
    {
      "type": "Dataset",
      "name": "QQP",
      "description": "Quora Question Pairs, a dataset for identifying duplicate questions."
    },
    {
      "type": "Dataset",
      "name": "RTE",
      "description": "Recognizing Textual Entailment, a dataset for entailment tasks."
    },
    {
      "type": "Dataset",
      "name": "STS-B",
      "description": "Semantic Textual Similarity Benchmark, a dataset for evaluating semantic similarity."
    },
    {
      "type": "Model",
      "name": "Ro BERTabase",
      "description": "A variant of the BERT model with a smaller architecture."
    },
    {
      "type": "Model",
      "name": "Ro BERTalarge",
      "description": "A larger variant of the BERT model designed for better performance."
    },
    {
      "type": "Model",
      "name": "De BERTa XXL",
      "description": "A large-scale model in the BERT family, optimized for various NLP tasks."
    },
    {
      "type": "Metric",
      "name": "Matthew\u2019s correlation",
      "description": "A metric used to evaluate the performance of classification models."
    },
    {
      "type": "Metric",
      "name": "Pearson correlation",
      "description": "A measure of the linear correlation between predicted and actual values."
    },
    {
      "type": "Technique",
      "name": "Bias-only or Bit Fit",
      "description": "A baseline technique where only bias vectors are trained while freezing other parameters."
    },
    {
      "type": "Technique",
      "name": "Pre\ufb01x-embedding tuning",
      "description": "A method that inserts special tokens among input tokens with trainable embeddings."
    },
    {
      "type": "Technique",
      "name": "Pre\ufb01x-layer tuning",
      "description": "An extension of pre\ufb01x-embedding tuning that involves learning at the layer level."
    },
    {
      "type": "Metric",
      "name": "lp",
      "description": "Denotes the number of pre\ufb01x tokens used in tuning."
    },
    {
      "type": "Metric",
      "name": "li",
      "description": "Denotes the number of in\ufb01x tokens used in tuning."
    },
    {
      "type": "Technique",
      "name": "Prefix-layer tuning",
      "description": "An extension to prefix-embedding tuning that learns activations after every Transformer layer."
    },
    {
      "type": "Technique",
      "name": "Adapter tuning",
      "description": "A method that inserts adapter layers between the self-attention module and the subsequent residual connection."
    },
    {
      "type": "Model",
      "name": "Adapter H",
      "description": "The original design of adapter tuning with two fully connected layers and a nonlinearity in between."
    },
    {
      "type": "Model",
      "name": "Adapter L",
      "description": "A more efficient design of adapter tuning proposed by Lin et al. that applies the adapter layer only after the MLP module."
    },
    {
      "type": "Metric",
      "name": "Trainable parameters",
      "description": "The number of parameters that can be adjusted during training, represented as |\u0398|."
    },
    {
      "type": "Architecture",
      "name": "Adapter P",
      "description": "A design similar to Adapter L proposed by Pfeiffer et al. (2021)."
    },
    {
      "type": "Architecture",
      "name": "Adapter D",
      "description": "Baseline design that drops some adapter layers for greater efficiency, proposed by R\u00fcckl\u00e9 et al. (2020)."
    },
    {
      "type": "Model",
      "name": "Wq",
      "description": "Weight matrix for query in attention mechanisms."
    },
    {
      "type": "Model",
      "name": "Wv",
      "description": "Weight matrix for value in attention mechanisms."
    },
    {
      "type": "Architecture",
      "name": "Weight Matrices",
      "description": "Matrices that represent the weights in a neural network model, specifically Wq and Wv in this context."
    },
    {
      "type": "Model",
      "name": "GPT-2 M",
      "description": "A medium-sized version of the GPT-2 language model."
    },
    {
      "type": "Model",
      "name": "GPT-2 L",
      "description": "A large-sized version of the GPT-2 language model."
    },
    {
      "type": "Metric",
      "name": "BLEU",
      "description": "A metric for evaluating the quality of text generated by models."
    },
    {
      "type": "Metric",
      "name": "NIST",
      "description": "A metric for evaluating machine translation quality."
    },
    {
      "type": "Metric",
      "name": "MET",
      "description": "A metric for evaluating the quality of generated text."
    },
    {
      "type": "Metric",
      "name": "ROUGE-L",
      "description": "A metric for evaluating the quality of summaries."
    },
    {
      "type": "Metric",
      "name": "CIDEr",
      "description": "A metric for evaluating image captioning quality."
    },
    {
      "type": "Dataset",
      "name": "E2E NLG Challenge",
      "description": "A benchmark dataset for evaluating natural language generation models."
    },
    {
      "type": "Library",
      "name": "Hugging Face Transformers",
      "description": "A library that provides pre-trained models for natural language processing."
    },
    {
      "type": "Model",
      "name": "RoBERTa base",
      "description": "A pre-trained language model with 125 million parameters."
    },
    {
      "type": "Model",
      "name": "RoBERTa large",
      "description": "A larger variant of the RoBERTa model with 355 million parameters."
    },
    {
      "type": "Technique",
      "name": "Adapters",
      "description": "A method for adapting pre-trained models to specific tasks."
    },
    {
      "type": "Model",
      "name": "De BERTa",
      "description": "A variant of BERT trained on a larger scale, performing competitively on benchmarks."
    },
    {
      "type": "Dataset",
      "name": "Super GLUE",
      "description": "An advanced benchmark for evaluating natural language understanding tasks."
    },
    {
      "type": "Dataset",
      "name": "Web NLG",
      "description": "A dataset for evaluating natural language generation systems, introduced by Gardent et al. in 2017."
    },
    {
      "type": "Dataset",
      "name": "DART",
      "description": "A dataset for evaluating data-to-text generation, introduced by Nan et al. in 2020."
    },
    {
      "type": "Dataset",
      "name": "Wiki SQL",
      "description": "A dataset for evaluating SQL query generation."
    },
    {
      "type": "Dataset",
      "name": "MNLI-m",
      "description": "Multi-Genre Natural Language Inference dataset."
    },
    {
      "type": "Metric",
      "name": "Rouge-1",
      "description": "A metric for evaluating the quality of summaries."
    },
    {
      "type": "Metric",
      "name": "Rouge-2",
      "description": "A metric for evaluating the quality of summaries."
    },
    {
      "type": "Metric",
      "name": "Rouge-L",
      "description": "A metric for evaluating the quality of summaries."
    },
    {
      "type": "Metric",
      "name": "Standard Deviation",
      "description": "A statistical measure used to quantify the amount of variation in a set of values."
    },
    {
      "type": "Metric",
      "name": "Performance Drop",
      "description": "A decrease in performance observed when using more than a certain number of special tokens."
    },
    {
      "type": "Dataset",
      "name": "Multi NLI",
      "description": "A dataset for natural language inference tasks."
    },
    {
      "type": "Metric",
      "name": "Validation Accuracy",
      "description": "A measure of model performance on validation data."
    },
    {
      "type": "Technique",
      "name": "Fine-Tune",
      "description": "A method of adapting a pre-trained model to a specific task."
    },
    {
      "type": "Technique",
      "name": "Prefix Embed",
      "description": "A method for tuning embeddings in the prefix layer."
    },
    {
      "type": "Technique",
      "name": "Prefix Layer",
      "description": "A tuning method that focuses on the prefix layer of the model."
    },
    {
      "type": "Technique",
      "name": "Adapter(H)",
      "description": "A technique for adapting models using lightweight adapters."
    },
    {
      "type": "Dataset",
      "name": "MNLI-matched",
      "description": "A dataset used for evaluating natural language inference tasks."
    },
    {
      "type": "Technique",
      "name": "Prompt Engineering",
      "description": "The empirical art of composing and formatting prompts to maximize a model\u2019s performance on a desired task."
    },
    {
      "type": "Technique",
      "name": "Fine-Tuning",
      "description": "The process of retraining a model pre-trained on general domains to a specific task."
    },
    {
      "type": "Technique",
      "name": "Parameter-Efficient Adaptation",
      "description": "Methods that adapt models using fewer parameters, often by inserting adapter layers."
    },
    {
      "type": "Technique",
      "name": "COMPACTER",
      "description": "A contemporary extension of adapter layers that parametrizes them using Kronecker products with a predetermined weight sharing scheme."
    },
    {
      "type": "Concept",
      "name": "Low-Rank Structures",
      "description": "Common structures in machine learning that indicate certain intrinsic low-rank properties."
    },
    {
      "type": "Paper",
      "name": "Lester et al. (2021)",
      "description": "A referenced work related to prompt engineering."
    },
    {
      "type": "Paper",
      "name": "Hambardzumyan et al. (2020)",
      "description": "A referenced work related to prompt engineering."
    },
    {
      "type": "Paper",
      "name": "Liu et al. (2021)",
      "description": "A referenced work related to prompt engineering."
    },
    {
      "type": "Paper",
      "name": "Li et al. (2016)",
      "description": "A referenced work discussing low-rank structures in machine learning."
    },
    {
      "type": "Paper",
      "name": "Cai et al. (2010)",
      "description": "A referenced work discussing low-rank structures in machine learning."
    },
    {
      "type": "Paper",
      "name": "Li et al. (2018)",
      "description": "A referenced work discussing low-rank structures in machine learning."
    },
    {
      "type": "Paper",
      "name": "Grasedyck et al. (2013)",
      "description": "A referenced work discussing low-rank structures in machine learning."
    },
    {
      "type": "Paper",
      "name": "Oymak et al. (2019)",
      "description": "A referenced work discussing low-rank properties in over-parametrized neural networks."
    },
    {
      "type": "Technique",
      "name": "Low-Rank Constraint",
      "description": "A method imposed during training to enforce low-rank properties in neural networks."
    },
    {
      "type": "Concept",
      "name": "Neural Tangent Kernels",
      "description": "A theoretical framework for understanding the behavior of neural networks during training."
    },
    {
      "type": "Concept",
      "name": "Over-Parametrized Neural Network",
      "description": "A neural network with more parameters than necessary, often leading to low-rank properties."
    },
    {
      "type": "Technique",
      "name": "Adversarial Training",
      "description": "A training method that aims to improve model robustness against adversarial examples."
    },
    {
      "type": "Concept",
      "name": "Low-Rank Structure",
      "description": "A mathematical structure that allows for efficient representation and adaptation of models."
    },
    {
      "type": "Metric",
      "name": "Trainable Parameters Reduction",
      "description": "A measure of how many parameters in a model can be trained, with a focus on reducing this number."
    },
    {
      "type": "Metric",
      "name": "trainable parameters",
      "description": "The number of parameters in a model that can be adjusted during training."
    },
    {
      "type": "Metric",
      "name": "Parameter Budget",
      "description": "The limit on the number of parameters that can be adapted, set to 18 million in this context."
    },
    {
      "type": "Metric",
      "name": "Validation accuracy",
      "description": "A measure of model performance on validation datasets."
    },
    {
      "type": "Architecture",
      "name": "Attention weights",
      "description": "Components of the model that determine the importance of different inputs."
    },
    {
      "type": "Weight Matrix",
      "name": "Wk",
      "description": "Key weight matrix used in the model."
    },
    {
      "type": "Weight Matrix",
      "name": "Wo",
      "description": "Output weight matrix used in the model."
    },
    {
      "type": "Parameter Rank",
      "name": "r",
      "description": "The rank used for adapting weight matrices."
    },
    {
      "type": "Matrix",
      "name": "Update Matrix (\u2206W)",
      "description": "The matrix used to update model weights during adaptation."
    },
    {
      "type": "Concept",
      "name": "Pre-training",
      "description": "The initial training phase of a model on a large dataset before fine-tuning on specific tasks."
    },
    {
      "type": "Concept",
      "name": "Downstream Task",
      "description": "A specific task that the model is fine-tuned on after pre-training."
    },
    {
      "type": "Metric",
      "name": "Subspace Similarity",
      "description": "A measure of how much one subspace overlaps with another, quantified using the Grassmann distance."
    },
    {
      "type": "Metric",
      "name": "Grassmann Distance",
      "description": "A mathematical measure used to quantify the distance between two subspaces."
    },
    {
      "type": "Matrix",
      "name": "Adaptation Matrices",
      "description": "Matrices learned during the adaptation process with different ranks."
    },
    {
      "type": "Matrix",
      "name": "Right-Singular Unitary Matrices",
      "description": "Matrices obtained from singular value decomposition of the adaptation matrices."
    },
    {
      "type": "Layer",
      "name": "48th Layer",
      "description": "A specific layer of the model where the analysis of subspace similarity is focused."
    },
    {
      "type": "Variable",
      "name": "Ar",
      "description": "A parameter representing the rank in the context of low-rank adaptation."
    },
    {
      "type": "Variable",
      "name": "\u2206Wq",
      "description": "The change in the query weights in the context of low-rank adaptation."
    },
    {
      "type": "Variable",
      "name": "\u2206Wv",
      "description": "The change in the value weights in the context of low-rank adaptation."
    },
    {
      "type": "Metric",
      "name": "Normalized Similarity",
      "description": "A measure used to quantify the similarity between subspaces."
    },
    {
      "type": "Architecture",
      "name": "Ar=8",
      "description": "A specific low-rank adaptation configuration with rank 8."
    },
    {
      "type": "Architecture",
      "name": "Ar=64",
      "description": "A specific low-rank adaptation configuration with rank 64."
    },
    {
      "type": "Matrix",
      "name": "\u2206W",
      "description": "The adaptation matrix used to modify the weights of the model."
    },
    {
      "type": "Dataset",
      "name": "Gaussian Matrices",
      "description": "Random matrices used for comparison that do not share common singular value directions."
    },
    {
      "type": "Metric",
      "name": "Frobenius norm",
      "description": "A measure used to compare the similarity between matrices."
    },
    {
      "type": "Architecture",
      "name": "U",
      "description": "The left singular-vector matrix of \u2206W."
    },
    {
      "type": "Architecture",
      "name": "V",
      "description": "The right singular-vector matrix of \u2206W."
    },
    {
      "type": "Architecture",
      "name": "A'r",
      "description": "Another matrix related to the low-rank adaptation process."
    },
    {
      "type": "Concept",
      "name": "Singular vectors",
      "description": "Vectors that represent the directions of maximum variance in a matrix."
    },
    {
      "type": "Concept",
      "name": "Weight matrices",
      "description": "Matrices that contain the weights of the neural network layers."
    },
    {
      "type": "Dataset",
      "name": "Random matrix",
      "description": "A matrix used as a baseline for comparison in the analysis."
    },
    {
      "type": "Metric",
      "name": "Ampli\ufb01cation Factor",
      "description": "A measure indicating how much the low-rank adaptation matrix amplifies important features."
    },
    {
      "type": "Concept",
      "name": "Task-Switching",
      "description": "The ability to quickly switch between different tasks without significant overhead."
    },
    {
      "type": "Section",
      "name": "Section H.4",
      "description": "Discusses the reasons for the smaller amplification factor when r = 64."
    },
    {
      "type": "Section",
      "name": "Section H.3",
      "description": "Provides visualization of how correlation changes with more top singular directions."
    },
    {
      "type": "Architecture",
      "name": "Neural Networks",
      "description": "A computational model inspired by the human brain, consisting of interconnected nodes (neurons)."
    },
    {
      "type": "Concept",
      "name": "Rank-de\ufb01ciency",
      "description": "A property of weight matrices indicating that they may not be full rank, suggesting potential for adaptation."
    },
    {
      "type": "Concept",
      "name": "Over-parameterization",
      "description": "A phenomenon in deep learning where models have more parameters than necessary, which can lead to better convergence properties."
    },
    {
      "type": "Technique",
      "name": "Layer Normalization",
      "description": "A technique used to normalize the inputs across the features for each data point, improving training stability."
    },
    {
      "type": "Dataset",
      "name": "Semeval-2017",
      "description": "A dataset used for evaluating semantic textual similarity in multilingual and crosslingual contexts."
    },
    {
      "type": "Metric",
      "name": "Semantic Textual Similarity",
      "description": "A measure used to evaluate how similar two pieces of text are."
    },
    {
      "type": "Architecture",
      "name": "Deep Neural Networks",
      "description": "A class of models used in natural language processing that can learn from multiple tasks."
    },
    {
      "type": "Conference",
      "name": "ICML",
      "description": "International Conference on Machine Learning, where significant research in machine learning is presented."
    },
    {
      "type": "Architecture",
      "name": "Bidirectional Transformers",
      "description": "A type of transformer architecture that processes text in both directions to improve understanding."
    },
    {
      "type": "Dataset",
      "name": "WebNLG",
      "description": "A dataset used for generating text from RDF data."
    },
    {
      "type": "Metric",
      "name": "Paraphrase Quality",
      "description": "A measure of how well generated sentences convey the same meaning as the original sentences."
    },
    {
      "type": "Dataset",
      "name": "Samsum corpus",
      "description": "A human-annotated dialogue dataset for abstractive summarization."
    },
    {
      "type": "Technique",
      "name": "Grassmann Discriminant Analysis",
      "description": "A method for subspace-based learning that provides a unifying view on various learning techniques."
    },
    {
      "type": "Technique",
      "name": "Word-level Adversarial Re Programming (WARP)",
      "description": "A technique for generating adversarial examples at the word level."
    },
    {
      "type": "Technique",
      "name": "Parameter-Efficient Transfer Learning",
      "description": "A technique for transferring knowledge from one model to another with minimal parameters."
    },
    {
      "type": "Technique",
      "name": "Low-Rank Expansions",
      "description": "A method to speed up convolutional neural networks by approximating them with low-rank matrices."
    },
    {
      "type": "Technique",
      "name": "Prompt Tuning",
      "description": "A method for fine-tuning models using parameter-efficient prompts."
    },
    {
      "type": "Technique",
      "name": "Overparameterized Neural Networks",
      "description": "Neural networks with more parameters than necessary for the task."
    },
    {
      "type": "Conference",
      "name": "Conference On Learning Theory",
      "description": "A conference focusing on learning theory in machine learning."
    },
    {
      "type": "Conference",
      "name": "Findings of the Association for Computational Linguistics: EMNLP",
      "description": "A conference that presents findings in computational linguistics."
    },
    {
      "type": "Conference",
      "name": "EMNLP 2020",
      "description": "The conference where the findings related to this paper were presented."
    },
    {
      "type": "Organization",
      "name": "Association for Computational Linguistics",
      "description": "The organization that hosted the EMNLP 2020 conference."
    },
    {
      "type": "Model",
      "name": "Roberta",
      "description": "A robustly optimized BERT pretraining approach."
    },
    {
      "type": "Technique",
      "name": "Decoupled weight decay regularization",
      "description": "A technique for regularizing neural networks."
    },
    {
      "type": "Model",
      "name": "Compacter",
      "description": "Efficient low-rank hypercomplex adapter layers."
    },
    {
      "type": "Dataset",
      "name": "e2e dataset",
      "description": "A dataset that presents new challenges for end-to-end generation tasks."
    },
    {
      "type": "Dataset",
      "name": "Dart",
      "description": "An open-domain structured data record to text generation dataset."
    },
    {
      "type": "Technique",
      "name": "Adapter-fusion",
      "description": "A method for non-destructive task composition in transfer learning."
    },
    {
      "type": "Technique",
      "name": "Semi-orthogonal low-rank matrix factorization",
      "description": "A technique for deep neural networks that involves low-rank matrix factorization."
    },
    {
      "type": "Technique",
      "name": "Generalization guarantees",
      "description": "A concept related to ensuring that neural networks can generalize well based on low-rank structures."
    },
    {
      "type": "Dataset",
      "name": "SQuAD",
      "description": "Stanford Question Answering Dataset, used for evaluating question answering systems."
    },
    {
      "type": "Metric",
      "name": "Unanswerable Questions",
      "description": "A metric used to assess the ability of models to identify questions that cannot be answered."
    },
    {
      "type": "Model",
      "name": "Megatron-LM",
      "description": "A model for training multi-billion parameter language models using model parallelism."
    },
    {
      "type": "Dataset",
      "name": "Sentiment Treebank",
      "description": "A dataset used for evaluating recursive deep models for semantic compositionality."
    },
    {
      "type": "Technique",
      "name": "Low-Rank Matrix Factorization",
      "description": "A technique for training deep neural networks with high-dimensional output targets."
    },
    {
      "type": "Dataset",
      "name": "SuperGLUE",
      "description": "An advanced benchmark for general-purpose language understanding systems."
    },
    {
      "type": "Dataset",
      "name": "Neural Network Acceptability Judgments",
      "description": "A dataset for evaluating neural network models on acceptability judgments."
    },
    {
      "type": "Dataset",
      "name": "Broad-Coverage Challenge Corpus",
      "description": "A corpus for sentence understanding through inference."
    },
    {
      "type": "Conference",
      "name": "North American Chapter of the Association for Computational Linguistics",
      "description": "A conference focused on human language technologies."
    },
    {
      "type": "Model",
      "name": "Transformers",
      "description": "A state-of-the-art architecture for natural language processing."
    },
    {
      "type": "Model",
      "name": "BitFit",
      "description": "A simple parameter-efficient fine-tuning technique for transformer-based masked language models."
    },
    {
      "type": "Model",
      "name": "Infinite-Width Neural Networks",
      "description": "A model discussed in the context of feature learning."
    },
    {
      "type": "Concept",
      "name": "Few-shot Learning",
      "description": "A learning paradigm that is beneficial when limited training samples are available."
    },
    {
      "type": "Conference",
      "name": "ICASSP",
      "description": "IEEE International Conference on Acoustics, Speech and Signal Processing."
    },
    {
      "type": "Technique",
      "name": "Few-shot learning",
      "description": "A learning paradigm where a model is trained with a very small number of examples."
    },
    {
      "type": "Metric",
      "name": "Latency",
      "description": "Time taken for a single forward pass in the model."
    },
    {
      "type": "Concept",
      "name": "Sequence Length",
      "description": "Length of the input sequences fed into the model."
    },
    {
      "type": "Concept",
      "name": "Adapter Bottleneck Dimension",
      "description": "Dimension of the bottleneck layer in the adapter design."
    },
    {
      "type": "Dataset",
      "name": "GLUE Benchmark",
      "description": "A wide-ranging collection of natural language understanding tasks."
    },
    {
      "type": "Dataset",
      "name": "E2E",
      "description": "Consists of roughly 42,000 training, 4,600 validation, and 4,600 test examples from the restaurant domain."
    },
    {
      "type": "Technique",
      "name": "Adam W",
      "description": "An optimization algorithm used for training."
    },
    {
      "type": "Metric",
      "name": "learning rate decay schedule",
      "description": "A strategy for adjusting the learning rate during training."
    },
    {
      "type": "Metric",
      "name": "Median",
      "description": "Statistical measure used to report results over multiple random seeds."
    },
    {
      "type": "Architecture",
      "name": "AdamW",
      "description": "An optimization algorithm used for training the models."
    },
    {
      "type": "Model",
      "name": "DEBERTA",
      "description": "A model architecture referenced in the context of training and adaptation."
    },
    {
      "type": "Metric",
      "name": "median",
      "description": "Statistical measure used to report results over multiple runs."
    },
    {
      "type": "Technique",
      "name": "linear learning rate decay",
      "description": "A schedule for adjusting the learning rate during training."
    },
    {
      "type": "Architecture",
      "name": "Bottleneck",
      "description": "A layer configuration used in neural networks to reduce dimensionality."
    },
    {
      "type": "Metric",
      "name": "Mean over random seeds",
      "description": "A statistical measure used to report the average performance across multiple runs."
    },
    {
      "type": "Hyperparameter",
      "name": "Batch size",
      "description": "The number of training samples used in one iteration of model training."
    },
    {
      "type": "Hyperparameter",
      "name": "Learning rate",
      "description": "A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated."
    },
    {
      "type": "Hyperparameter",
      "name": "Weight decay factor",
      "description": "A regularization technique used to prevent overfitting by penalizing large weights."
    },
    {
      "type": "Hyperparameter",
      "name": "Sequence length",
      "description": "The length of the input sequences fed into the model during training."
    },
    {
      "type": "Metric",
      "name": "Dropout Probability",
      "description": "A technique used to prevent overfitting by randomly setting a fraction of input units to 0."
    },
    {
      "type": "Metric",
      "name": "Validation Performance",
      "description": "A measure of the model's performance on validation data."
    },
    {
      "type": "Technique",
      "name": "Prefix Embedding Tuning",
      "description": "A variant of prefix tuning that uses trainable embeddings for special tokens."
    },
    {
      "type": "Technique",
      "name": "Prefix Layer Tuning",
      "description": "A variant of prefix tuning that modifies the hidden representations of special tokens."
    },
    {
      "type": "Technique",
      "name": "pre\ufb01x-embedding tuning",
      "description": "A method for tuning embeddings in language models."
    },
    {
      "type": "Hyperparameter",
      "name": "Epoch",
      "description": "One complete pass through the entire training dataset."
    },
    {
      "type": "Technique",
      "name": "Prefix-Embedding Tuning (PE)",
      "description": "A tuning method that uses prefix embeddings."
    },
    {
      "type": "Technique",
      "name": "Prefix-Layer Tuning (PL)",
      "description": "A tuning method that adjusts the layers of the model."
    },
    {
      "type": "Metric",
      "name": "Human Baseline",
      "description": "A performance benchmark based on human performance."
    },
    {
      "type": "Model",
      "name": "GPT-2 Medium",
      "description": "A variant of the GPT-2 language model with 354 million parameters."
    },
    {
      "type": "Model",
      "name": "GPT-2 Large",
      "description": "A larger variant of the GPT-2 language model with 774 million parameters."
    },
    {
      "type": "Metric",
      "name": "TER",
      "description": "Translation Edit Rate, a metric for evaluating the edit distance between generated and reference texts."
    },
    {
      "type": "Technique",
      "name": "FTTop 2",
      "description": "A specific adaptation method for fine-tuning models."
    },
    {
      "type": "Technique",
      "name": "Pre\ufb01x",
      "description": "An adaptation method that modifies the input to the model."
    },
    {
      "type": "Dataset",
      "name": "MNLI-100",
      "description": "A low-data version of the MNLI dataset with 100 training examples."
    },
    {
      "type": "Technique",
      "name": "Pre\ufb01x Embed",
      "description": "An adaptation approach that uses prefix embeddings."
    },
    {
      "type": "Technique",
      "name": "Pre\ufb01x Layer",
      "description": "An adaptation approach that uses prefix layers."
    },
    {
      "type": "Dataset",
      "name": "MNLI-Full",
      "description": "A larger version of the MNLI dataset for comprehensive evaluation."
    },
    {
      "type": "Dataset",
      "name": "MNLI-1k",
      "description": "A subset of the MNLI dataset containing 1,000 examples."
    },
    {
      "type": "Dataset",
      "name": "MNLI-10k",
      "description": "A subset of the MNLI dataset containing 10,000 examples."
    },
    {
      "type": "Metric",
      "name": "Projection Metric",
      "description": "Measures distance between subspaces."
    },
    {
      "type": "Matrix",
      "name": "UiA",
      "description": "A column orthonormal matrix obtained from the left singular matrix of matrix A."
    },
    {
      "type": "Matrix",
      "name": "UjB",
      "description": "A column orthonormal matrix obtained from the left singular matrix of matrix B."
    },
    {
      "type": "Technique",
      "name": "LoRA+PE",
      "description": "An extension of LoRA that incorporates prefix embeddings."
    },
    {
      "type": "Metric",
      "name": "Similarity",
      "description": "A measure defined to evaluate the relationship between two matrices."
    },
    {
      "type": "Metric",
      "name": "Validation Loss",
      "description": "A measure of how well a model performs on a validation dataset."
    },
    {
      "type": "Metric",
      "name": "Test Metrics",
      "description": "Various performance measures used to evaluate the effectiveness of a model."
    },
    {
      "type": "Concept",
      "name": "Optimal Rank",
      "description": "The best rank value for low-rank adaptation that yields the best performance."
    },
    {
      "type": "Architecture",
      "name": "Low-Rank Matrices",
      "description": "Matrices that have a rank lower than their dimensions, used in the context of model adaptation."
    },
    {
      "type": "Concept",
      "name": "Task-Specific Directions",
      "description": "Directions in the parameter space that are tailored for specific tasks, as opposed to general directions."
    },
    {
      "type": "Metric",
      "name": "Amplification Factor",
      "description": "A measure of how much task-specific directions are amplified by the weight updates in the model."
    },
    {
      "type": "Architecture",
      "name": "96-layer Transformer",
      "description": "A specific implementation of the Transformer model with 96 layers."
    },
    {
      "type": "Concept",
      "name": "SVD Decomposition",
      "description": "A mathematical technique used to factorize a matrix into singular vectors and singular values."
    },
    {
      "type": "Metric",
      "name": "METEOR",
      "description": "A metric that evaluates machine translation by considering synonyms and stemming."
    },
    {
      "type": "Metric",
      "name": "ROUGE L",
      "description": "A metric for evaluating the quality of summaries by comparing them to reference summaries."
    },
    {
      "type": "Architecture",
      "name": "Hyperparameters",
      "description": "Settings that are tuned to optimize model performance."
    },
    {
      "type": "Metric",
      "name": "Normalized Subspace Similarity",
      "description": "A metric used to measure the similarity between the singular directions of two matrices."
    },
    {
      "type": "Technique",
      "name": "Approximate Attention",
      "description": "Methods that reduce compute complexity by trading off model quality."
    },
    {
      "type": "Technique",
      "name": "IO-Aware Attention",
      "description": "Attention algorithms that account for memory reads and writes between GPU memory levels."
    },
    {
      "type": "Model",
      "name": "Flash Attention",
      "description": "An IO-aware exact attention algorithm that reduces memory reads/writes using tiling."
    },
    {
      "type": "Technique",
      "name": "block-sparse attention",
      "description": "An extension of Flash Attention that yields an approximate attention algorithm faster than existing methods."
    },
    {
      "type": "Model",
      "name": "BERT-large",
      "description": "A transformer model used for natural language processing tasks, evaluated with a sequence length of 512."
    },
    {
      "type": "Dataset",
      "name": "long-range arena",
      "description": "A benchmark dataset for evaluating the performance of models on long-context tasks, evaluated with sequence lengths between 1K and 4K."
    },
    {
      "type": "Metric",
      "name": "end-to-end wall-clock speedup",
      "description": "A performance metric indicating the speed improvement of training models."
    },
    {
      "type": "Metric",
      "name": "Perplexity",
      "description": "A measurement used to evaluate the quality of language models, with lower values indicating better performance."
    },
    {
      "type": "Dataset",
      "name": "Path-X",
      "description": "A challenge dataset used to evaluate the performance of models on long sequences."
    },
    {
      "type": "Dataset",
      "name": "Path-256",
      "description": "Another challenge dataset for evaluating model performance on long sequences."
    },
    {
      "type": "Concept",
      "name": "Attention",
      "description": "A mechanism that allows models to focus on specific parts of the input sequence when making predictions."
    },
    {
      "type": "Technique",
      "name": "Approximate Attention Methods",
      "description": "Methods aimed at reducing compute and memory requirements of attention mechanisms."
    },
    {
      "type": "Technique",
      "name": "Sparse-Approximation",
      "description": "A method that reduces the number of computations by focusing on a subset of the input."
    },
    {
      "type": "Technique",
      "name": "Low-Rank Approximation",
      "description": "A method that approximates matrices with lower-dimensional representations to reduce computations."
    },
    {
      "type": "Concept",
      "name": "IO-Awareness",
      "description": "The principle of accounting for memory access overheads in attention algorithms."
    },
    {
      "type": "Model",
      "name": "FlashAttention",
      "description": "A proposed method that aims to make attention faster and more memory-efficient."
    },
    {
      "type": "Architecture",
      "name": "GPU",
      "description": "Graphics Processing Unit, used for high-performance computing tasks."
    },
    {
      "type": "Memory Type",
      "name": "SRAM",
      "description": "Static Random-Access Memory, a type of fast on-chip memory."
    },
    {
      "type": "Memory Type",
      "name": "HBM",
      "description": "High Bandwidth Memory, a type of relatively slow GPU memory."
    },
    {
      "type": "Architecture",
      "name": "PyTorch",
      "description": "An open-source machine learning library used for applications such as computer vision and natural language processing."
    },
    {
      "type": "Memory Type",
      "name": "DRAM",
      "description": "Dynamic Random-Access Memory, used as main memory in computing systems."
    },
    {
      "type": "Concept",
      "name": "IO-aware algorithms",
      "description": "Algorithms designed to optimize input/output operations, particularly in memory-bound tasks."
    },
    {
      "type": "Framework",
      "name": "TensorFlow",
      "description": "An open-source platform for machine learning that provides a comprehensive ecosystem for building and deploying models."
    },
    {
      "type": "Concept",
      "name": "Softmax Reduction",
      "description": "A technique used to compute the softmax function efficiently without accessing the entire input."
    },
    {
      "type": "Architecture",
      "name": "CUDA",
      "description": "A parallel computing platform and application programming interface model created by NVIDIA."
    },
    {
      "type": "Concept",
      "name": "Intermediate Attention Matrix",
      "description": "A large matrix used in the attention mechanism that is typically stored and accessed during computations."
    },
    {
      "type": "Concept",
      "name": "IO complexity",
      "description": "A measure of the input/output operations required by an algorithm."
    },
    {
      "type": "Concept",
      "name": "Standard Attention",
      "description": "A traditional attention mechanism used in various neural network architectures."
    },
    {
      "type": "Technique",
      "name": "Block-Sparse Flash Attention",
      "description": "A sparse attention algorithm that is 2-4 times faster than Flash Attention, designed for longer sequence lengths."
    },
    {
      "type": "Metric",
      "name": "IO Complexity",
      "description": "A measure of the input/output operations required, which is improved in block-sparse Flash Attention compared to Flash Attention."
    },
    {
      "type": "Concept",
      "name": "Approximate Attention Algorithms",
      "description": "Algorithms that aim to reduce memory access overhead but face challenges that Flash Attention can help overcome."
    },
    {
      "type": "Architecture",
      "name": "Multi-GPU",
      "description": "An architecture that can be extended to support attention mechanisms in a multi-GPU setup."
    },
    {
      "type": "Concept",
      "name": "Kernel Regression",
      "description": "A statistical technique that can be extended to work with the proposed attention mechanisms."
    },
    {
      "type": "Metric",
      "name": "Sparsity Ratio",
      "description": "A factor that influences the improvement in IO complexity of block-sparse Flash Attention."
    },
    {
      "type": "Dataset",
      "name": "Long-Range Arena",
      "description": "A benchmark for evaluating the performance of models on long-context tasks."
    },
    {
      "type": "Metric",
      "name": "perplexity",
      "description": "A measurement of how well a probability distribution predicts a sample."
    },
    {
      "type": "Architecture",
      "name": "Block-sparse Flash Attention",
      "description": "An extension of Flash Attention that allows scaling to even longer sequences."
    },
    {
      "type": "Model",
      "name": "Linformer",
      "description": "An approximate attention method that becomes faster for sequence lengths beyond 1K."
    },
    {
      "type": "Architecture",
      "name": "A100 GPU",
      "description": "A specific GPU model with high bandwidth memory and significant memory capacity."
    },
    {
      "type": "Concept",
      "name": "GPU Memory Hierarchy",
      "description": "The structure of memory types in GPUs, which affects performance based on size and speed."
    },
    {
      "type": "Concept",
      "name": "High Bandwidth Memory (HBM)",
      "description": "A type of memory with high data transfer rates, used in GPUs."
    },
    {
      "type": "Concept",
      "name": "On-chip SRAM",
      "description": "A small but very fast memory located on the GPU chip, significantly faster than HBM."
    },
    {
      "type": "Concept",
      "name": "Compute-bound operations",
      "description": "Operations that are limited by the computational power rather than memory access."
    },
    {
      "type": "Concept",
      "name": "Memory-bound operations",
      "description": "Operations that are limited by memory access speed rather than computational power."
    },
    {
      "type": "Concept",
      "name": "Compute-bound",
      "description": "Operations where the time taken is determined by the number of arithmetic operations, with minimal time spent accessing memory."
    },
    {
      "type": "Concept",
      "name": "Memory-bound",
      "description": "Operations where the time taken is determined by the number of memory accesses, with minimal time spent in computation."
    },
    {
      "type": "Technique",
      "name": "Kernel fusion",
      "description": "An approach to accelerate memory-bound operations by combining multiple operations into a single kernel."
    },
    {
      "type": "Metric",
      "name": "Arithmetic intensity",
      "description": "A measure of the number of arithmetic operations per byte of memory access, used to classify operations as compute-bound or memory-bound."
    },
    {
      "type": "Technique",
      "name": "Kernel Fusion",
      "description": "An approach to accelerate memory-bound operations by combining multiple operations applied to the same input to reduce memory access."
    },
    {
      "type": "Concept",
      "name": "Memory-Bound Operations",
      "description": "Operations that are limited by the speed of memory access rather than the speed of computation."
    },
    {
      "type": "Architecture",
      "name": "Standard Attention Implementation",
      "description": "The conventional approach to compute attention outputs using matrices S and P."
    },
    {
      "type": "Metric",
      "name": "Memory Complexity",
      "description": "The amount of memory required, which is O(N^2) for standard attention implementations."
    },
    {
      "type": "Concept",
      "name": "Masking",
      "description": "An operation applied to the attention matrix to control which elements are considered."
    },
    {
      "type": "Concept",
      "name": "Softmax",
      "description": "A mathematical function used to normalize the attention scores."
    },
    {
      "type": "Concept",
      "name": "Block-Sparse Attention",
      "description": "An extension of Flash Attention that handles attention mechanisms with sparse data."
    },
    {
      "type": "Metric",
      "name": "Wall-Clock Time",
      "description": "The actual time taken to execute the attention algorithm, which Flash Attention aims to minimize."
    },
    {
      "type": "Technique",
      "name": "Tiling",
      "description": "A method to compute attention by splitting inputs into blocks to reduce memory access."
    },
    {
      "type": "Technique",
      "name": "Recomputation",
      "description": "A technique used to overcome challenges in computing exact attention with reduced memory access."
    },
    {
      "type": "Concept",
      "name": "Attention Output",
      "description": "The result of the attention mechanism computed from inputs Q, K, and V."
    },
    {
      "type": "Metric",
      "name": "Sub-quadratic HBM accesses",
      "description": "A performance goal aimed at reducing the number of high-bandwidth memory accesses."
    },
    {
      "type": "Technique",
      "name": "softmax with scaling",
      "description": "A numerical stability technique used in the computation of softmax for vectors."
    },
    {
      "type": "Metric",
      "name": "numerical stability",
      "description": "A property that ensures the softmax function behaves predictably under various input conditions."
    },
    {
      "type": "Function",
      "name": "softmax",
      "description": "A mathematical function that converts a vector of values into a probability distribution."
    },
    {
      "type": "Function",
      "name": "max",
      "description": "A function that returns the maximum value from a set of values."
    },
    {
      "type": "Function",
      "name": "exponential function",
      "description": "A mathematical function that raises Euler's number to the power of a given input."
    },
    {
      "type": "Architecture",
      "name": "Q-K-V",
      "description": "Query-Key-Value architecture used in attention mechanisms."
    },
    {
      "type": "Technique",
      "name": "Selective Gradient Checkpointing",
      "description": "A method to reduce the maximum amount of memory required during training by selectively storing certain outputs."
    },
    {
      "type": "Architecture",
      "name": "on-chip SRAM",
      "description": "Static Random Access Memory used for temporary storage during computations."
    },
    {
      "type": "Metric",
      "name": "block sizes",
      "description": "Sizes of blocks used for dividing matrices Q, K, and V for efficient processing."
    },
    {
      "type": "Metric",
      "name": "S",
      "description": "Matrix computed on-chip as the product of Q and the transpose of K."
    },
    {
      "type": "Metric",
      "name": "m",
      "description": "A metric representing the maximum values computed during the attention process."
    },
    {
      "type": "Metric",
      "name": "\u2113",
      "description": "A metric representing the sum of probabilities computed during the attention process."
    },
    {
      "type": "Concept",
      "name": "Algebraic Aggregation",
      "description": "A style of aggregation mentioned in the context of the algorithm."
    },
    {
      "type": "Metric",
      "name": "GFLOPs",
      "description": "A measure of computational performance, indicating the number of billion floating-point operations per second."
    },
    {
      "type": "Metric",
      "name": "HBM R/W (GB)",
      "description": "The amount of high-bandwidth memory read/write operations in gigabytes."
    },
    {
      "type": "Metric",
      "name": "Runtime (ms)",
      "description": "The time taken to execute a specific operation, measured in milliseconds."
    },
    {
      "type": "Technique",
      "name": "block-sparse Flash Attention",
      "description": "An optimized version of Flash Attention that improves speed based on the sparsity of the data."
    },
    {
      "type": "Metric",
      "name": "HBM accesses",
      "description": "The number of accesses to High Bandwidth Memory, which is a critical factor in the performance of attention mechanisms."
    },
    {
      "type": "Metric",
      "name": "sequence length (N)",
      "description": "The length of the input sequence processed by the attention mechanism."
    },
    {
      "type": "Metric",
      "name": "head dimension (d)",
      "description": "The dimensionality of each attention head in the attention mechanism."
    },
    {
      "type": "Metric",
      "name": "size of SRAM (M)",
      "description": "The size of the SRAM used in the attention mechanism, which affects performance."
    },
    {
      "type": "Concept",
      "name": "Exact Attention",
      "description": "A method of computing attention that ensures precise results."
    },
    {
      "type": "Concept",
      "name": "Lower-bound",
      "description": "A theoretical limit on the number of HBM accesses for computing exact attention."
    },
    {
      "type": "Variable",
      "name": "N",
      "description": "Sequence length in the context of attention mechanisms."
    },
    {
      "type": "Variable",
      "name": "d",
      "description": "Head dimension in the context of attention mechanisms."
    },
    {
      "type": "Variable",
      "name": "M",
      "description": "Size of SRAM used in the computation of attention."
    },
    {
      "type": "Concept",
      "name": "FLOP count",
      "description": "Floating Point Operations, a measure of computational complexity."
    },
    {
      "type": "Concept",
      "name": "block size",
      "description": "The size of the blocks used in Flash Attention, affecting HBM accesses and runtime."
    },
    {
      "type": "Concept",
      "name": "parameterized complexity",
      "description": "A complexity measure that considers parameters in algorithm analysis."
    },
    {
      "type": "Literature",
      "name": "streaming algorithms",
      "description": "A field of algorithms that process data streams and often discuss lower bounds."
    },
    {
      "type": "Matrix",
      "name": "Mask matrix (~M)",
      "description": "A binary matrix used to control which elements are considered in the attention computation."
    },
    {
      "type": "Matrix",
      "name": "Q, K, V",
      "description": "Input matrices representing queries (Q), keys (K), and values (V) in the attention mechanism."
    },
    {
      "type": "Matrix",
      "name": "P",
      "description": "The softmax-normalized score matrix."
    },
    {
      "type": "Matrix",
      "name": "O",
      "description": "The output matrix computed from the product of P and V."
    },
    {
      "type": "Concept",
      "name": "Block sparsity mask",
      "description": "A predefined mask that indicates which blocks of the attention matrix are nonzero."
    },
    {
      "type": "Metric",
      "name": "\u0398(\ud835\udc41\ud835\udc51)",
      "description": "A notation representing the complexity of the algorithm in terms of sequence length and head dimension."
    },
    {
      "type": "Concept",
      "name": "Sparsity",
      "description": "The property of having a significant number of zero elements in a matrix, which can improve computational efficiency."
    },
    {
      "type": "Technique",
      "name": "Butterfly sparsity",
      "description": "A specific type of fixed sparsity used in downstream experiments."
    },
    {
      "type": "Model",
      "name": "Megatron",
      "description": "A large-scale transformer model designed for training on multiple GPUs."
    },
    {
      "type": "Dataset",
      "name": "LRA benchmark",
      "description": "A benchmark used to evaluate the performance of models on long-range dependencies."
    },
    {
      "type": "Metric",
      "name": "Training Speed",
      "description": "A measure of how quickly a model can be trained."
    },
    {
      "type": "Metric",
      "name": "Attention Runtime",
      "description": "The time taken to compute attention in a model."
    },
    {
      "type": "Metric",
      "name": "Memory Benchmarks",
      "description": "Measurements of memory usage during model training."
    },
    {
      "type": "Benchmark",
      "name": "Long-Range Arena (LRA)",
      "description": "A benchmark used to evaluate the performance of Flash Attention."
    },
    {
      "type": "Dataset",
      "name": "Wikipedia",
      "description": "A large-scale dataset used for training the BERT model."
    },
    {
      "type": "Metric",
      "name": "Memory Footprint",
      "description": "The amount of memory used by the attention mechanism, which scales with sequence length."
    },
    {
      "type": "Metric",
      "name": "Runtime",
      "description": "The time taken to execute the attention mechanism."
    },
    {
      "type": "Dataset",
      "name": "Open Webtext",
      "description": "A dataset used for training language models, derived from web pages."
    },
    {
      "type": "Architecture",
      "name": "Nvidia MLPerf 1.1",
      "description": "A benchmark for measuring the performance of machine learning hardware and software."
    },
    {
      "type": "Implementation",
      "name": "Hugging Face",
      "description": "A widely used library for natural language processing tasks."
    },
    {
      "type": "Model",
      "name": "GPT-2 small",
      "description": "A smaller variant of the GPT-2 model used for natural language processing tasks."
    },
    {
      "type": "Model",
      "name": "GPT-2 medium",
      "description": "A medium variant of the GPT-2 model used for natural language processing tasks."
    },
    {
      "type": "Model",
      "name": "Huggingface",
      "description": "An implementation of the GPT-2 model used as a baseline for comparison."
    },
    {
      "type": "Dataset",
      "name": "Open Web Text",
      "description": "A dataset used for training language models, particularly GPT-2."
    },
    {
      "type": "Model",
      "name": "Linear Attention",
      "description": "An attention mechanism that reduces the computational complexity of standard attention."
    },
    {
      "type": "Model",
      "name": "Performer",
      "description": "A model that uses kernelized attention for efficiency."
    },
    {
      "type": "Model",
      "name": "Local Attention",
      "description": "An attention mechanism that focuses on local context."
    },
    {
      "type": "Model",
      "name": "Reformer",
      "description": "A model that uses reversible layers and locality-sensitive hashing for efficiency."
    },
    {
      "type": "Model",
      "name": "Smyrf",
      "description": "A model that employs a memory-efficient attention mechanism."
    },
    {
      "type": "Dataset",
      "name": "MIMIC-III",
      "description": "A dataset containing intensive care unit patient discharge summaries, annotated with multiple labels."
    },
    {
      "type": "Dataset",
      "name": "ECt HR",
      "description": "A dataset containing legal cases used for training models in long document classification."
    },
    {
      "type": "Metric",
      "name": "LRA accuracy",
      "description": "A measure of accuracy that is highly dependent on the tuning procedure."
    },
    {
      "type": "Dataset",
      "name": "unit patient discharge summaries",
      "description": "Summaries annotated with multiple labels."
    },
    {
      "type": "Technique",
      "name": "PyTorch Attention",
      "description": "The standard attention mechanism implemented in the PyTorch framework."
    },
    {
      "type": "Technique",
      "name": "Megatron Attention",
      "description": "An attention mechanism designed for large-scale models, part of the Megatron framework."
    },
    {
      "type": "Technique",
      "name": "Linformer Attention",
      "description": "An attention mechanism that approximates the full attention with linear complexity."
    },
    {
      "type": "Technique",
      "name": "OpenAI Sparse Attention",
      "description": "A sparse attention mechanism developed by OpenAI."
    },
    {
      "type": "Dataset",
      "name": "MIMIC",
      "description": "A dataset containing long text documents, with an average of 2,395 tokens."
    },
    {
      "type": "Dataset",
      "name": "European Court of Human Rights (ECtHR)",
      "description": "A dataset containing legal documents with an average of 2,197 tokens."
    },
    {
      "type": "Metric",
      "name": "micro F1",
      "description": "A performance metric used to evaluate the effectiveness of models on classification tasks."
    },
    {
      "type": "Model",
      "name": "SMYRF",
      "description": "A model that utilizes a memory-efficient attention mechanism."
    },
    {
      "type": "Dataset",
      "name": "Path-64",
      "description": "A dataset used for pretraining the transformer model."
    },
    {
      "type": "Technique",
      "name": "Sparse Attention",
      "description": "Attention mechanisms that use sparsity to reduce computational costs."
    },
    {
      "type": "Technique",
      "name": "IO-aware implementations",
      "description": "An approach to optimize attention mechanisms in deep learning by considering input/output operations."
    },
    {
      "type": "Technique",
      "name": "CUDA kernel",
      "description": "A low-level programming construct used to implement parallel computing on NVIDIA GPUs."
    },
    {
      "type": "Architecture",
      "name": "Deep Network",
      "description": "A neural network with multiple layers that can learn complex representations."
    },
    {
      "type": "Concept",
      "name": "High-level language",
      "description": "Programming languages that are more abstract and easier to use than low-level languages."
    },
    {
      "type": "Technique",
      "name": "Halide",
      "description": "A programming language designed for high-performance image processing."
    },
    {
      "type": "Concept",
      "name": "IO-Aware Deep Learning",
      "description": "An approach that considers input/output efficiency in deep learning computations."
    },
    {
      "type": "Technique",
      "name": "IO-aware implementation of attention",
      "description": "An optimized method for computing attention in Transformers that is efficient in terms of IO."
    },
    {
      "type": "Technique",
      "name": "Multi-GPU IO-Aware Methods",
      "description": "Methods that utilize multiple GPUs for computing attention while considering data transfer efficiency."
    },
    {
      "type": "Model",
      "name": "FMHA",
      "description": "Fast Multi-Head Attention, a code implementation used as a starting point for the authors' work."
    },
    {
      "type": "Funding Agency",
      "name": "NIH",
      "description": "National Institutes of Health, a funding agency supporting research."
    },
    {
      "type": "Funding Agency",
      "name": "NSF",
      "description": "National Science Foundation, a funding agency supporting scientific research."
    },
    {
      "type": "Funding Agency",
      "name": "ARL",
      "description": "Army Research Laboratory, a funding agency supporting research in military applications."
    },
    {
      "type": "Funding Agency",
      "name": "ONR",
      "description": "Office of Naval Research, a funding agency supporting research in naval applications."
    },
    {
      "type": "Architecture",
      "name": "IO-Aware Architecture",
      "description": "An architecture designed to optimize input/output operations in attention mechanisms."
    },
    {
      "type": "Funding Source",
      "name": "Department of Defense (DoD)",
      "description": "U.S. government department providing funding through the NDSEG Program."
    },
    {
      "type": "Project",
      "name": "Stanford DAWN project",
      "description": "A project involving collaboration with various tech companies for research."
    },
    {
      "type": "Grant",
      "name": "NSF grant CCF-1763481",
      "description": "Research support for Atri Rudra's work."
    },
    {
      "type": "Model",
      "name": "Lambda Networks",
      "description": "A model for modeling long-range interactions without attention."
    },
    {
      "type": "Model",
      "name": "Longformer",
      "description": "A transformer model designed for long-document processing."
    },
    {
      "type": "Metric",
      "name": "Memory Efficiency",
      "description": "A measure of how effectively a model uses memory resources."
    },
    {
      "type": "Metric",
      "name": "Speed",
      "description": "A measure of how quickly a model can process input data."
    },
    {
      "type": "Technique",
      "name": "Low-Rank Attention",
      "description": "An approach that approximates attention mechanisms using low-rank matrix factorization."
    },
    {
      "type": "Model",
      "name": "Transformer-XL",
      "description": "An extension of the Transformer model that allows for longer context handling."
    },
    {
      "type": "Dataset",
      "name": "Long Document Classification",
      "description": "A task involving the classification of lengthy documents using transformer-based models."
    },
    {
      "type": "Model",
      "name": "Butterfly Factorizations",
      "description": "A method for learning fast algorithms for linear transforms."
    },
    {
      "type": "Model",
      "name": "Kaleidoscope",
      "description": "An efficient, learnable representation for all structured linear maps."
    },
    {
      "type": "Model",
      "name": "Pixelated Butterfly",
      "description": "A method for simple and efficient sparse training for neural network models."
    },
    {
      "type": "Model",
      "name": "Monarch",
      "description": "Expressive structured matrices for efficient and accurate computations."
    },
    {
      "type": "Concept",
      "name": "Structured Matrices",
      "description": "Matrices that are organized in a way to improve efficiency and accuracy in training."
    },
    {
      "type": "Technique",
      "name": "Asymmetric Clustering",
      "description": "A method used for efficient attention mechanisms."
    },
    {
      "type": "Architecture",
      "name": "Deep Bidirectional Transformers",
      "description": "A neural network architecture that processes data in both directions for better context understanding."
    },
    {
      "type": "Technique",
      "name": "Layer-wise Optimal Brain Surgeon",
      "description": "A technique for learning to prune deep neural networks."
    },
    {
      "type": "Model",
      "name": "Transformers for Image Recognition",
      "description": "A model that applies transformers for image recognition tasks at scale."
    },
    {
      "type": "Technique",
      "name": "Fast Geometric Learning",
      "description": "A technique for efficient geometric learning using symbolic matrices."
    },
    {
      "type": "Model",
      "name": "Lottery Ticket Hypothesis",
      "description": "A hypothesis that suggests sparse, trainable neural networks can be found within larger networks."
    },
    {
      "type": "Concept",
      "name": "Parameterized Complexity Theory",
      "description": "A framework for analyzing the complexity of problems based on parameters."
    },
    {
      "type": "Dataset",
      "name": "OpenWebText Corpus",
      "description": "A dataset used for training language models, derived from web text."
    },
    {
      "type": "Model",
      "name": "State-Space Models",
      "description": "A class of models used for audio generation, mentioned in the context of a related work."
    },
    {
      "type": "Model",
      "name": "Hippo",
      "description": "A recurrent memory model with optimal polynomial projections."
    },
    {
      "type": "Model",
      "name": "Structured State Space Layers",
      "description": "A model that combines recurrent, convolutional, and continuous-time models."
    },
    {
      "type": "Architecture",
      "name": "Memory hierarchy",
      "description": "A design approach for organizing memory in computer architecture."
    },
    {
      "type": "Concept",
      "name": "Deep compression",
      "description": "A technique for compressing deep neural networks using pruning, quantization, and coding."
    },
    {
      "type": "Concept",
      "name": "Structured state spaces",
      "description": "A method for efficiently modeling long sequences in neural networks."
    },
    {
      "type": "Architecture",
      "name": "Ampere GPU",
      "description": "A GPU architecture analyzed for performance via microbenchmarking."
    },
    {
      "type": "Architecture",
      "name": "NVIDIA Volta GPU",
      "description": "A GPU architecture dissected through microbenchmarking."
    },
    {
      "type": "Architecture",
      "name": "Graphcore IPU",
      "description": "An architecture for processing that has been analyzed via microbenchmarking."
    },
    {
      "type": "Technique",
      "name": "Data Movement Optimization",
      "description": "A technique focused on optimizing data movement in transformer models."
    },
    {
      "type": "Architecture",
      "name": "Tensor Processing Unit (TPU)",
      "description": "A type of application-specific integrated circuit (ASIC) developed by Google for accelerating machine learning workloads."
    },
    {
      "type": "Architecture",
      "name": "RNN",
      "description": "Recurrent Neural Networks, a class of neural networks for processing sequential data."
    },
    {
      "type": "Model",
      "name": "Albert",
      "description": "A lite BERT for self-supervised learning of language representations."
    },
    {
      "type": "Technique",
      "name": "Runtime Neural Pruning",
      "description": "A technique for optimizing neural networks by removing unnecessary parameters."
    },
    {
      "type": "Concept",
      "name": "Deep Learning Compiler",
      "description": "A comprehensive survey on compilers designed for deep learning applications."
    },
    {
      "type": "Model",
      "name": "Performers",
      "description": "A model that uses kernelized attention mechanisms to improve efficiency."
    },
    {
      "type": "Model",
      "name": "Luna",
      "description": "A linear unified nested attention model."
    },
    {
      "type": "Benchmark",
      "name": "MLPerf",
      "description": "A benchmark for measuring the performance of machine learning hardware, software, and services."
    },
    {
      "type": "Architecture",
      "name": "Nvidia Tesla V100",
      "description": "A GPU architecture developed by NVIDIA, released in 2017."
    },
    {
      "type": "Architecture",
      "name": "Nvidia A100",
      "description": "A tensor core GPU architecture developed by NVIDIA, released in 2020."
    },
    {
      "type": "Architecture",
      "name": "Nvidia H100",
      "description": "A tensor core GPU architecture developed by NVIDIA, released in 2022."
    },
    {
      "type": "Dataset",
      "name": "Mlperf",
      "description": "A benchmark for measuring machine learning performance."
    },
    {
      "type": "Technique",
      "name": "Online normalizer calculation for softmax",
      "description": "A method for calculating normalizers in softmax functions."
    },
    {
      "type": "Paper",
      "name": "Self-attention does not need O(n^2) memory",
      "description": "A paper discussing memory efficiency in self-attention mechanisms."
    },
    {
      "type": "Paper",
      "name": "Language models are unsupervised multitask learners",
      "description": "A paper that presents the capabilities of language models in multitasking."
    },
    {
      "type": "Conference",
      "name": "Annual Meeting of the Association for Computational Linguistics",
      "description": "A conference where research in computational linguistics is presented."
    },
    {
      "type": "Model",
      "name": "Compressive Transformers",
      "description": "A model designed for long-range sequence modeling, introduced by Rae et al. in 2020."
    },
    {
      "type": "Conference",
      "name": "International Conference on Learning Representations (ICLR)",
      "description": "A conference where research on learning representations is presented."
    },
    {
      "type": "Dataset",
      "name": "Large-scale Matrix Completion",
      "description": "A dataset used for evaluating parallel stochastic gradient algorithms."
    },
    {
      "type": "Technique",
      "name": "Sparse Computation",
      "description": "A method that reduces computational costs by focusing on non-zero elements in data."
    },
    {
      "type": "Model",
      "name": "Routing Transformers",
      "description": "A model that utilizes efficient content-based sparse attention."
    },
    {
      "type": "Technique",
      "name": "Movement Pruning",
      "description": "An adaptive sparsity technique achieved through fine-tuning."
    },
    {
      "type": "Technique",
      "name": "Structured Transforms",
      "description": "Techniques for small-footprint deep learning."
    },
    {
      "type": "Dataset",
      "name": "Long Range Arena",
      "description": "A benchmark for evaluating the efficiency of transformers."
    },
    {
      "type": "Metric",
      "name": "Adaptive Attention Span",
      "description": "A method for adjusting the attention span in transformers."
    },
    {
      "type": "Model",
      "name": "DeepNet",
      "description": "A model that scales transformers to 1,000 layers."
    },
    {
      "type": "Metric",
      "name": "Roo\ufb02ine Model",
      "description": "A visual performance model for multicore architectures."
    },
    {
      "type": "Technique",
      "name": "Data Locality Optimization",
      "description": "An algorithm designed to optimize data locality in programming."
    },
    {
      "type": "Metric",
      "name": "Optimal space lower bounds",
      "description": "A theoretical measure for space efficiency in algorithms."
    },
    {
      "type": "Technique",
      "name": "Lightweight and dynamic convolutions",
      "description": "A method to reduce computational cost in attention mechanisms."
    },
    {
      "type": "Model",
      "name": "Nystr\u00f6mformer",
      "description": "A nyst\u00f6m-based algorithm for approximating self-attention."
    },
    {
      "type": "Model",
      "name": "Tokens-to-token vit",
      "description": "A model for training vision transformers from scratch on ImageNet."
    },
    {
      "type": "Model",
      "name": "Big Bird",
      "description": "A transformer model designed for processing longer sequences."
    },
    {
      "type": "Conference",
      "name": "AAAI Conference on Artificial Intelligence",
      "description": "A conference focused on advancements in artificial intelligence."
    },
    {
      "type": "Conference",
      "name": "IEEE/CVF International Conference on Computer Vision",
      "description": "A conference dedicated to computer vision research."
    },
    {
      "type": "Model",
      "name": "Attention Free Transformer",
      "description": "A transformer model that operates without traditional attention mechanisms."
    },
    {
      "type": "Model",
      "name": "Long-Short Transformer",
      "description": "Efficient transformers designed for both language and vision tasks."
    },
    {
      "type": "Concept",
      "name": "IO-Aware Runtime Optimization",
      "description": "Optimizing for reading and writing to fast/slow memory."
    },
    {
      "type": "Concept",
      "name": "I/O Complexity",
      "description": "Analyzing the complexity of input/output operations in computing."
    },
    {
      "type": "Concept",
      "name": "Memory Hierarchies",
      "description": "The organization of memory in a system, affecting performance."
    },
    {
      "type": "Concept",
      "name": "Working Set Model",
      "description": "A model that describes the set of data that a process is currently using."
    },
    {
      "type": "Concept",
      "name": "Data Locality",
      "description": "The principle of accessing data that is close to the processor to improve performance."
    },
    {
      "type": "Concept",
      "name": "Scalability",
      "description": "The capability of a system to handle a growing amount of work."
    },
    {
      "type": "Metric",
      "name": "Computational Complexity",
      "description": "A measure of the amount of computational resources required for an algorithm."
    },
    {
      "type": "Concept",
      "name": "Butterfly Matrices",
      "description": "Matrices that can express any structured matrices with almost optimal runtime and number of parameters."
    },
    {
      "type": "Concept",
      "name": "Toeplitz-like Matrices",
      "description": "A class of structured matrices proposed in machine learning."
    },
    {
      "type": "Concept",
      "name": "Low-Displacement Rank Matrices",
      "description": "Another class of structured matrices proposed in machine learning."
    },
    {
      "type": "Concept",
      "name": "Quasi-Separable Matrices",
      "description": "A class of structured matrices proposed in machine learning."
    },
    {
      "type": "Technique",
      "name": "Fast Transforms",
      "description": "Transforms commonly encountered in signal processing, such as Fourier and Chebyshev transforms."
    },
    {
      "type": "Concept",
      "name": "Sparse Training",
      "description": "A method of training models that involves using sparse representations to improve efficiency."
    },
    {
      "type": "Concept",
      "name": "Lottery Tickets Hypothesis",
      "description": "A hypothesis suggesting that smaller sub-networks can perform as well as larger dense networks."
    },
    {
      "type": "Concept",
      "name": "Hardware Lottery",
      "description": "The phenomenon where the performance of algorithms can vary significantly based on hardware optimizations."
    },
    {
      "type": "Model",
      "name": "Scatterbrain",
      "description": "A model that uses sparse attention techniques to manage long-range dependencies."
    },
    {
      "type": "Model",
      "name": "Long-short transformer",
      "description": "A model designed to handle long sequences with improved efficiency."
    },
    {
      "type": "Model",
      "name": "Combiner",
      "description": "A model that integrates various attention mechanisms for better performance."
    },
    {
      "type": "Dataset",
      "name": "Long-range Arena",
      "description": "A benchmark for evaluating the performance of attention mechanisms on long sequences."
    },
    {
      "type": "Model",
      "name": "Compressive Transformer",
      "description": "A model that compresses information to extend the context length in sequence processing."
    },
    {
      "type": "Technique",
      "name": "Hi PPO",
      "description": "A technique that projects history on a polynomial basis for accurate reconstruction through state-space models."
    },
    {
      "type": "Model",
      "name": "S 4",
      "description": "An extension of Hi PPO that combines strengths of CNNs, RNNs, and continuous models for efficient training and inference."
    },
    {
      "type": "Model",
      "name": "AFT",
      "description": "A model that is part of the attempts to replace attention in image classification and language modeling."
    },
    {
      "type": "Model",
      "name": "FLASH",
      "description": "A model that is part of the attempts to replace attention in image classification and language modeling."
    },
    {
      "type": "Technique",
      "name": "Softmax normalization",
      "description": "A technique used to decouple the columns of K and V in attention computation."
    },
    {
      "type": "Metric",
      "name": "Memory footprint",
      "description": "A measure of the amount of memory used during the computation."
    },
    {
      "type": "Architecture",
      "name": "Gradient Checkpointing",
      "description": "A technique used to reduce memory usage during the backward pass of neural network training."
    },
    {
      "type": "Concept",
      "name": "Backward Pass",
      "description": "The phase in neural network training where gradients are computed for optimization."
    },
    {
      "type": "Metric",
      "name": "Loss Function",
      "description": "A scalar function that measures the difference between predicted and actual outcomes."
    },
    {
      "type": "Concept",
      "name": "Input Gradients",
      "description": "Gradients computed with respect to the input variables of the model."
    },
    {
      "type": "Concept",
      "name": "Output Gradient",
      "description": "The gradient of the loss function with respect to the output of the model."
    },
    {
      "type": "Concept",
      "name": "Reverse-mode Autodiff",
      "description": "A method for computing gradients efficiently using the chain rule."
    },
    {
      "type": "Concept",
      "name": "Softmax Function",
      "description": "A function that converts a vector of values into a probability distribution."
    },
    {
      "type": "Concept",
      "name": "Jacobian",
      "description": "A matrix of all first-order partial derivatives of a vector-valued function."
    },
    {
      "type": "Concept",
      "name": "Pointwise Multiplication",
      "description": "An operation used in the context of attention mechanisms."
    },
    {
      "type": "Variable",
      "name": "n",
      "description": "Number of input sequences."
    },
    {
      "type": "Concept",
      "name": "Input Sequences",
      "description": "The sequences of queries (Q), keys (K), and values (V) used in the attention mechanism."
    },
    {
      "type": "Metric",
      "name": "Softmax Scaling",
      "description": "A scaling factor applied during the softmax operation, typically 1/sqrt(d)."
    },
    {
      "type": "Technique",
      "name": "Masking Function",
      "description": "A function that modifies input sequences to handle variable lengths by padding."
    },
    {
      "type": "Algorithm",
      "name": "Algorithm 2",
      "description": "The full algorithm for the Flash Attention forward pass."
    },
    {
      "type": "Metric",
      "name": "softmax scaling constant",
      "description": "A constant used to scale the softmax function in the attention mechanism."
    },
    {
      "type": "Concept",
      "name": "masking function",
      "description": "A function used to apply a mask to the attention scores."
    },
    {
      "type": "Metric",
      "name": "dropout probability",
      "description": "The probability of dropping units during training to prevent overfitting."
    },
    {
      "type": "Metric",
      "name": "Computational Speed",
      "description": "A measure of how quickly a model can perform computations."
    },
    {
      "type": "Algorithm",
      "name": "Standard Attention Backward Pass",
      "description": "An algorithm that describes the process of computing gradients in the attention mechanism."
    },
    {
      "type": "Concept",
      "name": "Dropout Mask",
      "description": "A mask used during training to prevent overfitting by randomly setting a fraction of input units to zero."
    },
    {
      "type": "Concept",
      "name": "Pseudo-Random Number Generator",
      "description": "A generator used to create random numbers, which can be used to regenerate the dropout mask."
    },
    {
      "type": "Metric",
      "name": "Memory Usage",
      "description": "The amount of memory required during the computation, specifically noted as O(1N) extra memory in the context."
    },
    {
      "type": "Concept",
      "name": "Softmax Gradient",
      "description": "A gradient computation used in the context of neural networks, particularly in relation to the softmax function."
    },
    {
      "type": "Equation",
      "name": "Eq. (4)",
      "description": "An equation referenced for computing the softmax gradient."
    },
    {
      "type": "Concept",
      "name": "Dot Product",
      "description": "A mathematical operation used to compute the similarity between two vectors."
    },
    {
      "type": "Algorithm",
      "name": "Flash Attention Backward Pass",
      "description": "An algorithm that implements the backward pass for the Flash Attention mechanism."
    },
    {
      "type": "Matrix",
      "name": "Q-K-V-O",
      "description": "Matrices representing query (Q), key (K), value (V), and output (O) in the attention mechanism."
    },
    {
      "type": "Vector",
      "name": "\u2113-m",
      "description": "Vectors used in the Flash Attention algorithm, stored in high-bandwidth memory (HBM)."
    },
    {
      "type": "Constant",
      "name": "softmax scaling constant (\u03c4)",
      "description": "A constant used for scaling in the softmax function within the attention mechanism."
    },
    {
      "type": "Parameter",
      "name": "dropout probability (p_drop)",
      "description": "A parameter that defines the dropout rate used during training to prevent overfitting."
    },
    {
      "type": "State",
      "name": "pseudo-random number generator state (R)",
      "description": "State from the forward pass used to initialize the pseudo-random number generator."
    },
    {
      "type": "Metric",
      "name": "dropout mask",
      "description": "A mask used to prevent overfitting by randomly dropping units during training."
    },
    {
      "type": "Concept",
      "name": "IO-complexity",
      "description": "A measure of the input/output operations required during computation."
    },
    {
      "type": "Concept",
      "name": "Forward Pass",
      "description": "The phase in neural network training where inputs are processed to produce outputs."
    },
    {
      "type": "Theorem",
      "name": "Theorem 2",
      "description": "Analyzes the IO-complexity of the forward pass."
    },
    {
      "type": "Theorem",
      "name": "Theorem 5",
      "description": "Analyzes the IO-complexity of the backward pass."
    },
    {
      "type": "Technique",
      "name": "Rabe and Staats Algorithm",
      "description": "An algorithm that focuses on reducing the total memory footprint in attention mechanisms."
    },
    {
      "type": "Metric",
      "name": "Memory Accesses",
      "description": "The number of memory reads/writes, which is a primary factor determining runtime."
    },
    {
      "type": "Technique",
      "name": "Rabe and Staats",
      "description": "An alternative method for attention that summarizes information from blocks using temporary outputs and softmax normalization statistics."
    },
    {
      "type": "Metric",
      "name": "Memory Requirement",
      "description": "The total amount of memory needed for an operation, which is influenced by the number of memory accesses."
    },
    {
      "type": "Algorithm",
      "name": "Algorithm 1",
      "description": "An algorithm that outlines the steps for computing attention in the FlashAttention technique."
    },
    {
      "type": "Model",
      "name": "Q",
      "description": "Query matrix used in attention mechanisms."
    },
    {
      "type": "Model",
      "name": "K",
      "description": "Key matrix used in attention mechanisms."
    },
    {
      "type": "Metric",
      "name": "rowmax",
      "description": "A metric used to compute the maximum value in a row of a matrix."
    },
    {
      "type": "Metric",
      "name": "rowsum",
      "description": "A metric used to compute the sum of values in a row of a matrix."
    },
    {
      "type": "Concept",
      "name": "Row-max",
      "description": "The maximum value in a specific row of a matrix, used for normalization in attention calculations."
    },
    {
      "type": "Concept",
      "name": "Slice",
      "description": "A subset of a matrix or tensor, defined by specific rows and columns."
    },
    {
      "type": "Algorithm",
      "name": "Algorithm 0",
      "description": "The algorithm referenced for computing the attention mechanism."
    },
    {
      "type": "Concept",
      "name": "global memory",
      "description": "Memory that is accessible globally, used for reading inputs in standard attention implementation."
    },
    {
      "type": "Concept",
      "name": "on-chip memory",
      "description": "Memory that is integrated into the chip, used for storing blocks of K and V."
    },
    {
      "type": "Variable",
      "name": "T_c",
      "description": "Represents the number of passes over Q and O in the streaming attention process."
    },
    {
      "type": "Variable",
      "name": "B_c",
      "description": "Block size for K."
    },
    {
      "type": "Variable",
      "name": "B_r",
      "description": "Block size for V."
    },
    {
      "type": "Concept",
      "name": "Block Sizes",
      "description": "Parameters that determine the size of blocks K_j, V_j, and Q_i in the context of memory efficiency."
    },
    {
      "type": "Metric",
      "name": "On-chip Memory",
      "description": "Memory that is directly accessible by the processor, crucial for the performance of attention mechanisms."
    },
    {
      "type": "Architecture",
      "name": "HBM (High Bandwidth Memory)",
      "description": "A type of memory architecture that provides high data transfer rates."
    },
    {
      "type": "Concept",
      "name": "Proposition 3",
      "description": "A theoretical statement regarding the number of HBM accesses in attention computation."
    },
    {
      "type": "Metric",
      "name": "HBM Accesses",
      "description": "The number of accesses to High Bandwidth Memory during the attention computation."
    },
    {
      "type": "Variable",
      "name": "Tc",
      "description": "Represents the number of passes over the input data."
    },
    {
      "type": "Variable",
      "name": "Bc",
      "description": "Represents the block size for computation."
    },
    {
      "type": "Variable",
      "name": "Br",
      "description": "Represents the reduced block size."
    },
    {
      "type": "Metric",
      "name": "Block sizes",
      "description": "Constraints on the sizes of blocks used in the attention mechanism."
    },
    {
      "type": "Dataset",
      "name": "Matrices Q, K, V",
      "description": "Input matrices representing queries, keys, and values in the attention mechanism."
    },
    {
      "type": "Metric",
      "name": "block-sparsity mask",
      "description": "A representation indicating which blocks of data are nonzero in the context of attention mechanisms."
    },
    {
      "type": "Architecture",
      "name": "Multi-GPU Attention",
      "description": "An approach to distribute attention computation across multiple GPUs for large language models."
    },
    {
      "type": "Concept",
      "name": "Sparse weight matrices",
      "description": "Weight matrices in MLP layers that are not fully populated, aimed at improving efficiency."
    },
    {
      "type": "Concept",
      "name": "IO-awareness",
      "description": "An implementation consideration that takes input/output operations into account to improve performance."
    },
    {
      "type": "Concept",
      "name": "Attention matrix",
      "description": "A matrix that represents the attention scores between elements in a sequence."
    },
    {
      "type": "Model",
      "name": "Kernel machine learning",
      "description": "A machine learning approach that utilizes kernel functions for various tasks."
    },
    {
      "type": "Concept",
      "name": "Low-rank matrix",
      "description": "A matrix that can be approximated by a product of two smaller matrices, reducing computational complexity."
    },
    {
      "type": "Optimizer",
      "name": "LAMB",
      "description": "An optimizer used in training BERT-large with specific hyperparameters."
    },
    {
      "type": "Library",
      "name": "Ke Ops",
      "description": "A library that optimizes kernel operations by reducing memory reads/writes."
    },
    {
      "type": "Concept",
      "name": "Kernel Machine Learning",
      "description": "A machine learning paradigm that uses kernel functions to measure similarity between data points."
    },
    {
      "type": "Matrix",
      "name": "Attention Matrix",
      "description": "A matrix that represents the attention scores between different elements in a sequence."
    },
    {
      "type": "Matrix",
      "name": "Kernel Matrix",
      "description": "A matrix that represents the similarity between data points in kernel machine learning."
    },
    {
      "type": "Technique",
      "name": "LAMB optimizer",
      "description": "An optimizer used for training the model with a specific learning rate."
    },
    {
      "type": "Dataset",
      "name": "MLPerf 1.1",
      "description": "A benchmark dataset used for training and validation, providing a reference implementation."
    },
    {
      "type": "Concept",
      "name": "FP 16 precision",
      "description": "A precision format used during training to improve performance and memory efficiency."
    },
    {
      "type": "Metric",
      "name": "wall-clock run-time",
      "description": "The measured time taken for training runs."
    },
    {
      "type": "Model",
      "name": "Apex AMP",
      "description": "A tool used for mixed precision training with optimization levels."
    },
    {
      "type": "Hardware",
      "name": "A100-80 GB GPUs",
      "description": "The type of GPUs used for training the model."
    },
    {
      "type": "Technique",
      "name": "Gradient Accumulation",
      "description": "A technique used to fit larger batch sizes into limited GPU memory by accumulating gradients over multiple iterations."
    },
    {
      "type": "Dataset",
      "name": "Openwebtext",
      "description": "A dataset used for training language models, derived from web text."
    },
    {
      "type": "Architecture",
      "name": "Mixed-Precision Training",
      "description": "A training technique that uses both 16-bit and 32-bit floating-point types to improve performance and reduce memory usage."
    },
    {
      "type": "Library",
      "name": "Huggingface Transformers",
      "description": "A library providing implementations of various transformer models, including GPT-2."
    },
    {
      "type": "Library",
      "name": "Nvidia\u2019s Megatron-LM",
      "description": "A library for training large language models efficiently on GPUs."
    },
    {
      "type": "Metric",
      "name": "validation perplexity",
      "description": "A metric used to evaluate the performance of language models during training."
    },
    {
      "type": "Model",
      "name": "GPT-2-small",
      "description": "A smaller version of the GPT-2 model used for natural language processing tasks."
    },
    {
      "type": "Model",
      "name": "GPT-2-medium",
      "description": "A medium-sized version of the GPT-2 model, larger than GPT-2-small."
    },
    {
      "type": "Dataset",
      "name": "Long-range arena",
      "description": "A benchmark suite for evaluating models on long-range dependencies in sequences."
    },
    {
      "type": "Metric",
      "name": "Validation perplexity",
      "description": "A measure of how well a probability model predicts a sample."
    },
    {
      "type": "Metric",
      "name": "Wallclock-Time Speedup",
      "description": "A measure of the time efficiency of different attention methods across tasks."
    },
    {
      "type": "Technique",
      "name": "Apex FMHA",
      "description": "The fastest implementation of attention known at the time, tailored for short sequences."
    },
    {
      "type": "Metric",
      "name": "val accuracy",
      "description": "Validation accuracy used to evaluate model performance."
    },
    {
      "type": "Architecture",
      "name": "A100-SXM",
      "description": "A type of GPU used for measuring the performance of attention methods."
    },
    {
      "type": "Architecture",
      "name": "Turing GPUs",
      "description": "A type of GPU architecture supported by FlashAttention."
    },
    {
      "type": "Architecture",
      "name": "Ampere GPUs",
      "description": "Another type of GPU architecture supported by FlashAttention."
    },
    {
      "type": "Model",
      "name": "A100",
      "description": "NVIDIA GPU model used for evaluating speedup in attention mechanisms."
    },
    {
      "type": "Model",
      "name": "RTX 3090",
      "description": "NVIDIA GPU model used for evaluating speedup in attention mechanisms."
    },
    {
      "type": "Model",
      "name": "T4",
      "description": "NVIDIA GPU model used for evaluating speedup in attention mechanisms."
    },
    {
      "type": "Metric",
      "name": "speedup",
      "description": "The performance improvement observed when using FlashAttention on different GPU models."
    },
    {
      "type": "Model",
      "name": "Long Short Former (LSFormer)",
      "description": "A model that combines long and short-range attention mechanisms."
    },
    {
      "type": "Model",
      "name": "Big Bird Attention",
      "description": "An attention mechanism designed for processing long sequences efficiently."
    },
    {
      "type": "Parameter",
      "name": "Attention Dimension",
      "description": "The dimension of each attention head, set to 64."
    },
    {
      "type": "Model",
      "name": "Py Torch Attention",
      "description": "An attention mechanism implemented in PyTorch."
    },
    {
      "type": "Metric",
      "name": "Forward pass runtime",
      "description": "The time taken for the forward pass of various attention mechanisms."
    },
    {
      "type": "Model",
      "name": "LSformer",
      "description": "A model that employs a linearized self-attention mechanism."
    },
    {
      "type": "Model",
      "name": "Block Sparse",
      "description": "An attention mechanism that uses block sparsity."
    },
    {
      "type": "Metric",
      "name": "Timing Results",
      "description": "Measurements of the time taken for forward and backward passes."
    },
    {
      "type": "Model",
      "name": "Block-Sparse",
      "description": "An OpenAI model that does not support sequence lengths longer than 4096."
    },
    {
      "type": "Architecture",
      "name": "FP 16",
      "description": "Floating point representation used for measurements."
    },
    {
      "type": "Architecture",
      "name": "FP 32",
      "description": "Floating point representation used for Local Attention implementation."
    },
    {
      "type": "Model",
      "name": "Block Sparse Flash Attention",
      "description": "An extension of Flash Attention with block sparse capabilities."
    },
    {
      "type": "Metric",
      "name": "Backward pass runtime",
      "description": "The time taken for the backward pass in milliseconds."
    },
    {
      "type": "Metric",
      "name": "Memory usage",
      "description": "The amount of memory (in MB) used by various attention mechanisms."
    },
    {
      "type": "Task",
      "name": "Question Answering",
      "description": "A natural language processing task where the model is required to answer questions based on a given context."
    },
    {
      "type": "Task",
      "name": "Language Inference",
      "description": "A task that involves determining the relationship between sentences, such as entailment or contradiction."
    },
    {
      "type": "Metric",
      "name": "GLUE score",
      "description": "A benchmark score for evaluating natural language understanding systems."
    },
    {
      "type": "Metric",
      "name": "Multi NLI accuracy",
      "description": "A metric for evaluating the accuracy of models on the Multi-Genre Natural Language Inference task."
    },
    {
      "type": "Dataset",
      "name": "SQuAD v1.1",
      "description": "A dataset for question answering that includes questions posed on a set of Wikipedia articles."
    },
    {
      "type": "Metric",
      "name": "SQuAD v1.1 Test F1",
      "description": "A metric used to evaluate the performance of models on the SQuAD v1.1 dataset."
    },
    {
      "type": "Dataset",
      "name": "SQuAD v2.0",
      "description": "An extension of SQuAD v1.1 that includes unanswerable questions."
    },
    {
      "type": "Metric",
      "name": "SQuAD v2.0 Test F1",
      "description": "A metric used to evaluate the performance of models on the SQuAD v2.0 dataset."
    },
    {
      "type": "Technique",
      "name": "Language model pre-training",
      "description": "A technique that improves performance on various natural language processing tasks."
    },
    {
      "type": "Task",
      "name": "Natural Language Inference",
      "description": "A task that involves predicting the relationship between sentences."
    },
    {
      "type": "Task",
      "name": "Paraphrasing",
      "description": "A task that involves rephrasing sentences while preserving their meaning."
    },
    {
      "type": "Technique",
      "name": "Named Entity Recognition",
      "description": "A token-level task where models produce fine-grained output at the token level."
    },
    {
      "type": "Model",
      "name": "ELMo",
      "description": "A pre-trained language representation model that uses task-specific architectures."
    },
    {
      "type": "Model",
      "name": "Generative Pre-trained Transformer (Open AI GPT)",
      "description": "A model that introduces minimal task-specific parameters and is trained by fine-tuning."
    },
    {
      "type": "Model",
      "name": "Open AI GPT",
      "description": "A generative pre-trained transformer model that uses unidirectional language models."
    },
    {
      "type": "Architecture",
      "name": "Left-to-right architecture",
      "description": "An architecture used in Open AI GPT where tokens can only attend to previous tokens."
    },
    {
      "type": "Concept",
      "name": "Unidirectional language models",
      "description": "Language models that process text in one direction, limiting the representation power."
    },
    {
      "type": "Concept",
      "name": "Self-attention layers",
      "description": "Layers in the transformer architecture that allow tokens to attend to each other."
    },
    {
      "type": "Technique",
      "name": "Masked Language Model (MLM)",
      "description": "A pre-training objective that randomly masks tokens in the input and predicts the original vocabulary id of the masked tokens."
    },
    {
      "type": "Task",
      "name": "Cloze task",
      "description": "A task where words are omitted from a text and the objective is to predict the missing words."
    },
    {
      "type": "Technique",
      "name": "Next Sentence Prediction",
      "description": "A task that jointly pre-trains text-pair representations."
    },
    {
      "type": "Model",
      "name": "Unidirectional Language Models",
      "description": "Language models that process text in a single direction, either left-to-right or right-to-left."
    },
    {
      "type": "Model",
      "name": "Peters et al. (2018)",
      "description": "A model that uses a shallow concatenation of independently trained left-to-right and right-to-left language models."
    },
    {
      "type": "Task",
      "name": "NLP tasks",
      "description": "A suite of sentence-level and token-level tasks on which BERT outperforms many task-specific architectures."
    },
    {
      "type": "Dataset",
      "name": "General Language Representations",
      "description": "A long history of research focused on learning widely applicable representations of words."
    },
    {
      "type": "Model",
      "name": "Word2Vec",
      "description": "A neural approach to learning word representations introduced by Mikolov et al. in 2013."
    },
    {
      "type": "Model",
      "name": "GloVe",
      "description": "A neural approach to learning word representations introduced by Pennington et al."
    },
    {
      "type": "Concept",
      "name": "Pre-trained word embeddings",
      "description": "Integral part of modern NLP systems that offer significant improvements over embeddings learned from scratch."
    },
    {
      "type": "Technique",
      "name": "Left-to-right language modeling",
      "description": "Objective used to pretrain word embedding vectors."
    },
    {
      "type": "Technique",
      "name": "Ranking candidate next sentences",
      "description": "Objective used to train sentence representations."
    },
    {
      "type": "Model",
      "name": "Sentence embeddings",
      "description": "Representations of sentences that can be used for various NLP tasks."
    },
    {
      "type": "Model",
      "name": "Paragraph embeddings",
      "description": "Representations of paragraphs that can be used for various NLP tasks."
    },
    {
      "type": "Technique",
      "name": "Denoising Autoencoder",
      "description": "A technique used to derive objectives for training sentence representations."
    },
    {
      "type": "Dataset",
      "name": "NLP Benchmarks",
      "description": "A collection of major benchmarks for evaluating natural language processing models."
    },
    {
      "type": "Metric",
      "name": "State of the Art",
      "description": "A term used to describe the highest level of development achieved in a field."
    },
    {
      "type": "Architecture",
      "name": "Contextual Word Embeddings",
      "description": "A method of integrating word embeddings with task-specific architectures."
    },
    {
      "type": "Metric",
      "name": "Sentiment Analysis",
      "description": "A benchmark task in NLP that involves determining the sentiment expressed in text."
    },
    {
      "type": "Technique",
      "name": "Cloze Task",
      "description": "A task used to improve the robustness of text generation models."
    },
    {
      "type": "Model",
      "name": "LSTM",
      "description": "A type of recurrent neural network used for learning contextual representations."
    },
    {
      "type": "Model",
      "name": "Sentence/Document Encoders",
      "description": "Models that produce contextual token representations from unlabeled text."
    },
    {
      "type": "Dataset",
      "name": "GLUE benchmark",
      "description": "A collection of tasks for evaluating natural language understanding systems."
    },
    {
      "type": "Technique",
      "name": "Next Sentence Prediction (NSP)",
      "description": "A pre-training task that involves predicting whether a given sentence follows another."
    },
    {
      "type": "Task",
      "name": "Machine Translation",
      "description": "A supervised task that involves translating text from one language to another."
    },
    {
      "type": "Concept",
      "name": "Downstream tasks",
      "description": "Specific tasks that utilize the fine-tuned models derived from the pre-trained BERT model."
    },
    {
      "type": "Model",
      "name": "BERTBASE",
      "description": "A model configuration of BERT with 12 layers, hidden size of 768, and 12 self-attention heads."
    },
    {
      "type": "Model",
      "name": "BERTLARGE",
      "description": "A model configuration of BERT with 24 layers, hidden size of 1024, and 16 self-attention heads."
    },
    {
      "type": "Technique",
      "name": "Bidirectional Self-Attention",
      "description": "A technique used in BERT that allows the model to consider context from both directions."
    },
    {
      "type": "Technique",
      "name": "Constrained Self-Attention",
      "description": "A technique used in GPT that limits attention to previous tokens."
    },
    {
      "type": "Technique",
      "name": "Word Piece embeddings",
      "description": "A method for tokenizing text into subword units, allowing for a flexible vocabulary."
    },
    {
      "type": "Token",
      "name": "[CLS]",
      "description": "A special classification token used at the beginning of input sequences for classification tasks."
    },
    {
      "type": "Token",
      "name": "[SEP]",
      "description": "A special token used to separate sentences in a sequence."
    },
    {
      "type": "Dataset",
      "name": "30,000 token vocabulary",
      "description": "The vocabulary size used for the Word Piece embeddings in BERT."
    },
    {
      "type": "Technique",
      "name": "Masked LM",
      "description": "An unsupervised task used for pre-training BERT that involves predicting masked tokens in a sentence."
    },
    {
      "type": "Embedding",
      "name": "Input Embedding",
      "description": "The representation of input tokens constructed from token, segment, and position embeddings."
    },
    {
      "type": "Vector",
      "name": "Hidden Vector",
      "description": "The final hidden vector representation of tokens and the special [CLS] token."
    },
    {
      "type": "Architecture",
      "name": "Transformer decoder",
      "description": "A model architecture that processes input in a left-to-right manner, used for text generation."
    },
    {
      "type": "Concept",
      "name": "Bidirectional conditioning",
      "description": "A method that allows a model to consider context from both directions."
    },
    {
      "type": "Concept",
      "name": "Conditional language models",
      "description": "Models that predict the next word based on previous words, typically trained in one direction."
    },
    {
      "type": "Technique",
      "name": "Denoising auto-encoders",
      "description": "A type of model that reconstructs the entire input rather than just predicting masked words."
    },
    {
      "type": "Metric",
      "name": "Word Piece tokens",
      "description": "A method of tokenization used in the BERT model."
    },
    {
      "type": "Technique",
      "name": "Masked Language Modeling",
      "description": "A training technique where certain tokens in the input are masked and predicted by the model."
    },
    {
      "type": "Task",
      "name": "Question Answering (QA)",
      "description": "A downstream task that requires understanding and answering questions based on text."
    },
    {
      "type": "Task",
      "name": "Natural Language Inference (NLI)",
      "description": "A task that involves determining the relationship between two sentences, such as entailment or contradiction."
    },
    {
      "type": "Metric",
      "name": "Cross Entropy Loss",
      "description": "A loss function used to measure the performance of a model whose output is a probability value between 0 and 1."
    },
    {
      "type": "Technique",
      "name": "Binarized Next Sentence Prediction",
      "description": "A method for training models to understand sentence relationships by using a binary classification task."
    },
    {
      "type": "Dataset",
      "name": "Books Corpus",
      "description": "A dataset containing 800 million words used for pre-training BERT."
    },
    {
      "type": "Dataset",
      "name": "English Wikipedia",
      "description": "A dataset containing 2,500 million words used for pre-training BERT."
    },
    {
      "type": "Dataset",
      "name": "Billion Word Benchmark",
      "description": "A shuffled sentence-level corpus that is less effective for extracting long contiguous sequences."
    },
    {
      "type": "Architecture",
      "name": "Self-attention mechanism",
      "description": "A mechanism in the Transformer architecture that allows BERT to model relationships in text."
    },
    {
      "type": "Technique",
      "name": "Bidirectional cross attention",
      "description": "A method used in some applications involving text pairs to encode relationships."
    },
    {
      "type": "Technique",
      "name": "Self-Attention Mechanism",
      "description": "A mechanism that allows the model to weigh the importance of different words in a sentence when encoding."
    },
    {
      "type": "Concept",
      "name": "Bidirectional Cross Attention",
      "description": "A method that allows the model to attend to both sentences in a pair simultaneously."
    },
    {
      "type": "Task",
      "name": "Entailment",
      "description": "The task of determining if one sentence logically follows from another."
    },
    {
      "type": "Task",
      "name": "Text Classification",
      "description": "The task of categorizing text into predefined classes."
    },
    {
      "type": "Task",
      "name": "Sequence Tagging",
      "description": "The task of assigning labels to each token in a sequence."
    },
    {
      "type": "Concept",
      "name": "[CLS] Representation",
      "description": "A special token representation used for classification tasks."
    },
    {
      "type": "Metric",
      "name": "CLS representation",
      "description": "A representation used in BERT for classification tasks."
    },
    {
      "type": "Metric",
      "name": "F1 score",
      "description": "A measure of a model's accuracy that considers both precision and recall."
    },
    {
      "type": "Model",
      "name": "Bi LSTM+ELMo+Attn",
      "description": "A model combining bidirectional LSTM, ELMo embeddings, and attention mechanisms."
    },
    {
      "type": "Metric",
      "name": "F1 Score",
      "description": "A measure of a model's accuracy that considers both precision and recall."
    },
    {
      "type": "Metric",
      "name": "Spearman Correlation",
      "description": "A measure of rank correlation used to assess the strength of association between two variables."
    },
    {
      "type": "Technique",
      "name": "fine-tuning",
      "description": "The process of adjusting a pre-trained model on a specific task to improve performance."
    },
    {
      "type": "Dataset",
      "name": "Trivia QA",
      "description": "A dataset used for training question answering systems, containing trivia questions."
    },
    {
      "type": "Model",
      "name": "QANet",
      "description": "A model described in the literature that has improved after publication."
    },
    {
      "type": "Model",
      "name": "nlnet",
      "description": "An ensemble model that competes in language understanding tasks."
    },
    {
      "type": "Metric",
      "name": "EM",
      "description": "Exact Match, a metric used to evaluate the performance of models."
    },
    {
      "type": "Metric",
      "name": "F1",
      "description": "F1 score, a metric that combines precision and recall for model evaluation."
    },
    {
      "type": "Dataset",
      "name": "SQuAD 1.1",
      "description": "A dataset for evaluating question answering systems."
    },
    {
      "type": "Dataset",
      "name": "SQuAD 2.0",
      "description": "An updated version of the SQuAD dataset that includes unanswerable questions."
    },
    {
      "type": "Model",
      "name": "ESIM",
      "description": "Enhanced Sequential Inference Model, a model used for natural language inference."
    },
    {
      "type": "Model",
      "name": "OpenAI GPT",
      "description": "Generative Pre-trained Transformer, a model for generating human-like text."
    },
    {
      "type": "Dataset",
      "name": "SWAG",
      "description": "Situations With Adversarial Generations dataset containing sentence-pair completion examples for evaluating grounded common-sense inference."
    },
    {
      "type": "Model",
      "name": "ESIM+ELMo",
      "description": "A baseline system that BERT outperforms, combining ESIM and ELMo models."
    },
    {
      "type": "Metric",
      "name": "Accuracy Improvement",
      "description": "The percentage by which BERT outperforms other models, specifically +27.1% over ESIM+ELMo and +8.3% over Open AI GPT."
    },
    {
      "type": "Technique",
      "name": "Masked Language Modeling (MLM)",
      "description": "A training technique where some percentage of input tokens are masked and the model learns to predict them."
    },
    {
      "type": "Model",
      "name": "LTR & No NSP",
      "description": "A left-context-only model trained without the next sentence prediction task."
    },
    {
      "type": "Model",
      "name": "BiLSTM",
      "description": "A bidirectional Long Short-Term Memory model used to enhance token predictions."
    },
    {
      "type": "Technique",
      "name": "LTR",
      "description": "Left-to-right training approach for language models."
    },
    {
      "type": "Technique",
      "name": "NSP",
      "description": "Next Sentence Prediction, a task used in training BERT."
    },
    {
      "type": "Technique",
      "name": "MLM",
      "description": "Masked Language Model, a training objective used in BERT."
    },
    {
      "type": "Metric",
      "name": "Dev Set accuracy",
      "description": "A measure of model performance based on accuracy on a development set."
    },
    {
      "type": "Architecture",
      "name": "Bidirectional Model",
      "description": "A model architecture that processes input data in both left-to-right and right-to-left directions."
    },
    {
      "type": "Model",
      "name": "BERT BASE",
      "description": "A model containing 110 million parameters."
    },
    {
      "type": "Model",
      "name": "BERT LARGE",
      "description": "A model containing 340 million parameters."
    },
    {
      "type": "Architecture",
      "name": "Transformer (Vaswani et al. 2017)",
      "description": "The largest Transformer explored in prior literature with specific parameters."
    },
    {
      "type": "Architecture",
      "name": "Transformer (Al-Rfou et al. 2018)",
      "description": "The largest Transformer found in the literature with specific parameters."
    },
    {
      "type": "Metric",
      "name": "LM perplexity",
      "description": "A measure of how well a probability model predicts a sample."
    },
    {
      "type": "Concept",
      "name": "Model size",
      "description": "Refers to the number of parameters in a model, which impacts its performance."
    },
    {
      "type": "Task",
      "name": "Language Modeling",
      "description": "The task of predicting the next word in a sequence given the previous words."
    },
    {
      "type": "Technique",
      "name": "Feature-based Approach",
      "description": "An approach where fixed features are extracted from the pre-trained model without fine-tuning all parameters."
    },
    {
      "type": "Model",
      "name": "bi-LM",
      "description": "A bidirectional language model used for various language tasks."
    },
    {
      "type": "Metric",
      "name": "Hidden Dimension Size",
      "description": "A parameter that affects the performance of language models, specifically in terms of the number of dimensions in the model's hidden layers."
    },
    {
      "type": "Dataset",
      "name": "CoNLL-2003",
      "description": "A dataset used for Named Entity Recognition (NER) tasks."
    },
    {
      "type": "Technique",
      "name": "Feature-based approach",
      "description": "An approach where fixed features are extracted from a pre-trained model."
    },
    {
      "type": "Technique",
      "name": "Tagging task",
      "description": "A formulation of the NER task where entities are tagged in the input text."
    },
    {
      "type": "Architecture",
      "name": "Bi LSTM",
      "description": "A bidirectional Long Short-Term Memory network used for classification."
    },
    {
      "type": "Architecture",
      "name": "Deep Bidirectional Architecture",
      "description": "An architecture that allows the model to consider context from both directions in a sentence."
    },
    {
      "type": "Technique",
      "name": "Feature-based approaches",
      "description": "Methods that utilize features extracted from a model without further training on the task."
    },
    {
      "type": "Concept",
      "name": "Unsupervised Pre-training",
      "description": "Training a model on a large dataset without labeled outputs to learn general features."
    },
    {
      "type": "Technique",
      "name": "Contextual string embeddings",
      "description": "A method for generating embeddings that capture context for sequence labeling."
    },
    {
      "type": "Technique",
      "name": "Character-level language modeling",
      "description": "A modeling approach that focuses on character-level representations using self-attention."
    },
    {
      "type": "Dataset",
      "name": "PASCAL recognizing textual entailment challenge",
      "description": "A dataset used for evaluating models on textual entailment tasks."
    },
    {
      "type": "Technique",
      "name": "Domain adaptation",
      "description": "A technique for adapting models to new domains using structural correspondence learning."
    },
    {
      "type": "Dataset",
      "name": "Annotated Corpus",
      "description": "A large annotated corpus for learning natural language inference."
    },
    {
      "type": "Technique",
      "name": "Class-based n-gram models",
      "description": "A technique for modeling natural language using class-based approaches."
    },
    {
      "type": "Dataset",
      "name": "Quora question pairs",
      "description": "A dataset used for evaluating models on question similarity tasks."
    },
    {
      "type": "Metric",
      "name": "One billion word benchmark",
      "description": "A benchmark for measuring progress in statistical language modeling."
    },
    {
      "type": "Technique",
      "name": "Multitask Learning",
      "description": "An approach that involves training a model on multiple tasks simultaneously."
    },
    {
      "type": "Dataset",
      "name": "Universal Sentence Representations",
      "description": "A dataset used for training models to understand sentence-level semantics."
    },
    {
      "type": "Dataset",
      "name": "Natural Language Inference Data",
      "description": "Data used for supervised learning to derive universal sentence representations."
    },
    {
      "type": "Dataset",
      "name": "Corpus of Sentential Paraphrases",
      "description": "A corpus constructed for the purpose of studying paraphrasing in sentences."
    },
    {
      "type": "Model",
      "name": "MaskGAN",
      "description": "A model for text generation that improves performance by filling in missing parts of text."
    },
    {
      "type": "Metric",
      "name": "Gaussian Error Linear Units (GELUs)",
      "description": "A type of activation function used in neural networks."
    },
    {
      "type": "Technique",
      "name": "Distributed Representations",
      "description": "A method for representing sentences in a continuous vector space."
    },
    {
      "type": "Model",
      "name": "Universal Language Model",
      "description": "A model fine-tuned for text classification tasks."
    },
    {
      "type": "Technique",
      "name": "Universal Language Model Fine-tuning",
      "description": "A technique for fine-tuning language models for specific tasks."
    },
    {
      "type": "Technique",
      "name": "Reinforced Mnemonic Reader",
      "description": "A technique for machine reading comprehension that utilizes reinforcement learning."
    },
    {
      "type": "Technique",
      "name": "Discourse-based Objectives",
      "description": "Objectives aimed at improving unsupervised sentence representation learning."
    },
    {
      "type": "Dataset",
      "name": "TriviaQA",
      "description": "A large scale distantly supervised challenge dataset for reading comprehension."
    },
    {
      "type": "Technique",
      "name": "Skip-thought vectors",
      "description": "A technique for learning sentence representations."
    },
    {
      "type": "Technique",
      "name": "Distributed representations",
      "description": "A method for representing sentences and documents in a continuous vector space."
    },
    {
      "type": "Dataset",
      "name": "Winograd Schema Challenge",
      "description": "A challenge designed to test commonsense reasoning."
    },
    {
      "type": "Model",
      "name": "Sentence representations framework",
      "description": "An efficient framework for learning sentence representations."
    },
    {
      "type": "Technique",
      "name": "Contextualized word vectors",
      "description": "Word vectors that are generated based on the context in which words appear."
    },
    {
      "type": "Technique",
      "name": "Bidirectional LSTM",
      "description": "A type of recurrent neural network that processes data in both forward and backward directions."
    },
    {
      "type": "Model",
      "name": "Hierarchical distributed language model",
      "description": "A scalable model for language representation that captures hierarchical structures."
    },
    {
      "type": "Technique",
      "name": "Decomposable attention",
      "description": "An attention mechanism that breaks down the attention process into simpler components."
    },
    {
      "type": "Technique",
      "name": "Bidirectional Language Model",
      "description": "A model that predicts words in a sequence by considering both left and right context."
    },
    {
      "type": "Technique",
      "name": "Decomposable Attention Model",
      "description": "A model for natural language inference that decomposes attention mechanisms."
    },
    {
      "type": "Metric",
      "name": "Semi-supervised Sequence Tagging",
      "description": "A method for tagging sequences using a combination of labeled and unlabeled data."
    },
    {
      "type": "Architecture",
      "name": "Contextualized Word Representations",
      "description": "Word representations that capture context-dependent meanings."
    },
    {
      "type": "Model",
      "name": "Bidirectional Attention Flow",
      "description": "A model architecture for machine comprehension that uses attention mechanisms."
    },
    {
      "type": "Metric",
      "name": "Semantic Compositionality",
      "description": "A measure of how well a model can understand the meaning of sentences based on their components."
    },
    {
      "type": "Metric",
      "name": "Readability",
      "description": "A measure of how easy a text is to read, often assessed using the Cloze procedure."
    },
    {
      "type": "Dataset",
      "name": "Multi-granularity hierarchical attention fusion networks",
      "description": "A dataset used for reading comprehension and question answering."
    },
    {
      "type": "Dataset",
      "name": "A broad-coverage challenge corpus for sentence understanding through inference",
      "description": "A dataset designed to evaluate sentence understanding capabilities."
    },
    {
      "type": "Model",
      "name": "Google's Neural Machine Translation System",
      "description": "A neural network model for machine translation developed by Google."
    },
    {
      "type": "Technique",
      "name": "Neural Machine Translation",
      "description": "A technique for translating text using neural networks."
    },
    {
      "type": "Technique",
      "name": "Ablation Study",
      "description": "A method to analyze the impact of different components or parameters of a model."
    },
    {
      "type": "Technique",
      "name": "Masking Procedures",
      "description": "Techniques used to hide certain parts of the input data during training to improve model robustness."
    },
    {
      "type": "Technique",
      "name": "Masking Procedure",
      "description": "A method used in the Masked LM task to randomly replace or keep words in a sentence."
    },
    {
      "type": "Concept",
      "name": "Pre-training Tasks",
      "description": "Tasks performed before fine-tuning a model to improve its performance on specific tasks."
    },
    {
      "type": "Metric",
      "name": "Language Understanding Capability",
      "description": "The ability of a model to comprehend and generate human language."
    },
    {
      "type": "Architecture",
      "name": "Bidirectional Transformer",
      "description": "An architecture that allows the model to consider both left and right context in all layers."
    },
    {
      "type": "Architecture",
      "name": "Left-to-Right Transformer",
      "description": "An architecture that predicts tokens in a sequential manner from left to right."
    },
    {
      "type": "Metric",
      "name": "MLM (Masked Language Model)",
      "description": "A training objective that predicts masked tokens in a sequence."
    },
    {
      "type": "Dataset",
      "name": "Corpus",
      "description": "The collection of text data used for training the BERT model."
    },
    {
      "type": "Dataset",
      "name": "3.3 billion word corpus",
      "description": "A large corpus used for training the BERT model."
    },
    {
      "type": "Technique",
      "name": "Word Piece Tokenization",
      "description": "A method for tokenizing text into subword units."
    },
    {
      "type": "Technique",
      "name": "GELU Activation",
      "description": "Gaussian Error Linear Unit, an activation function used in the model."
    },
    {
      "type": "Technique",
      "name": "Learning Rate Warmup",
      "description": "A technique to gradually increase the learning rate during the initial training steps."
    },
    {
      "type": "Technique",
      "name": "Linear Decay",
      "description": "A method to decrease the learning rate linearly after the warmup phase."
    },
    {
      "type": "Architecture",
      "name": "Cloud TPU",
      "description": "Tensor Processing Units used for training the models."
    },
    {
      "type": "Metric",
      "name": "Masked LM Likelihood",
      "description": "A metric used to evaluate the model's performance on masked language modeling tasks."
    },
    {
      "type": "Metric",
      "name": "Next Sentence Prediction Likelihood",
      "description": "A metric used to evaluate the model's performance on predicting the next sentence."
    },
    {
      "type": "Dataset",
      "name": "100k+ labeled training examples",
      "description": "A large dataset used for training that is less sensitive to hyperparameter choices."
    },
    {
      "type": "Dataset",
      "name": "Development Set",
      "description": "A subset of data used to tune the parameters of the model and evaluate its performance."
    },
    {
      "type": "Concept",
      "name": "Contextual Representation",
      "description": "The representation of a token in the context of surrounding tokens."
    },
    {
      "type": "Dataset",
      "name": "Stanford Question Answering Dataset",
      "description": "A dataset used for question answering tasks."
    },
    {
      "type": "Dataset",
      "name": "WNLI",
      "description": "Winograd NLI is a small natural language inference dataset."
    },
    {
      "type": "Metric",
      "name": "65.1 baseline accuracy",
      "description": "The baseline accuracy for predicting the majority class in the GLUE benchmark."
    },
    {
      "type": "Metric",
      "name": "Dev Accuracy",
      "description": "A metric used to evaluate the performance of the model on the development set."
    },
    {
      "type": "Metric",
      "name": "NER",
      "description": "Named Entity Recognition, a task for identifying entities in text."
    },
    {
      "type": "Metric",
      "name": "Dev Set Results",
      "description": "Evaluation metrics reported for the model's performance on specific tasks."
    },
    {
      "type": "Architecture",
      "name": "Masking Strategies",
      "description": "Different strategies for masking tokens during the masked language model pre-training."
    },
    {
      "type": "Metric",
      "name": "Dev set results",
      "description": "Evaluation results on the development set to assess model performance."
    },
    {
      "type": "Task",
      "name": "Named Entity Recognition (NER)",
      "description": "A task that involves identifying and classifying entities in text."
    },
    {
      "type": "Strategy",
      "name": "MASK strategy",
      "description": "A masking strategy used in MLM that replaces tokens with a [MASK] token."
    },
    {
      "type": "Strategy",
      "name": "RND strategy",
      "description": "A random masking strategy that performs worse than the proposed strategy in the context."
    },
    {
      "type": "Architecture",
      "name": "Sequence Transduction Models",
      "description": "Models that typically include an encoder and a decoder, often based on recurrent or convolutional neural networks."
    },
    {
      "type": "Dataset",
      "name": "WMT 2014 English-to-German",
      "description": "A machine translation task where the model achieved a BLEU score of 28.4."
    },
    {
      "type": "Dataset",
      "name": "WMT 2014 English-to-French",
      "description": "A machine translation task where the model established a new single-model state-of-the-art BLEU score of 41.8."
    },
    {
      "type": "Architecture",
      "name": "Network Architecture",
      "description": "The structural design of the Transformer model."
    },
    {
      "type": "Technique",
      "name": "Scaled Dot-Product Attention",
      "description": "A specific attention mechanism proposed by Noam Shazeer."
    },
    {
      "type": "Architecture",
      "name": "Position Representation",
      "description": "A parameter-free representation of position in the input sequence, proposed by Noam Shazeer."
    },
    {
      "type": "Codebase",
      "name": "Tensor2Tensor",
      "description": "A library used for implementing and evaluating model variants."
    },
    {
      "type": "Concept",
      "name": "Recurrent Neural Networks",
      "description": "A class of neural networks designed for sequence modeling and transduction problems."
    },
    {
      "type": "Model",
      "name": "Long Short-Term Memory (LSTM)",
      "description": "A type of recurrent neural network that can learn long-term dependencies."
    },
    {
      "type": "Model",
      "name": "Gated Recurrent Neural Networks",
      "description": "A variant of recurrent neural networks that uses gating mechanisms to control the flow of information."
    },
    {
      "type": "Technique",
      "name": "Sequence Modeling",
      "description": "The process of predicting the next element in a sequence based on previous elements."
    },
    {
      "type": "Technique",
      "name": "Transduction",
      "description": "The process of converting one sequence into another, such as in machine translation."
    },
    {
      "type": "Architecture",
      "name": "Encoder-Decoder Architecture",
      "description": "A neural network architecture used for sequence-to-sequence tasks."
    },
    {
      "type": "Metric",
      "name": "Memory Constraints",
      "description": "Limitations on memory usage that affect the training of models on longer sequences."
    },
    {
      "type": "Technique",
      "name": "Factorization Tricks",
      "description": "Techniques that have achieved significant improvements in computational efficiency."
    },
    {
      "type": "Architecture",
      "name": "Recurrent Network",
      "description": "A type of neural network architecture that processes sequences of data, often used in conjunction with attention mechanisms."
    },
    {
      "type": "Technique",
      "name": "Extended Neural GPU",
      "description": "A technique aimed at reducing sequential computation in neural networks."
    },
    {
      "type": "Technique",
      "name": "Byte Net",
      "description": "A technique that uses convolutional neural networks to compute hidden representations in parallel."
    },
    {
      "type": "Technique",
      "name": "Conv S2S",
      "description": "A technique that employs convolutional neural networks as a basic building block for sequence-to-sequence tasks."
    },
    {
      "type": "Metric",
      "name": "Translation Quality",
      "description": "A measure of the effectiveness of a translation model."
    },
    {
      "type": "Model",
      "name": "End-to-end memory networks",
      "description": "A model based on a recurrent attention mechanism that performs well on simple-language question answering and language modeling tasks."
    },
    {
      "type": "Task",
      "name": "Reading comprehension",
      "description": "A task where self-attention has been successfully applied."
    },
    {
      "type": "Task",
      "name": "Abstractive summarization",
      "description": "A task where self-attention has been successfully applied."
    },
    {
      "type": "Task",
      "name": "Textual entailment",
      "description": "A task where self-attention has been successfully applied."
    },
    {
      "type": "Task",
      "name": "Learning task-independent sentence representations",
      "description": "A task where self-attention has been successfully applied."
    },
    {
      "type": "Architecture",
      "name": "Encoder-Decoder",
      "description": "A structure commonly used in neural sequence transduction models where the encoder maps input sequences to continuous representations and the decoder generates output sequences."
    },
    {
      "type": "Technique",
      "name": "Auto-regressive generation",
      "description": "A method where the model generates output one element at a time, using previously generated elements as input."
    },
    {
      "type": "Concept",
      "name": "Continuous representations",
      "description": "Representations of data in a continuous space, denoted as z = (z1,..., zn)."
    },
    {
      "type": "Symbol",
      "name": "Output sequence",
      "description": "The sequence of symbols generated by the decoder, denoted as (y1,..., ym)."
    },
    {
      "type": "Technique",
      "name": "Multi-Head Self-Attention",
      "description": "An extension of self-attention that allows the model to jointly attend to information from different representation subspaces."
    },
    {
      "type": "Technique",
      "name": "Feed-Forward Network",
      "description": "A simple, position-wise fully connected network used in each layer of the encoder and decoder."
    },
    {
      "type": "Concept",
      "name": "Residual Connection",
      "description": "A shortcut connection that helps in training deep networks by allowing gradients to flow through the network."
    },
    {
      "type": "Concept",
      "name": "Residual Connections",
      "description": "Connections that allow gradients to flow through the network more easily by adding the input of a layer to its output."
    },
    {
      "type": "Concept",
      "name": "Attention Function",
      "description": "A mapping of a query and a set of key-value pairs to an output, where all components are vectors."
    },
    {
      "type": "Technique",
      "name": "Dot-product Attention",
      "description": "An attention function that computes outputs using the dot product of query and key matrices."
    },
    {
      "type": "Technique",
      "name": "Additive Attention",
      "description": "An attention function that computes compatibility using a feed-forward network."
    },
    {
      "type": "Metric",
      "name": "dk",
      "description": "A scaling factor used in the attention mechanism."
    },
    {
      "type": "Technique",
      "name": "Multi-head attention",
      "description": "A technique that allows the model to jointly attend to information from different representation subspaces at different positions."
    },
    {
      "type": "Architecture",
      "name": "Encoder-decoder attention",
      "description": "A layer in the Transformer model where queries come from the previous decoder layer."
    },
    {
      "type": "Parameter Matrix",
      "name": "WQ, WK, WV, WO",
      "description": "Parameter matrices used in the attention mechanism for projecting input values."
    },
    {
      "type": "Technique",
      "name": "Multi-head Attention",
      "description": "A technique that allows the model to focus on different parts of the input sequence simultaneously."
    },
    {
      "type": "Model",
      "name": "Sequence-to-Sequence Models",
      "description": "Models that transform a sequence of input data into a sequence of output data."
    },
    {
      "type": "Technique",
      "name": "Position-wise Feed-Forward Networks",
      "description": "A component of the Transformer architecture that applies a fully connected feed-forward network to each position in the sequence independently."
    },
    {
      "type": "Activation Function",
      "name": "ReLU",
      "description": "A non-linear activation function defined as the positive part of its argument, used in the feed-forward networks."
    },
    {
      "type": "Architecture",
      "name": "Feed Forward Network (FFN)",
      "description": "A component of the Transformer model that applies linear transformations to the input."
    },
    {
      "type": "Metric",
      "name": "Next-token probabilities",
      "description": "Probabilities predicted for the next token in a sequence, derived from the decoder output."
    },
    {
      "type": "Concept",
      "name": "Learned Embeddings",
      "description": "Vectors of fixed dimension that represent input and output tokens in the model."
    },
    {
      "type": "Metric",
      "name": "Dimensionality",
      "description": "The size of the input and output vectors, specified as dmodel = 512 and dff = 2048."
    },
    {
      "type": "Concept",
      "name": "Positional Encoding",
      "description": "A method to inject information about the relative or absolute position of tokens in a sequence to enable the model to utilize the order of the sequence."
    },
    {
      "type": "Technique",
      "name": "Restricted Self-Attention",
      "description": "A variant of self-attention that limits the attention mechanism to a specific neighborhood size."
    },
    {
      "type": "Metric",
      "name": "Complexity per Layer",
      "description": "A measure of the computational complexity associated with different layer types in the model."
    },
    {
      "type": "Metric",
      "name": "Maximum Path Length",
      "description": "The longest path through the network layers, which affects the model's ability to capture dependencies in the data."
    },
    {
      "type": "Concept",
      "name": "Positional Encodings",
      "description": "Additions to input embeddings that provide information about the position of tokens in a sequence."
    },
    {
      "type": "Technique",
      "name": "Sine and Cosine Functions",
      "description": "Mathematical functions used to create positional encodings with varying frequencies."
    },
    {
      "type": "Architecture",
      "name": "Encoder-Decoder Stacks",
      "description": "The structure used in the Transformer model, consisting of an encoder and a decoder."
    },
    {
      "type": "Concept",
      "name": "Parallelization",
      "description": "The ability to perform multiple computations simultaneously, which is a key advantage of self-attention over recurrent layers."
    },
    {
      "type": "Concept",
      "name": "Long-range dependencies",
      "description": "Key challenge in sequence transduction tasks, affecting the ability to learn relationships over long distances in data."
    },
    {
      "type": "Metric",
      "name": "Computational complexity",
      "description": "Total computational complexity per layer, important for understanding the efficiency of models."
    },
    {
      "type": "Metric",
      "name": "Path length",
      "description": "Length of paths that signals must traverse in the network, impacting the learning of long-range dependencies."
    },
    {
      "type": "Architecture",
      "name": "Self-Attention Layer",
      "description": "A layer that connects all positions with a constant number of sequentially executed operations."
    },
    {
      "type": "Architecture",
      "name": "Recurrent Layer",
      "description": "A layer that requires O(n) sequential operations, making it slower than self-attention layers in terms of computational complexity."
    },
    {
      "type": "Technique",
      "name": "Convolutional Layer",
      "description": "A layer in neural networks that applies convolution operations to input data, often used for processing grid-like data."
    },
    {
      "type": "Technique",
      "name": "Dilated Convolutions",
      "description": "A type of convolution that expands the kernel's receptive field without increasing the number of parameters."
    },
    {
      "type": "Dataset",
      "name": "Word-Piece",
      "description": "A subword tokenization technique used in natural language processing to handle out-of-vocabulary words."
    },
    {
      "type": "Dataset",
      "name": "Byte-Pair",
      "description": "A data compression technique that is also used for subword tokenization in NLP."
    },
    {
      "type": "Technique",
      "name": "Separable Convolutions",
      "description": "A type of convolution that reduces computational complexity compared to standard convolutions."
    },
    {
      "type": "Architecture",
      "name": "Point-wise Feed-Forward Layer",
      "description": "A layer that applies a feed-forward neural network to each position independently."
    },
    {
      "type": "Metric",
      "name": "Complexity",
      "description": "A measure of the computational resources required by a model, often expressed in Big O notation."
    },
    {
      "type": "Dataset",
      "name": "WMT 2014 English-German",
      "description": "A dataset consisting of about 4.5 million sentence pairs used for training."
    },
    {
      "type": "Dataset",
      "name": "WMT 2014 English-French",
      "description": "A significantly larger dataset consisting of 36 million sentences used for training."
    },
    {
      "type": "Technique",
      "name": "Byte-Pair Encoding",
      "description": "A technique used for encoding sentences with a shared source-target vocabulary."
    },
    {
      "type": "Hardware",
      "name": "NVIDIA P 100 GPUs",
      "description": "The hardware used for training the models."
    },
    {
      "type": "Technique",
      "name": "Adam optimizer",
      "description": "An optimization algorithm that computes adaptive learning rates for each parameter."
    },
    {
      "type": "Concept",
      "name": "Learning Rate Scheduling",
      "description": "A technique to adjust the learning rate during training to improve convergence."
    },
    {
      "type": "Hardware",
      "name": "NVIDIA P100 GPUs",
      "description": "Graphics processing units used for training the models."
    },
    {
      "type": "Dataset",
      "name": "newstest 2014",
      "description": "A dataset used for evaluating machine translation systems, specifically for English-to-German and English-to-French translations."
    },
    {
      "type": "Model",
      "name": "Deep-Att + Pos Unk",
      "description": "A model that combines deep attention mechanisms with positional unknown tokens."
    },
    {
      "type": "Model",
      "name": "GNMT + RL",
      "description": "Google's Neural Machine Translation model enhanced with reinforcement learning."
    },
    {
      "type": "Model",
      "name": "MoE",
      "description": "A mixture of experts model used in machine translation."
    },
    {
      "type": "Technique",
      "name": "Label Smoothing",
      "description": "A technique used during training to improve model accuracy by making the model less confident in its predictions."
    },
    {
      "type": "Model",
      "name": "Transformer (big)",
      "description": "A larger variant of the Transformer model that achieves state-of-the-art performance on translation tasks."
    },
    {
      "type": "Metric",
      "name": "BLEU score",
      "description": "A metric for evaluating the quality of text which has been machine-translated from one language to another."
    },
    {
      "type": "Technique",
      "name": "beam search",
      "description": "A search algorithm that explores a graph by expanding the most promising nodes in a limited set."
    },
    {
      "type": "Architecture",
      "name": "big model",
      "description": "A larger variant of the Transformer model that achieves higher performance on translation tasks."
    },
    {
      "type": "Hyperparameter",
      "name": "dropout rate",
      "description": "A regularization technique used to prevent overfitting by randomly setting a fraction of input units to 0."
    },
    {
      "type": "Hyperparameter",
      "name": "length penalty",
      "description": "A penalty applied during beam search to discourage overly long translations."
    },
    {
      "type": "Metric",
      "name": "Training Costs",
      "description": "An estimate of the resources required to train a model, often measured in floating point operations."
    },
    {
      "type": "Architecture",
      "name": "K80",
      "description": "A type of GPU used for training, with a sustained capacity of 2.8 TFLOPS."
    },
    {
      "type": "Architecture",
      "name": "K40",
      "description": "A type of GPU used for training, with a sustained capacity of 3.7 TFLOPS."
    },
    {
      "type": "Architecture",
      "name": "M40",
      "description": "A type of GPU used for training, with a sustained capacity of 6.0 TFLOPS."
    },
    {
      "type": "Architecture",
      "name": "P100",
      "description": "A type of GPU used for training, with a sustained capacity of 9.5 TFLOPS."
    },
    {
      "type": "Dataset",
      "name": "English-to-German Translation Dataset",
      "description": "A dataset used to evaluate the performance of translation models specifically for English to German language translation."
    },
    {
      "type": "Dataset",
      "name": "newstest 2013",
      "description": "A dataset used for evaluating English-to-German translation performance."
    },
    {
      "type": "Metric",
      "name": "PPL",
      "description": "Perplexity, a measurement of how well a probability distribution predicts a sample."
    },
    {
      "type": "Architecture",
      "name": "Transformer architecture variations",
      "description": "Different configurations of the Transformer model, varying parameters such as depth and width."
    },
    {
      "type": "Technique",
      "name": "Beam Search",
      "description": "A search algorithm used for decoding sequences in the model."
    },
    {
      "type": "Model",
      "name": "Single-Head Attention",
      "description": "A variant of attention mechanism that uses a single attention head."
    },
    {
      "type": "Dataset",
      "name": "Wall Street Journal (WSJ)",
      "description": "A portion of the Penn Treebank used for training the Transformer model."
    },
    {
      "type": "Dataset",
      "name": "Penn Treebank",
      "description": "A dataset containing annotated sentences used for various natural language processing tasks."
    },
    {
      "type": "Dataset",
      "name": "Berkley Parser corpora",
      "description": "A larger dataset used in a semi-supervised setting for training the Transformer model."
    },
    {
      "type": "Metric",
      "name": "state-of-the-art results",
      "description": "A benchmark indicating the best performance achieved in a specific task."
    },
    {
      "type": "Architecture",
      "name": "4-layer transformer",
      "description": "A specific configuration of the Transformer model with four layers."
    },
    {
      "type": "Metric",
      "name": "Vocabulary Size",
      "description": "The number of tokens used in the model, specified as 16K for WSJ only and 32K for semi-supervised settings."
    },
    {
      "type": "Model",
      "name": "English-to-German base translation model",
      "description": "A baseline translation model used for comparison in the experiments."
    },
    {
      "type": "Dataset",
      "name": "WSJ",
      "description": "The Wall Street Journal dataset used for training and evaluating parsers."
    },
    {
      "type": "Technique",
      "name": "Discriminative",
      "description": "A training approach where models are trained to discriminate between different outputs."
    },
    {
      "type": "Technique",
      "name": "Semi-supervised",
      "description": "A training approach that uses both labeled and unlabeled data."
    },
    {
      "type": "Technique",
      "name": "Multi-task",
      "description": "A training approach that involves learning multiple tasks simultaneously."
    },
    {
      "type": "Technique",
      "name": "Generative",
      "description": "A training approach that focuses on generating outputs based on input data."
    },
    {
      "type": "Model",
      "name": "Recurrent Neural Network Grammar",
      "description": "A model that was previously reported for sequence-to-sequence tasks."
    },
    {
      "type": "Model",
      "name": "Berkeley-Parser",
      "description": "A parsing model that the Transformer outperforms even when trained on a limited dataset."
    },
    {
      "type": "Technique",
      "name": "multi-headed self-attention",
      "description": "A technique used in the Transformer model to replace recurrent layers."
    },
    {
      "type": "Goal",
      "name": "Efficient Handling of Large Inputs",
      "description": "Research goal to apply attention mechanisms to large inputs like images, audio, and video."
    },
    {
      "type": "Goal",
      "name": "Less Sequential Generation",
      "description": "Research goal to make the generation process of models less sequential."
    },
    {
      "type": "Metric",
      "name": "BLEU Score",
      "description": "A metric for evaluating the quality of text which has been machine-translated from one language to another."
    },
    {
      "type": "Architecture",
      "name": "Sequence to Sequence Learning",
      "description": "A framework for modeling sequences where an input sequence is transformed into an output sequence."
    },
    {
      "type": "Technique",
      "name": "Self-training",
      "description": "A method for improving parsing accuracy by leveraging unannotated data."
    },
    {
      "type": "Model",
      "name": "Deep Reinforced Model",
      "description": "A model designed for abstractive summarization using reinforcement learning techniques."
    },
    {
      "type": "Technique",
      "name": "Tree Annotation",
      "description": "A method for learning accurate and interpretable tree structures in natural language processing."
    },
    {
      "type": "Dataset",
      "name": "Machine Translation Datasets",
      "description": "Datasets used for training models on translating text from one language to another."
    },
    {
      "type": "Dataset",
      "name": "WMT",
      "description": "A benchmark dataset used for evaluating machine translation systems."
    },
    {
      "type": "Architecture",
      "name": "Input-Input Layer",
      "description": "A layer in the Transformer model that processes input sequences."
    },
    {
      "type": "Concept",
      "name": "Anaphora Resolution",
      "description": "The task of determining which words in a sentence refer to the same entity."
    },
    {
      "type": "Model",
      "name": "LLaMA",
      "description": "A collection of foundation language models ranging from 7B to 65B parameters."
    },
    {
      "type": "Model",
      "name": "Chinchilla",
      "description": "A competitive language model with 70 billion parameters."
    },
    {
      "type": "Model",
      "name": "PaLM",
      "description": "A competitive language model with 540 billion parameters."
    },
    {
      "type": "Dataset",
      "name": "Publicly Available Datasets",
      "description": "Datasets that are accessible to the public and used for training LLaMA."
    },
    {
      "type": "Metric",
      "name": "Benchmarks",
      "description": "Standardized tests used to evaluate the performance of language models."
    },
    {
      "type": "Concept",
      "name": "Large Language Models (LLMs)",
      "description": "Models trained on massive corpora of texts that can perform tasks from textual instructions or examples."
    },
    {
      "type": "Dataset",
      "name": "Massive corpora of texts",
      "description": "Large datasets used to train language models."
    },
    {
      "type": "Metric",
      "name": "Scaling laws",
      "description": "Guidelines to determine optimal scaling of dataset and model sizes for training."
    },
    {
      "type": "Model",
      "name": "Hoffmann et al. (2022)",
      "description": "Research that shows smaller models trained on more data can outperform larger models given a compute budget."
    },
    {
      "type": "Model",
      "name": "Kaplan et al. (2020)",
      "description": "Research that discusses the scaling of models to achieve few-shot properties."
    },
    {
      "type": "Model",
      "name": "Brown et al. (2020)",
      "description": "Research that highlights the ability of LLMs to perform tasks from textual instructions."
    },
    {
      "type": "Model",
      "name": "Chowdhery et al. (2022)",
      "description": "Research focused on further scaling language models."
    },
    {
      "type": "Model",
      "name": "Rae et al. (2021)",
      "description": "Research that contributes to the understanding of scaling language models."
    },
    {
      "type": "Model",
      "name": "10 B model",
      "description": "A language model recommended by Hoffmann et al. (2022) to be trained on 200 B tokens."
    },
    {
      "type": "Model",
      "name": "7 B model",
      "description": "A smaller language model that continues to improve performance even after training on 1 T tokens."
    },
    {
      "type": "Dataset",
      "name": "200 B tokens",
      "description": "The dataset size recommended for training the 10 B model."
    },
    {
      "type": "Dataset",
      "name": "1 T tokens",
      "description": "The dataset size on which the 7 B model continues to improve performance."
    },
    {
      "type": "Concept",
      "name": "Inference Budget",
      "description": "The critical budget consideration when serving a language model at scale."
    },
    {
      "type": "Concept",
      "name": "Training Compute Budget",
      "description": "The budget for training a model, which is often prioritized over inference budget."
    },
    {
      "type": "Model",
      "name": "PaLM-540B",
      "description": "A large language model that is also competitive with LLaMA's 65 B-parameter model."
    },
    {
      "type": "Dataset",
      "name": "Publicly Available Data",
      "description": "Data used by LLaMA that is compatible with open-sourcing, unlike data used by many existing models."
    },
    {
      "type": "Model",
      "name": "Pa LM",
      "description": "A language model that is mentioned as a competitor to LLaMA."
    },
    {
      "type": "Model",
      "name": "OPT",
      "description": "An exception in the landscape of language models that uses publicly available data."
    },
    {
      "type": "Model",
      "name": "GPT-Neo X",
      "description": "A language model that is mentioned as using publicly available data."
    },
    {
      "type": "Model",
      "name": "BLOOM",
      "description": "A language model that is noted for its use of publicly available data."
    },
    {
      "type": "Model",
      "name": "GLM",
      "description": "A language model that is also an exception in the context of publicly available data."
    },
    {
      "type": "Metric",
      "name": "Standard Benchmarks",
      "description": "A set of benchmarks used to compare the performance of LLaMA with other language models."
    },
    {
      "type": "Concept",
      "name": "Biases and Toxicity",
      "description": "Issues that are examined in the models, particularly in the context of responsible AI."
    },
    {
      "type": "Technique",
      "name": "Chinchilla scaling laws",
      "description": "Scaling laws that guide the training of large language models."
    },
    {
      "type": "Dataset",
      "name": "Common Crawl",
      "description": "A dataset consisting of web-crawled data used for training language models."
    },
    {
      "type": "Technique",
      "name": "CCNet pipeline",
      "description": "A preprocessing pipeline used to deduplicate and identify languages in the Common Crawl data."
    },
    {
      "type": "Dataset",
      "name": "Pre-training Data",
      "description": "A mixture of several sources covering diverse domains for training LLaMA."
    },
    {
      "type": "Dataset",
      "name": "C4",
      "description": "A dataset derived from Common Crawl, specifically processed for language model training, including deduplication and language identification."
    },
    {
      "type": "Technique",
      "name": "Deduplication",
      "description": "The process of removing duplicate entries from a dataset to ensure data quality."
    },
    {
      "type": "Technique",
      "name": "Language Identification",
      "description": "The process of determining the language of a given text, used to filter non-English pages."
    },
    {
      "type": "Technique",
      "name": "Quality Filtering",
      "description": "The process of removing low-quality content based on certain heuristics."
    },
    {
      "type": "Model",
      "name": "n-gram Language Model",
      "description": "A statistical language model that predicts the next item in a sequence based on the previous n items."
    },
    {
      "type": "Model",
      "name": "Linear Classifier",
      "description": "A model used to classify data points into categories based on a linear predictor function."
    },
    {
      "type": "Dataset",
      "name": "Github",
      "description": "A dataset containing public projects available on GitHub."
    },
    {
      "type": "Dataset",
      "name": "Books",
      "description": "A dataset containing various books."
    },
    {
      "type": "Dataset",
      "name": "Ar Xiv",
      "description": "A dataset containing research papers from the ArXiv repository."
    },
    {
      "type": "Dataset",
      "name": "Stack Exchange",
      "description": "A dataset containing questions and answers from the Stack Exchange platform."
    },
    {
      "type": "Technique",
      "name": "Heuristics",
      "description": "Methods used for quality filtering based on specific criteria."
    },
    {
      "type": "Dataset",
      "name": "C",
      "description": "A dataset contributing 15.0% to the pre-training data, with a size of 783 GB."
    },
    {
      "type": "Dataset",
      "name": "Gutenberg Project",
      "description": "A collection of public domain books included in the training dataset."
    },
    {
      "type": "Dataset",
      "name": "Books 3",
      "description": "A section of The Pile dataset used for training large language models."
    },
    {
      "type": "Metric",
      "name": "Sampling Proportion",
      "description": "The proportion of each dataset used in the pre-training process."
    },
    {
      "type": "Metric",
      "name": "Disk Size",
      "description": "The size of each dataset in gigabytes."
    },
    {
      "type": "Dataset",
      "name": "The Pile",
      "description": "A publicly available dataset for training large language models."
    },
    {
      "type": "Dataset",
      "name": "arXiv",
      "description": "A repository of scientific papers, processed to add scientific data to the dataset."
    },
    {
      "type": "Technique",
      "name": "Byte-Pair Encoding (BPE)",
      "description": "An algorithm used for tokenizing data by merging the most frequent pairs of bytes."
    },
    {
      "type": "Dataset",
      "name": "28 largest websites",
      "description": "A collection of data from the largest websites used for training the model."
    },
    {
      "type": "Implementation",
      "name": "SentencePiece",
      "description": "A library used for implementing the byte-pair encoding algorithm."
    },
    {
      "type": "Concept",
      "name": "HTML tags",
      "description": "Markup language elements that were removed from the text data."
    },
    {
      "type": "Concept",
      "name": "UTF-8 characters",
      "description": "A character encoding standard that was decomposed into bytes for unknown characters."
    },
    {
      "type": "Technique",
      "name": "Pre-normalization",
      "description": "An improvement to the transformer architecture inspired by GPT-3."
    },
    {
      "type": "Metric",
      "name": "Tokens",
      "description": "The unit of training data, with the entire dataset containing roughly 1.4 trillion tokens."
    },
    {
      "type": "Technique",
      "name": "Swi GLU activation function",
      "description": "An activation function introduced by Shazeer (2020) that replaces the ReLU non-linearity to improve performance."
    },
    {
      "type": "Technique",
      "name": "Rotary Embeddings",
      "description": "A method that replaces absolute positional embeddings with rotary positional embeddings (RoPE) at each layer of the network."
    },
    {
      "type": "Model",
      "name": "GPTNeo",
      "description": "A model that inspired the use of rotary embeddings."
    },
    {
      "type": "Model",
      "name": "LLaMA-7B",
      "description": "A language model with 7 billion parameters."
    },
    {
      "type": "Model",
      "name": "LLaMA-13B",
      "description": "A language model with 13 billion parameters."
    },
    {
      "type": "Model",
      "name": "LLaMA-33B",
      "description": "A language model with 33 billion parameters."
    },
    {
      "type": "Model",
      "name": "LLaMA-65B",
      "description": "A language model with 65 billion parameters."
    },
    {
      "type": "Technique",
      "name": "AdamW optimizer",
      "description": "An optimization algorithm used for training the models."
    },
    {
      "type": "Metric",
      "name": "Training loss",
      "description": "A measure of the model's performance during training."
    },
    {
      "type": "Concept",
      "name": "Cosine learning rate schedule",
      "description": "A method for adjusting the learning rate during training."
    },
    {
      "type": "Concept",
      "name": "Weight decay",
      "description": "A regularization technique used to prevent overfitting."
    },
    {
      "type": "Concept",
      "name": "Gradient clipping",
      "description": "A technique to prevent exploding gradients during training."
    },
    {
      "type": "Dataset",
      "name": "1.4T tokens",
      "description": "The dataset used for training LLaMA-33B and LLaMA-65B models."
    },
    {
      "type": "Dataset",
      "name": "1.0T tokens",
      "description": "The dataset used for training LLaMA-7B and LLaMA-13B models."
    },
    {
      "type": "Technique",
      "name": "Causal Multi-Head Attention",
      "description": "An efficient implementation used to reduce memory usage and runtime in language modeling."
    },
    {
      "type": "Library",
      "name": "xformers",
      "description": "A library that provides an efficient implementation of causal multi-head attention."
    },
    {
      "type": "Technique",
      "name": "Checkpointing",
      "description": "A method to reduce the amount of activations recomputed during the backward pass."
    },
    {
      "type": "Technique",
      "name": "Backward Pass Optimization",
      "description": "An optimization technique that avoids storing attention weights and computing masked key/query scores."
    },
    {
      "type": "Research",
      "name": "Rabe and Staats (2021)",
      "description": "Inspiration for the efficient implementation of causal multi-head attention."
    },
    {
      "type": "Research",
      "name": "Dao et al. (2022)",
      "description": "Referenced work that contributes to the backward pass optimization."
    },
    {
      "type": "Model",
      "name": "Gopher",
      "description": "A language model with 280 billion parameters."
    },
    {
      "type": "Dataset",
      "name": "Bool Q",
      "description": "A dataset for evaluating common sense reasoning."
    },
    {
      "type": "Dataset",
      "name": "PIQA",
      "description": "A dataset for evaluating physical commonsense reasoning."
    },
    {
      "type": "Dataset",
      "name": "SIQA",
      "description": "A dataset for evaluating social commonsense reasoning."
    },
    {
      "type": "Dataset",
      "name": "Hella Swag",
      "description": "A dataset for evaluating commonsense reasoning."
    },
    {
      "type": "Dataset",
      "name": "Wino",
      "description": "A dataset for evaluating pronoun resolution."
    },
    {
      "type": "Dataset",
      "name": "Grande",
      "description": "A dataset for evaluating commonsense reasoning."
    },
    {
      "type": "Dataset",
      "name": "ARC-e",
      "description": "A dataset for evaluating elementary science questions."
    },
    {
      "type": "Dataset",
      "name": "ARC-c",
      "description": "A dataset for evaluating challenging science questions."
    },
    {
      "type": "Dataset",
      "name": "OBQA",
      "description": "A dataset for evaluating open-book question answering."
    },
    {
      "type": "Metric",
      "name": "Zero-shot performance",
      "description": "Performance metric for evaluating models without fine-tuning."
    },
    {
      "type": "Technique",
      "name": "Model and sequence parallelism",
      "description": "Technique to reduce memory usage during model training."
    },
    {
      "type": "Dataset",
      "name": "1.4 T tokens dataset",
      "description": "The dataset used for training the LLaMA model, containing 1.4 trillion tokens."
    },
    {
      "type": "Metric",
      "name": "tokens/sec/GPU",
      "description": "A performance metric indicating the number of tokens processed per second per GPU."
    },
    {
      "type": "Technique",
      "name": "zero-shot",
      "description": "A task type where the model provides answers based on a textual description without prior examples."
    },
    {
      "type": "Technique",
      "name": "few-shot",
      "description": "A task type where the model is given a few examples and generates answers based on them."
    },
    {
      "type": "Model",
      "name": "GPT-J",
      "description": "An open-source language model developed by EleutherAI."
    },
    {
      "type": "Model",
      "name": "GPT-Neo",
      "description": "An open-source language model developed by EleutherAI."
    },
    {
      "type": "Model",
      "name": "OPT-IML",
      "description": "An instruction-tuned model based on OPT."
    },
    {
      "type": "Model",
      "name": "Flan-PaLM",
      "description": "An instruction-tuned model based on PaLM."
    },
    {
      "type": "Dataset",
      "name": "Open Book QA",
      "description": "A dataset for evaluating question answering models."
    },
    {
      "type": "Metric",
      "name": "Likelihood",
      "description": "A measure used to select the most appropriate completion based on context."
    },
    {
      "type": "Dataset",
      "name": "Natural Questions",
      "description": "A benchmark for evaluating question answering systems."
    },
    {
      "type": "Metric",
      "name": "Likelihood Normalization",
      "description": "A method for normalizing likelihood based on context."
    },
    {
      "type": "Dataset",
      "name": "Wino Grande",
      "description": "A dataset for evaluating gender bias in language models."
    },
    {
      "type": "Dataset",
      "name": "ARC easy and challenge",
      "description": "A dataset for evaluating models on science questions."
    },
    {
      "type": "Model",
      "name": "Chinchilla-70B",
      "description": "A large language model that is compared against LLaMA-65B."
    },
    {
      "type": "Metric",
      "name": "Exact Match Performance",
      "description": "A metric used to evaluate the performance of models in question answering."
    },
    {
      "type": "Dataset",
      "name": "RACE",
      "description": "A reading comprehension benchmark dataset collected from English exams."
    },
    {
      "type": "Metric",
      "name": "Zero-shot accuracy",
      "description": "A performance metric indicating how well a model performs without prior training on specific tasks."
    },
    {
      "type": "Metric",
      "name": "Exact match performance",
      "description": "A metric used to evaluate the correctness of answers in QA tasks."
    },
    {
      "type": "Model",
      "name": "Minerva",
      "description": "A series of PaLM models fine-tuned on mathematical data."
    },
    {
      "type": "Dataset",
      "name": "MATH",
      "description": "A dataset of 12K middle school and high school mathematics problems."
    },
    {
      "type": "Dataset",
      "name": "GSM 8k",
      "description": "A set of middle school mathematical problems."
    },
    {
      "type": "Metric",
      "name": "maj 1@k",
      "description": "An evaluation metric where k samples are generated for each problem and majority voting is performed."
    },
    {
      "type": "Model",
      "name": "Minerva-62B",
      "description": "A language model that is compared against LLaMA-65B on the GSM 8k dataset."
    },
    {
      "type": "Benchmark",
      "name": "Human Eval",
      "description": "A benchmark for evaluating code generation from natural language descriptions."
    },
    {
      "type": "Benchmark",
      "name": "MBPP",
      "description": "A benchmark for evaluating the ability of models to write code from natural language descriptions."
    },
    {
      "type": "Technique",
      "name": "Majority Voting",
      "description": "A method used for evaluating model performance by generating samples and selecting the most common output."
    },
    {
      "type": "Model",
      "name": "LaMDA",
      "description": "A language model that has not been fine-tuned on code."
    },
    {
      "type": "Metric",
      "name": "pass@1",
      "description": "A metric used to evaluate the performance of models on test cases."
    },
    {
      "type": "Model",
      "name": "PaLM-Coder",
      "description": "A variant of PaLM that increases performance on code tasks."
    },
    {
      "type": "Metric",
      "name": "pass@100",
      "description": "A performance metric used to evaluate model accuracy on code tasks."
    },
    {
      "type": "Metric",
      "name": "pass@80",
      "description": "A performance metric used to evaluate model accuracy on code tasks."
    },
    {
      "type": "Model",
      "name": "Pa LM-Coder",
      "description": "A variant of Pa LM that is fine-tuned on code-specific tokens."
    },
    {
      "type": "Dataset",
      "name": "MMLU",
      "description": "Massive Multitask Language Understanding benchmark consisting of multiple choice questions."
    },
    {
      "type": "Metric",
      "name": "pass@ score",
      "description": "A performance metric reported for code generation tasks."
    },
    {
      "type": "Concept",
      "name": "3-shot prompts",
      "description": "A setting where the model generates outputs based on three provided examples."
    },
    {
      "type": "Concept",
      "name": "5-shot setting",
      "description": "A setting used for evaluating models on the MMLU benchmark."
    },
    {
      "type": "Concept",
      "name": "pre-training data",
      "description": "The data used to train the model before evaluation."
    },
    {
      "type": "Dataset",
      "name": "ArXiv",
      "description": "A dataset used in the pre-training of LLaMA."
    },
    {
      "type": "Dataset",
      "name": "Gutenberg",
      "description": "A dataset used in the pre-training of LLaMA."
    },
    {
      "type": "Metric",
      "name": "training perplexity",
      "description": "A metric used to track the performance of models during training."
    },
    {
      "type": "Model",
      "name": "LLaMA-I",
      "description": "An instruct model derived from LLaMA-65B, showing improved performance on MMLU."
    },
    {
      "type": "Model",
      "name": "OPT-30B",
      "description": "A language model used for comparison in instruction fine-tuning."
    },
    {
      "type": "Model",
      "name": "GLM-120B",
      "description": "A language model used for comparison in instruction fine-tuning."
    },
    {
      "type": "Model",
      "name": "PaLM-62B",
      "description": "A language model used for comparison in instruction fine-tuning."
    },
    {
      "type": "Model",
      "name": "OPT-IML-Max-30B",
      "description": "A language model used for comparison in instruction fine-tuning."
    },
    {
      "type": "Model",
      "name": "Flan-T5-XXL-11B",
      "description": "A language model used for comparison in instruction fine-tuning."
    },
    {
      "type": "Model",
      "name": "Flan-PaLM-62B",
      "description": "A language model used for comparison in instruction fine-tuning."
    },
    {
      "type": "Model",
      "name": "Flan-PaLM-cont-62B",
      "description": "A language model used for comparison in instruction fine-tuning."
    },
    {
      "type": "Model",
      "name": "GPT code-davinci-002",
      "description": "The state-of-the-art model on MMLU, achieving 77.4%."
    },
    {
      "type": "Metric",
      "name": "Performance on MMLU",
      "description": "A metric used to evaluate the effectiveness of language models on the MMLU dataset."
    },
    {
      "type": "Concept",
      "name": "Instruction Fine-tuning",
      "description": "A technique used to improve the performance of language models on specific tasks."
    },
    {
      "type": "Concept",
      "name": "Bias, Toxicity and Misinformation",
      "description": "Issues related to large language models reproducing and amplifying biases present in training data."
    },
    {
      "type": "Model",
      "name": "LLaMA-65 B",
      "description": "A large language model evaluated for its potential to generate toxic content and detect stereotypes."
    },
    {
      "type": "Dataset",
      "name": "Web Data",
      "description": "A large proportion of the training data for LLaMA-65 B sourced from the web."
    },
    {
      "type": "Metric",
      "name": "Toxic Content Production Benchmark",
      "description": "Benchmarks used to evaluate the model's potential for generating toxic content."
    },
    {
      "type": "Metric",
      "name": "Stereotypes Detection Benchmark",
      "description": "Benchmarks used to evaluate the model's ability to detect stereotypes."
    },
    {
      "type": "Concept",
      "name": "Bias in Language Models",
      "description": "The phenomenon where language models reproduce and amplify biases present in their training data."
    },
    {
      "type": "Concept",
      "name": "Toxic Content Generation",
      "description": "The generation of offensive or harmful content by language models."
    },
    {
      "type": "Dataset",
      "name": "Real Toxicity Prompts",
      "description": "A benchmark used to evaluate the generation of toxic language by language models."
    },
    {
      "type": "Metric",
      "name": "toxicity score",
      "description": "A score ranging from 0 (non-toxic) to 1 (toxic) used to evaluate the toxicity of model outputs."
    },
    {
      "type": "API",
      "name": "Perspective API",
      "description": "A third-party API used to automatically evaluate the toxicity score of model outputs."
    },
    {
      "type": "Concept",
      "name": "Toxicity",
      "description": "The degree to which generated text may be harmful or offensive."
    },
    {
      "type": "Technique",
      "name": "Greedy Decoder",
      "description": "A decoding strategy used to generate text from language models."
    },
    {
      "type": "Concept",
      "name": "Sampling Strategy",
      "description": "The method used to select prompts for evaluation."
    },
    {
      "type": "Work",
      "name": "Zhang et al. (2022)",
      "description": "Previous work referenced in the context of model size and performance."
    },
    {
      "type": "Dataset",
      "name": "Crow S-Pairs",
      "description": "A dataset used to measure biases in language models across various categories."
    },
    {
      "type": "Concept",
      "name": "Bias",
      "description": "The tendency of a model to favor certain stereotypes over others."
    },
    {
      "type": "Model",
      "name": "OPT-175B",
      "description": "A large language model developed by Meta AI, used for comparison in bias evaluation."
    },
    {
      "type": "Dataset",
      "name": "Wino Gender",
      "description": "A co-reference resolution dataset used to evaluate gender biases in models."
    },
    {
      "type": "Dataset",
      "name": "Wino Gender dataset",
      "description": "A dataset used to evaluate co-reference resolution performance with respect to gender biases in pronouns."
    },
    {
      "type": "Technique",
      "name": "Co-reference resolution",
      "description": "A technique to determine which words in a sentence refer to the same entity."
    },
    {
      "type": "Concept",
      "name": "Societal biases",
      "description": "Preconceived notions associated with occupations that may affect model performance."
    },
    {
      "type": "Pronoun",
      "name": "her/her/she",
      "description": "A set of pronouns used in the evaluation of co-reference resolution."
    },
    {
      "type": "Pronoun",
      "name": "his/him/he",
      "description": "A set of pronouns used in the evaluation of co-reference resolution."
    },
    {
      "type": "Pronoun",
      "name": "their/them/someone",
      "description": "A set of pronouns used in the evaluation of co-reference resolution."
    },
    {
      "type": "Metric",
      "name": "co-reference scores",
      "description": "Scores that measure the performance of the model in resolving co-reference for different pronouns."
    },
    {
      "type": "Concept",
      "name": "gender bias",
      "description": "A potential bias observed in the model's performance related to gendered pronouns."
    },
    {
      "type": "Metric",
      "name": "Truthful QA",
      "description": "A metric that measures the truthfulness of a model's claims."
    },
    {
      "type": "Concept",
      "name": "Societal Biases",
      "description": "Biases that affect the performance of models based on gender and occupation."
    },
    {
      "type": "Concept",
      "name": "Literal Truth",
      "description": "The definition of truth in the context of real-world claims."
    },
    {
      "type": "Concept",
      "name": "Misformation",
      "description": "The risk of a model generating misinformation or false claims."
    },
    {
      "type": "Dataset",
      "name": "Adversarial Questions Benchmark",
      "description": "A benchmark designed to evaluate the risks of a model to generate misinformation, consisting of questions written in diverse styles covering 38 categories."
    },
    {
      "type": "Metric",
      "name": "Co-reference resolution accuracy",
      "description": "A metric used to evaluate the performance of LLaMA models on pronoun resolution."
    },
    {
      "type": "Metric",
      "name": "truthful models",
      "description": "A category used to measure the accuracy of model responses."
    },
    {
      "type": "Metric",
      "name": "informative",
      "description": "A category used to measure the quality of information provided by the model."
    },
    {
      "type": "Concept",
      "name": "carbon footprint",
      "description": "The total carbon emissions resulting from the energy consumption of model training."
    },
    {
      "type": "Technique",
      "name": "energy consumption estimation",
      "description": "A method to calculate the energy used in training models, referenced from Wu et al. (2022)."
    },
    {
      "type": "Concept",
      "name": "Power Usage Effectiveness (PUE)",
      "description": "A metric used to determine the efficiency of power usage in data centers."
    },
    {
      "type": "Metric",
      "name": "Carbon Emission",
      "description": "The amount of carbon dioxide equivalent emitted during the training of models."
    },
    {
      "type": "Metric",
      "name": "Carbon Intensity Factor",
      "description": "The average carbon intensity factor used for calculations, set at 0.385 kg CO2 eq/KWh."
    },
    {
      "type": "Metric",
      "name": "t CO2 eq",
      "description": "A metric used to estimate carbon emissions from model training."
    },
    {
      "type": "Technique",
      "name": "next token prediction",
      "description": "A core problem in natural language processing framed in the context of language models."
    },
    {
      "type": "Concept",
      "name": "language modeling",
      "description": "A benchmark proposed to measure progress toward artificial intelligence using language."
    },
    {
      "type": "Architecture",
      "name": "n-gram models",
      "description": "Traditional language models based on count statistics of n-grams."
    },
    {
      "type": "Technique",
      "name": "smoothing techniques",
      "description": "Methods proposed to improve the estimation of rare events in language modeling."
    },
    {
      "type": "Technique",
      "name": "neural networks",
      "description": "A class of models that have been successfully applied to language modeling tasks in recent decades."
    },
    {
      "type": "Model",
      "name": "LLaMA-7",
      "description": "A language model with 7 billion parameters."
    },
    {
      "type": "Model",
      "name": "LLaMA-13",
      "description": "A language model with 13 billion parameters."
    },
    {
      "type": "Model",
      "name": "LLaMA-33",
      "description": "A language model with 33 billion parameters."
    },
    {
      "type": "Model",
      "name": "LLaMA-65",
      "description": "A language model with 65 billion parameters."
    },
    {
      "type": "Model",
      "name": "BLOOM-175B",
      "description": "A language model with 175 billion parameters."
    },
    {
      "type": "Metric",
      "name": "Power Consumption",
      "description": "The total power consumed during the training of models, measured in MWh."
    },
    {
      "type": "Architecture",
      "name": "NVLink",
      "description": "A high-speed interconnect technology used in GPU systems."
    },
    {
      "type": "Model",
      "name": "Feed Forward Models",
      "description": "A type of neural network model introduced by Bengio et al. in 2000."
    },
    {
      "type": "Model",
      "name": "LSTMs",
      "description": "Long Short-Term Memory networks, a type of recurrent neural network introduced by Hochreiter and Schmidhuber in 1997."
    },
    {
      "type": "Model",
      "name": "Transformer Networks",
      "description": "A model architecture based on self-attention, introduced by Vaswani et al. in 2017."
    },
    {
      "type": "Technique",
      "name": "Stupid Backoff",
      "description": "A simple smoothing technique used in language models."
    },
    {
      "type": "Technique",
      "name": "Kneser-Ney Smoothing",
      "description": "A smoothing technique that was scaled to Web-scale data by Heafield et al. in 2013."
    },
    {
      "type": "Metric",
      "name": "n-grams",
      "description": "Contiguous sequences of n items from a given sample of text."
    },
    {
      "type": "Technique",
      "name": "Kneser-Ney smoothing",
      "description": "A technique used for smoothing in language models."
    },
    {
      "type": "Model",
      "name": "5-gram model",
      "description": "A language model trained on 975 billion tokens from Common Crawl."
    },
    {
      "type": "Metric",
      "name": "One Billion Word benchmark",
      "description": "A large scale training dataset introduced to measure the progress of language models."
    },
    {
      "type": "Model",
      "name": "T5",
      "description": "A text-to-text transformer model that achieved significant results in NLP."
    },
    {
      "type": "Model",
      "name": "Jurassic-1",
      "description": "A large language model developed after GPT-3."
    },
    {
      "type": "Model",
      "name": "Megatron-Turing NLG",
      "description": "A large language model that follows the advancements of GPT-3."
    },
    {
      "type": "Concept",
      "name": "Power Laws",
      "description": "Relationships showing the impact of model and dataset sizes on performance."
    },
    {
      "type": "Technique",
      "name": "Learning Rate Schedule",
      "description": "A method adapted for scaling datasets to improve model performance."
    },
    {
      "type": "Dataset",
      "name": "Publicly available data",
      "description": "Data used for training the LLaMA models without proprietary datasets."
    },
    {
      "type": "Technique",
      "name": "Finetuning on instructions",
      "description": "A technique that leads to promising results when applied to LLaMA models."
    },
    {
      "type": "Technique",
      "name": "Finetuning",
      "description": "A technique observed to lead to promising results when applied to language models on instructions."
    },
    {
      "type": "Dataset",
      "name": "Pretraining Corpora",
      "description": "Larger datasets used for pretraining models, which the authors plan to utilize in future work."
    },
    {
      "type": "Team",
      "name": "xformers team",
      "description": "Team involved in the development and support of LLaMA."
    },
    {
      "type": "Technique",
      "name": "data deduplication",
      "description": "Process of removing duplicate data to improve model training."
    },
    {
      "type": "Team",
      "name": "AI infra team",
      "description": "Team providing infrastructure support for AI-related tasks."
    },
    {
      "type": "Metric",
      "name": "training stability",
      "description": "Measure of how consistently a model can be trained without issues."
    },
    {
      "type": "Team",
      "name": "evaluation team",
      "description": "Team responsible for evaluating the performance of the model."
    },
    {
      "type": "Dataset",
      "name": "data collection",
      "description": "Process of gathering data for training and evaluation purposes."
    },
    {
      "type": "Model",
      "name": "GPT-NeoX-20B",
      "description": "An open-source autoregressive language model."
    },
    {
      "type": "Concept",
      "name": "Neural Probabilistic Language Model",
      "description": "A model proposed by Yoshua Bengio et al. for language processing."
    },
    {
      "type": "Metric",
      "name": "Statistical Approach",
      "description": "A method used in machine translation that relies on statistical models to improve translation accuracy."
    },
    {
      "type": "Conference",
      "name": "EMNLP-CoNLL",
      "description": "A joint conference on empirical methods in natural language processing and computational natural language learning."
    },
    {
      "type": "Dataset",
      "name": "One Billion Word Benchmark",
      "description": "A benchmark for measuring progress in statistical language modeling."
    },
    {
      "type": "Technique",
      "name": "Language Model Evaluation",
      "description": "The process of assessing the performance of language models, particularly in code."
    },
    {
      "type": "Dataset",
      "name": "Code Dataset",
      "description": "A dataset used for training and evaluating language models on code-related tasks."
    },
    {
      "type": "Model",
      "name": "Palm",
      "description": "A language model that scales with pathways."
    },
    {
      "type": "Technique",
      "name": "Scaling language modeling",
      "description": "A technique for improving the performance of language models."
    },
    {
      "type": "Dataset",
      "name": "BoolQ",
      "description": "A dataset exploring the difficulty of natural yes/no questions."
    },
    {
      "type": "Dataset",
      "name": "ARC",
      "description": "A dataset for evaluating AI's reasoning capabilities in question answering."
    },
    {
      "type": "Technique",
      "name": "Instruction-Finetuning",
      "description": "A technique for fine-tuning language models on specific tasks."
    },
    {
      "type": "Dataset",
      "name": "Math Word Problems",
      "description": "Dataset used for training verifiers to solve math word problems."
    },
    {
      "type": "Model",
      "name": "Incoder",
      "description": "A generative model for code infilling and synthesis."
    },
    {
      "type": "Dataset",
      "name": "Realtoxicityprompts",
      "description": "A dataset for evaluating neural toxic degeneration in language models."
    },
    {
      "type": "Dataset",
      "name": "Math Dataset",
      "description": "A dataset used for measuring mathematical problem solving."
    },
    {
      "type": "Metric",
      "name": "Massive Multitask Language Understanding",
      "description": "A metric for evaluating language understanding across multiple tasks."
    },
    {
      "type": "Technique",
      "name": "Compute-optimal training",
      "description": "A method for training large language models efficiently."
    },
    {
      "type": "Technique",
      "name": "Instruction meta learning",
      "description": "A technique for scaling language model instruction learning."
    },
    {
      "type": "Metric",
      "name": "Scaling Laws",
      "description": "Principles that describe how model performance scales with size and data."
    },
    {
      "type": "Technique",
      "name": "Activation Re-computation",
      "description": "A technique to reduce memory usage in large transformer models."
    },
    {
      "type": "Metric",
      "name": "Quantifying Social Biases",
      "description": "A method to assess social biases in contextual word representations."
    },
    {
      "type": "Dataset",
      "name": "TruthfulQA",
      "description": "Measuring how models mimic human falsehoods."
    },
    {
      "type": "Concept",
      "name": "Quantitative Reasoning",
      "description": "Problems solved using language models."
    },
    {
      "type": "Technique",
      "name": "Decoupled Weight Decay Regularization",
      "description": "A technique for regularizing neural networks introduced by Ilya Loshchilov and Frank Hutter."
    },
    {
      "type": "Dataset",
      "name": "Crow S-pairs",
      "description": "A challenge dataset for measuring social biases in masked language models."
    },
    {
      "type": "Model",
      "name": "Recurrent Neural Network",
      "description": "A type of neural network model used for language modeling."
    },
    {
      "type": "Dataset",
      "name": "Open Book Question Answering Dataset",
      "description": "A dataset for evaluating open book question answering capabilities."
    },
    {
      "type": "Model",
      "name": "Codegen",
      "description": "An open large language model for code with multi-turn program synthesis."
    },
    {
      "type": "Technique",
      "name": "Human Feedback",
      "description": "Training language models to follow instructions with human feedback."
    },
    {
      "type": "Technique",
      "name": "Generative Pre-training",
      "description": "A method for improving language understanding by pre-training models on large datasets."
    },
    {
      "type": "Architecture",
      "name": "Text-to-Text Transformer",
      "description": "A unified architecture for transfer learning in NLP tasks introduced by Raffel et al."
    },
    {
      "type": "Dataset",
      "name": "Winogrande",
      "description": "An adversarial Winograd schema challenge at scale."
    },
    {
      "type": "Dataset",
      "name": "SocialIQA",
      "description": "Commonsense reasoning about social interactions."
    },
    {
      "type": "Metric",
      "name": "Generalization Error",
      "description": "A measure of a model's ability to generalize to unseen data."
    },
    {
      "type": "Conference",
      "name": "NAACL-HLT 2018",
      "description": "A conference focused on natural language processing."
    },
    {
      "type": "Model",
      "name": "Bloom",
      "description": "A 176 billion parameter open-access multilingual language model."
    },
    {
      "type": "Concept",
      "name": "Subword Units",
      "description": "A method for handling rare words in neural machine translation."
    },
    {
      "type": "Metric",
      "name": "Bias in Language Generation",
      "description": "The tendency of language models to produce biased outputs."
    },
    {
      "type": "Model",
      "name": "Megatron-Turing NLG 530B",
      "description": "A large-scale generative language model."
    },
    {
      "type": "Model",
      "name": "Roformer",
      "description": "An architecture for language modeling."
    },
    {
      "type": "Technique",
      "name": "Model Parallelism",
      "description": "A technique used to train large models by distributing them across multiple devices."
    },
    {
      "type": "Technique",
      "name": "DeepSpeed",
      "description": "A deep learning optimization library that enables training of large models."
    },
    {
      "type": "Model",
      "name": "Lamda",
      "description": "Language models for dialog applications referenced in the paper."
    },
    {
      "type": "Model",
      "name": "GPT-J-6B",
      "description": "A 6 Billion Parameter Autoregressive Language Model referenced in the paper."
    },
    {
      "type": "Model",
      "name": "Self-consistency",
      "description": "A technique that improves chain of thought reasoning in language models."
    },
    {
      "type": "Dataset",
      "name": "CCNet",
      "description": "A dataset extracted from web crawl data for high-quality monolingual datasets."
    },
    {
      "type": "Concept",
      "name": "Sustainable AI",
      "description": "Focuses on the environmental implications, challenges, and opportunities of AI."
    },
    {
      "type": "Dataset",
      "name": "Hellaswag",
      "description": "A dataset used to evaluate commonsense reasoning in language models."
    },
    {
      "type": "Model",
      "name": "GLM-130B",
      "description": "An open bilingual pre-trained model mentioned in the context."
    },
    {
      "type": "Technique",
      "name": "Root Mean Square Layer Normalization",
      "description": "Technique discussed in the context of improving model training."
    },
    {
      "type": "Metric",
      "name": "Exact Match",
      "description": "A standard metric used to evaluate generated answers by checking if they match any answer in the list after normalization."
    },
    {
      "type": "Technique",
      "name": "Greedy Decoding",
      "description": "A technique used to generate answers by stopping at the first line break, final dot, or comma."
    },
    {
      "type": "Technique",
      "name": "1-shot setting",
      "description": "A setting where the model is provided with one example to learn from."
    },
    {
      "type": "Metric",
      "name": "STEM",
      "description": "A category of subjects including Science, Technology, Engineering, and Mathematics."
    },
    {
      "type": "Metric",
      "name": "Other",
      "description": "A category for subjects not classified under STEM."
    },
    {
      "type": "Metric",
      "name": "Abstract Algebra",
      "description": "A branch of mathematics dealing with algebraic structures."
    },
    {
      "type": "Metric",
      "name": "Anatomy",
      "description": "The branch of biology concerned with the study of the structure of organisms."
    },
    {
      "type": "Metric",
      "name": "Astronomy",
      "description": "The scientific study of celestial bodies and the universe."
    },
    {
      "type": "Metric",
      "name": "Business Ethics",
      "description": "The study of proper business policies and practices regarding potentially controversial issues."
    },
    {
      "type": "Metric",
      "name": "Clinical Knowledge",
      "description": "Knowledge related to clinical practices and healthcare."
    },
    {
      "type": "Metric",
      "name": "College Biology",
      "description": "The study of living organisms, their structure, function, growth, and evolution."
    },
    {
      "type": "Metric",
      "name": "College Chemistry",
      "description": "The study of matter, its properties, and the changes it undergoes."
    },
    {
      "type": "Metric",
      "name": "College Computer Science",
      "description": "The study of computers and computational systems."
    },
    {
      "type": "Metric",
      "name": "College Mathematics",
      "description": "The study of numbers, quantities, shapes, and their relationships."
    },
    {
      "type": "Metric",
      "name": "College Medicine",
      "description": "The study of health and healing practices."
    },
    {
      "type": "Metric",
      "name": "College Physics",
      "description": "The study of matter, energy, and the fundamental forces of nature."
    },
    {
      "type": "Metric",
      "name": "Computer Security",
      "description": "The protection of computer systems from theft or damage."
    },
    {
      "type": "Metric",
      "name": "Conceptual Physics",
      "description": "An approach to teaching physics that emphasizes understanding concepts."
    },
    {
      "type": "Metric",
      "name": "Econometrics",
      "description": "The application of statistical methods to economic data."
    },
    {
      "type": "Metric",
      "name": "STEM Scores",
      "description": "Scores representing performance in various STEM subjects."
    },
    {
      "type": "Metric",
      "name": "Social Science Scores",
      "description": "Scores representing performance in various social science subjects."
    },
    {
      "type": "Metric",
      "name": "Humanities Scores",
      "description": "Scores representing performance in various humanities subjects."
    },
    {
      "type": "Dataset",
      "name": "High School Subjects",
      "description": "A collection of subjects including Biology, Chemistry, Computer Science, etc., used for evaluation."
    },
    {
      "type": "Metric",
      "name": "Performance Scores",
      "description": "Scores representing the performance of various subjects in high school education."
    },
    {
      "type": "Concept",
      "name": "Social Science",
      "description": "A category of subjects that includes Government, Politics, Macroeconomics, Microeconomics, Psychology, and Human Sexuality."
    },
    {
      "type": "Concept",
      "name": "Humanities",
      "description": "A category of subjects that includes History and Law."
    },
    {
      "type": "Concept",
      "name": "Machine Learning",
      "description": "A field of study that involves algorithms and statistical models that enable computers to perform tasks without explicit instructions."
    },
    {
      "type": "Domain",
      "name": "Professional Fields",
      "description": "Includes specialized areas such as law, medicine, and psychology."
    },
    {
      "type": "Domain",
      "name": "Others",
      "description": "A category encompassing various fields not specifically listed."
    },
    {
      "type": "Metric",
      "name": "5-shot results",
      "description": "Performance measurement based on 5 examples per task."
    },
    {
      "type": "Domain",
      "name": "All",
      "description": "A category in the MMLU dataset that aggregates results across all domains."
    },
    {
      "type": "Concept",
      "name": "Fibonacci sequence",
      "description": "An infinite sequence discovered by Leonardo of Pisa, known for its rapid growth."
    },
    {
      "type": "Technique",
      "name": "Instruction finetuning",
      "description": "A method to improve model performance by training on specific instructions."
    },
    {
      "type": "Historical Figure",
      "name": "Leonardo of Pisa",
      "description": "An Italian mathematician also known as Fibonacci, who studied the Fibonacci sequence."
    },
    {
      "type": "Publication",
      "name": "Liber abaci",
      "description": "A book written by Fibonacci in 1202 that discusses the Fibonacci sequence."
    },
    {
      "type": "Concept",
      "name": "Foundation Language Models",
      "description": "A category of language models that serve as a base for various NLP tasks."
    },
    {
      "type": "Concept",
      "name": "Deep Learning",
      "description": "A mix of rock, punk, and rap music, reflecting on the field of deep learning."
    },
    {
      "type": "Person",
      "name": "Yann Le Cun",
      "description": "Founder of deep learning and artist behind the album 'Deep Learning'."
    },
    {
      "type": "Model",
      "name": "Deep Learning Album",
      "description": "A solo music album by Yann Le Cun that explores themes related to AI research and academia."
    },
    {
      "type": "Person",
      "name": "Le Cun",
      "description": "A wordsmith and artist who discusses AI in his songs."
    },
    {
      "type": "Concept",
      "name": "AI",
      "description": "Artificial Intelligence, the simulation of human intelligence processes by machines."
    },
    {
      "type": "Technique",
      "name": "Training",
      "description": "The process of teaching a model to learn from data."
    },
    {
      "type": "Concept",
      "name": "Perception",
      "description": "The ability of a model to interpret and understand sensory information."
    },
    {
      "type": "Concept",
      "name": "Network",
      "description": "A system of interconnected elements, often referring to neural networks in AI."
    },
    {
      "type": "Concept",
      "name": "Telegraph",
      "description": "An early form of communication technology considered by Gauss as a precursor to social media."
    },
    {
      "type": "Concept",
      "name": "Social Media",
      "description": "Platforms for social interaction, compared to the telegraph by Gauss."
    },
    {
      "type": "Concept",
      "name": "Geek",
      "description": "A term used by Gauss to describe early adopters of technology."
    },
    {
      "type": "Concept",
      "name": "Character Limit",
      "description": "A constraint in platforms like Twitter, mentioned in the context of communication efficiency."
    },
    {
      "type": "Dataset",
      "name": "Instruction Dataset",
      "description": "A dataset used for fine-tuning LLaMA-65B, referenced from Chung et al. (2022)."
    },
    {
      "type": "Concept",
      "name": "HTTP Request",
      "description": "A method for sending requests to a server, demonstrated in the context of JavaScript."
    },
    {
      "type": "Technique",
      "name": "GET Request",
      "description": "A specific type of HTTP request used to retrieve data from a server."
    },
    {
      "type": "Technique",
      "name": "Fetch API",
      "description": "A modern interface for making HTTP requests in JavaScript."
    },
    {
      "type": "Technique",
      "name": "Regular Expressions",
      "description": "A sequence of characters that define a search pattern, often used for string manipulation."
    },
    {
      "type": "Language",
      "name": "Python",
      "description": "A programming language in which the function extraction code is written."
    },
    {
      "type": "Concept",
      "name": "Sicilian Defense",
      "description": "A popular chess opening that counters white's early attacks and allows black to establish a strong position."
    },
    {
      "type": "Concept",
      "name": "French Defense",
      "description": "A popular chess opening characterized by the moves 1.e4 e6, effective against white's early strategies."
    },
    {
      "type": "Concept",
      "name": "Caro-Kann Defense",
      "description": "A solid chess opening that allows black to counter white's attacks while maintaining a strong center."
    },
    {
      "type": "Concept",
      "name": "Ruy Lopez",
      "description": "A classical chess opening that begins with 1.e4 e5, aiming for control of the center."
    },
    {
      "type": "Concept",
      "name": "Italian Game",
      "description": "A chess opening that starts with 1.e4 e5, focusing on rapid development and control of the center."
    },
    {
      "type": "Concept",
      "name": "Scotch Game",
      "description": "A chess opening that begins with 1.e4 e5, known for its aggressive nature and early central control."
    },
    {
      "type": "Concept",
      "name": "Open Games",
      "description": "A category of chess openings where white plays aggressively to control the center."
    },
    {
      "type": "Concept",
      "name": "Ethical Implications",
      "description": "Considerations regarding the moral impact of using language models."
    },
    {
      "type": "Concept",
      "name": "Legal and Policy Constraints",
      "description": "Regulations that may govern the use of language models."
    },
    {
      "type": "Concept",
      "name": "Theory of Relativity",
      "description": "A foundational theory of modern physics proposed by Albert Einstein."
    },
    {
      "type": "Concept",
      "name": "Law of Photons",
      "description": "A discovery by Einstein related to the behavior of light and energy."
    },
    {
      "type": "Concept",
      "name": "Quantum Mechanics",
      "description": "A fundamental theory in physics that describes the physical properties of nature at the scale of atoms and subatomic particles."
    },
    {
      "type": "Equation",
      "name": "E = mc^2",
      "description": "Einstein's famous equation that states energy is equal to mass times the speed of light squared."
    }
  ],
  "relationships": [
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Mechanism",
      "description": "The Transformer architecture utilizes attention mechanisms for processing data."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Convolutional Neural Networks (CNNs)",
      "description": "The paper compares the performance of pure transformers to that of CNNs in image recognition tasks."
    },
    {
      "type": "INTRODUCES",
      "source": "Transformer",
      "target": "Image Patches",
      "description": "The paper introduces the application of transformers directly to sequences of image patches."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer (ViT)",
      "target": "Convolutional Networks",
      "description": "ViT attains excellent results compared to state-of-the-art convolutional networks."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Self-attention",
      "description": "ViT utilizes self-attention mechanisms for processing image patches."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Self-attention",
      "description": "Transformers are built on the self-attention mechanism."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "Transformers utilize self-attention mechanisms to process input data."
    },
    {
      "type": "COMPARES_TO",
      "source": "CNN",
      "target": "Transformer",
      "description": "CNN architectures are compared to Transformers in terms of performance in image recognition."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Image Recognition",
      "description": "Transformers aim to improve image recognition tasks by leveraging their computational efficiency."
    },
    {
      "type": "INTRODUCES",
      "source": "Transformer",
      "target": "Large-scale image datasets",
      "description": "Transformers are introduced as a viable model for large-scale image datasets."
    },
    {
      "type": "EXTENDS",
      "source": "CNN",
      "target": "Self-Attention",
      "description": "Some works extend CNN architectures by incorporating self-attention mechanisms."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Image Patching",
      "description": "The Transformer model uses image patching to process images as sequences of linear embeddings."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Res Nets",
      "description": "The performance of the Transformer model is compared to Res Nets of comparable size."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Image Classification",
      "description": "The Transformer model aims to improve image classification tasks."
    },
    {
      "type": "INTRODUCES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "Vision Transformer",
      "description": "This paper introduces the Vision Transformer model for image recognition."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "Fine-tuning",
      "description": "The Vision Transformer model utilizes fine-tuning to improve performance on specific tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "Vision Transformer",
      "target": "Transformer",
      "description": "The Vision Transformer model builds on the transformer architecture."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "ImageNet",
      "description": "The paper evaluates the Vision Transformer model on the ImageNet dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Large Scale Training",
      "target": "Vision Transformer (ViT)",
      "description": "Large scale training improves the performance of the Vision Transformer model."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "ImageNet-21k",
      "description": "The Vision Transformer model is trained using the ImageNet-21k dataset."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "JFT-300M",
      "description": "The Vision Transformer model is trained using the JFT-300M dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "state of the art",
      "description": "The Vision Transformer model approaches or beats the state of the art on multiple image recognition benchmarks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "Inductive Bias",
      "description": "The performance of the Vision Transformer model shows that large scale training trumps inductive bias."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "GPT",
      "target": "Transformer",
      "description": "GPT models are built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "self-attention",
      "description": "BERT uses self-attention as part of its architecture."
    },
    {
      "type": "USES",
      "source": "GPT",
      "target": "self-attention",
      "description": "GPT uses self-attention as part of its architecture."
    },
    {
      "type": "EXTENDS",
      "source": "self-attention",
      "target": "local neighborhoods self-attention",
      "description": "Local neighborhoods self-attention extends the concept of self-attention to reduce computational cost."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "VTAB",
      "description": "The paper evaluates the performance of its proposed methods on the VTAB dataset."
    },
    {
      "type": "BUILDS_ON",
      "source": "Sparse Transformers",
      "target": "Self-Attention",
      "description": "Sparse Transformers utilize scalable approximations to enhance the self-attention mechanism."
    },
    {
      "type": "IMPROVES",
      "source": "Local Multi-Head Attention",
      "target": "Self-Attention",
      "description": "Local Multi-Head Attention improves self-attention by focusing on local neighborhoods."
    },
    {
      "type": "USES",
      "source": "Cordonnier et al. (2020)",
      "target": "Attention in Blocks",
      "description": "The model by Cordonnier et al. employs attention in blocks of varying sizes."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT",
      "target": "CNN",
      "description": "ViT demonstrates competitive performance compared to state-of-the-art CNNs."
    },
    {
      "type": "EXTENDS",
      "source": "Cordonnier et al. (2020)",
      "target": "ViT",
      "description": "The work extends the model of Cordonnier et al. by handling medium-resolution images."
    },
    {
      "type": "USES",
      "source": "Bello et al. (2019)",
      "target": "Self-Attention",
      "description": "Bello et al. combine CNNs with self-attention for image classification."
    },
    {
      "type": "USES",
      "source": "Hu et al. (2018)",
      "target": "Self-Attention",
      "description": "Hu et al. use self-attention to process CNN outputs for object detection."
    },
    {
      "type": "USES",
      "source": "Carion et al. (2020)",
      "target": "Self-Attention",
      "description": "Carion et al. utilize self-attention in the context of object detection."
    },
    {
      "type": "USES",
      "source": "Wang et al. (2018)",
      "target": "Self-Attention",
      "description": "Wang et al. apply self-attention techniques in video processing."
    },
    {
      "type": "USES",
      "source": "Sun et al. (2019)",
      "target": "Self-Attention",
      "description": "Sun et al. utilize self-attention for video processing tasks."
    },
    {
      "type": "IMPROVES",
      "source": "image GPT (i GPT)",
      "target": "Image Net",
      "description": "The image GPT model achieves a maximal accuracy of 72% on the Image Net dataset."
    },
    {
      "type": "EXTENDS",
      "source": "image GPT (i GPT)",
      "target": "image recognition",
      "description": "The model contributes to the field of image recognition by applying Transformers to image data."
    },
    {
      "type": "USES",
      "source": "image GPT (i GPT)",
      "target": "standard benchmarks",
      "description": "The model's performance is evaluated against standard benchmarks."
    },
    {
      "type": "BUILDS_ON",
      "source": "CNN transfer learning",
      "target": "Transformer",
      "description": "The Transformer model builds on the concept of CNN transfer learning."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "ResNet",
      "description": "The paper compares the performance of Transformers to ResNet-based models."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "ImageNet-21k",
      "description": "The Transformer model uses the ImageNet-21k dataset for training."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "JFT-300M",
      "description": "The Transformer model uses the JFT-300M dataset for training."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Multi-Head Attention",
      "description": "The Vision Transformer model utilizes multi-head attention to process image patches."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "Vision Transformer (ViT)",
      "description": "The Vision Transformer extends the transformer architecture for image recognition."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "MLP",
      "description": "The transformer architecture builds on the concept of multi-layer perceptrons for feature processing."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Position Embedding",
      "description": "The Vision Transformer uses position embeddings to retain spatial information of image patches."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Class Embedding",
      "description": "The Vision Transformer incorporates class embeddings to classify the input images."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Patch",
      "description": "The Vision Transformer processes image data in the form of patches."
    },
    {
      "type": "BUILDS_ON",
      "source": "Vision Transformer (ViT)",
      "target": "Transformer",
      "description": "The Vision Transformer model builds on the Transformer architecture to process images."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Position Embeddings",
      "description": "The Vision Transformer uses position embeddings to maintain spatial information of image patches."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Classification Token",
      "description": "The Vision Transformer incorporates a classification token to facilitate image classification tasks."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "NLP Transformer",
      "description": "The Transformer architecture extends the concepts used in NLP Transformers to handle image data."
    },
    {
      "type": "INSPIRED_BY",
      "source": "Vision Transformer (ViT)",
      "target": "Attention is All You Need",
      "description": "The design of the Vision Transformer is inspired by the original Transformer architecture introduced in the paper by Vaswani et al."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Token Embeddings",
      "description": "The Transformer model uses token embeddings as input."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "BERT",
      "description": "The Transformer model extends the concepts introduced by BERT in handling token embeddings."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Patch Embeddings",
      "description": "The Transformer builds on the concept of patch embeddings derived from image patches."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Flattening",
      "description": "The Transformer uses the technique of flattening to reshape images into patches."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "The Transformer model architecture builds on concepts introduced by BERT."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Multiheaded Self-Attention (MSA)",
      "description": "The Transformer encoder uses multiheaded self-attention as a core component."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "MLP",
      "description": "The classification head of the model is implemented using a multi-layer perceptron."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Layernorm (LN)",
      "description": "Layer normalization is applied in the Transformer encoder architecture."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Position Embeddings",
      "description": "Position embeddings are used to retain positional information in the input sequence."
    },
    {
      "type": "INTRODUCES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "Image Representation",
      "description": "The paper introduces the concept of image representation as the output of the Transformer encoder."
    },
    {
      "type": "USES",
      "source": "Transformer encoder",
      "target": "Multiheaded Self-Attention (MSA)",
      "description": "The Transformer encoder uses multiheaded self-attention as a key component."
    },
    {
      "type": "USES",
      "source": "Transformer encoder",
      "target": "MLP blocks",
      "description": "The Transformer encoder incorporates MLP blocks in its architecture."
    },
    {
      "type": "APPLIES",
      "source": "Transformer encoder",
      "target": "Layernorm (LN)",
      "description": "Layernorm is applied before every block in the Transformer encoder."
    },
    {
      "type": "APPLIES",
      "source": "Transformer encoder",
      "target": "Residual connections",
      "description": "Residual connections are applied after every block in the Transformer encoder."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Self-Attention",
      "description": "The Vision Transformer uses self-attention layers to process image data globally."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "MLP",
      "description": "The Vision Transformer employs MLP layers for local processing of features."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "CNNs",
      "description": "The Vision Transformer is compared to CNNs in terms of inductive bias and locality."
    },
    {
      "type": "INTRODUCES",
      "source": "Vision Transformer (ViT)",
      "target": "Position Embeddings",
      "description": "The Vision Transformer introduces position embeddings to adjust for different image resolutions."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer (ViT)",
      "target": "Inductive Bias",
      "description": "The Vision Transformer has less image-specific inductive bias compared to CNNs."
    },
    {
      "type": "USES",
      "source": "ViT",
      "target": "Position Embeddings",
      "description": "ViT uses position embeddings to encode spatial information of image patches."
    },
    {
      "type": "IMPROVES",
      "source": "Hybrid Architecture",
      "target": "ViT",
      "description": "The hybrid architecture improves ViT by using CNN feature maps for input sequences."
    },
    {
      "type": "BUILDS_ON",
      "source": "Fine-tuning",
      "target": "Large datasets",
      "description": "Fine-tuning builds on large datasets used for pre-training the model."
    },
    {
      "type": "USES",
      "source": "Hybrid Architecture",
      "target": "CNN",
      "description": "The hybrid architecture uses CNNs to extract feature maps for input to the Transformer."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Fine-tuning",
      "description": "The Vision Transformer model uses fine-tuning to adapt to downstream tasks."
    },
    {
      "type": "IMPROVES",
      "source": "Fine-tuning",
      "target": "Higher Resolution Training",
      "description": "Fine-tuning is often improved by training at higher resolutions."
    },
    {
      "type": "EXTENDS",
      "source": "Position Embeddings",
      "target": "Patch Extraction",
      "description": "Position embeddings are extended to accommodate patch extraction in higher resolution images."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "ResNet",
      "description": "The paper evaluates the representation learning capabilities of ViT in comparison to ResNet."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "ILSVRC-2012 ImageNet",
      "description": "ViT uses the ILSVRC-2012 ImageNet dataset to explore model scalability."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer (ViT)",
      "target": "State of the art",
      "description": "ViT attains state of the art performance on most recognition benchmarks."
    },
    {
      "type": "INTRODUCES",
      "source": "Self-supervision",
      "target": "Vision Transformer (ViT)",
      "description": "The paper introduces the concept of self-supervision in the context of ViT."
    },
    {
      "type": "USES",
      "source": "ViT",
      "target": "ILSVRC-2012 ImageNet",
      "description": "ViT uses the ILSVRC-2012 ImageNet dataset for training and evaluation."
    },
    {
      "type": "USES",
      "source": "ViT",
      "target": "ImageNet-21k",
      "description": "ViT uses the ImageNet-21k dataset for training and evaluation."
    },
    {
      "type": "USES",
      "source": "ViT",
      "target": "JFT",
      "description": "ViT uses the JFT dataset for training and evaluation."
    },
    {
      "type": "EVALUATES",
      "source": "ViT",
      "target": "CIFAR-10",
      "description": "ViT is evaluated on the CIFAR-10 dataset."
    },
    {
      "type": "EVALUATES",
      "source": "ViT",
      "target": "CIFAR-100",
      "description": "ViT is evaluated on the CIFAR-100 dataset."
    },
    {
      "type": "EVALUATES",
      "source": "ViT",
      "target": "Oxford-IIIT Pets",
      "description": "ViT is evaluated on the Oxford-IIIT Pets dataset."
    },
    {
      "type": "EVALUATES",
      "source": "ViT",
      "target": "Oxford Flowers-102",
      "description": "ViT is evaluated on the Oxford Flowers-102 dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Self-supervised learning",
      "target": "ViT",
      "description": "Self-supervised learning techniques improve the performance of ViT."
    },
    {
      "type": "BUILDS_ON",
      "source": "ViT-Base",
      "target": "BERT",
      "description": "ViT-Base configurations are based on those used for BERT."
    },
    {
      "type": "BUILDS_ON",
      "source": "ViT-Large",
      "target": "BERT",
      "description": "ViT-Large configurations are based on those used for BERT."
    },
    {
      "type": "BUILDS_ON",
      "source": "ViT-Huge",
      "target": "BERT",
      "description": "ViT-Huge configurations are based on those used for BERT."
    },
    {
      "type": "EVALUATES",
      "source": "ViT-Base",
      "target": "VTAB",
      "description": "ViT-Base is evaluated on the VTAB classification suite."
    },
    {
      "type": "EVALUATES",
      "source": "ViT-Large",
      "target": "VTAB",
      "description": "ViT-Large is evaluated on the VTAB classification suite."
    },
    {
      "type": "EVALUATES",
      "source": "ViT-Huge",
      "target": "VTAB",
      "description": "ViT-Huge is evaluated on the VTAB classification suite."
    },
    {
      "type": "BUILDS_ON",
      "source": "ViT-L/16",
      "target": "BERT",
      "description": "The ViT-L/16 model is directly adopted from BERT."
    },
    {
      "type": "IMPROVES",
      "source": "ResNet",
      "target": "Transfer",
      "description": "Modifications to ResNet improve transfer performance."
    },
    {
      "type": "USES",
      "source": "ResNet (BiT)",
      "target": "Group Normalization",
      "description": "ResNet (BiT) uses Group Normalization instead of Batch Normalization."
    },
    {
      "type": "USES",
      "source": "ResNet (BiT)",
      "target": "Standardized Convolutions",
      "description": "ResNet (BiT) uses standardized convolutions in its architecture."
    },
    {
      "type": "USES",
      "source": "Res Net (Bi T)",
      "target": "Vi T",
      "description": "The modified ResNet model feeds intermediate feature maps into the Vision Transformer."
    },
    {
      "type": "IMPROVES",
      "source": "Adam",
      "target": "Res Nets",
      "description": "Adam optimization works slightly better than SGD for Res Nets in the given setting."
    },
    {
      "type": "EXTENDS",
      "source": "Res Net 50",
      "target": "Stage 3",
      "description": "Stage 3 is extended by placing the same number of layers as in stage 4."
    },
    {
      "type": "USES",
      "source": "ViT-L/16",
      "target": "SGD",
      "description": "ViT-L/16 uses SGD with momentum for fine-tuning."
    },
    {
      "type": "USES",
      "source": "ViT-H/14",
      "target": "SGD",
      "description": "ViT-H/14 uses SGD with momentum for fine-tuning."
    },
    {
      "type": "IMPROVES",
      "source": "Adam",
      "target": "SGD",
      "description": "Adam works slightly better than SGD for Res Nets in the given setting."
    },
    {
      "type": "COMPARES_TO",
      "source": "Fine-tuning accuracy",
      "target": "Few-shot accuracy",
      "description": "Fine-tuning accuracy captures performance after fine-tuning, while few-shot accuracy is obtained through a regression problem."
    },
    {
      "type": "USES",
      "source": "Fine-tuning accuracy",
      "target": "ImageNet",
      "description": "Fine-tuning accuracy is reported on the ImageNet dataset."
    },
    {
      "type": "USES",
      "source": "Few-shot accuracy",
      "target": "ImageNet",
      "description": "Few-shot accuracy is evaluated on the ImageNet dataset."
    },
    {
      "type": "EXTENDS",
      "source": "Polyak & Juditsky averaging",
      "target": "ViT-L/16",
      "description": "Polyak & Juditsky averaging is used with a factor of 0.9999 for ViT-L/16."
    },
    {
      "type": "EXTENDS",
      "source": "Polyak & Juditsky averaging",
      "target": "ViT-H/14",
      "description": "Polyak & Juditsky averaging is used with a factor of 0.9999 for ViT-H/14."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-H/14",
      "target": "Big Transfer (BiT)",
      "description": "ViT-H/14 is compared to Big Transfer (BiT) in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-L/16",
      "target": "Noisy Student",
      "description": "ViT-L/16 is compared to Noisy Student, which is the state of the art on ImageNet."
    },
    {
      "type": "USES",
      "source": "Noisy Student",
      "target": "ImageNet",
      "description": "Noisy Student is trained using the ImageNet dataset."
    },
    {
      "type": "USES",
      "source": "Noisy Student",
      "target": "JFT-300M",
      "description": "Noisy Student is also trained using the JFT-300M dataset."
    },
    {
      "type": "IMPROVES",
      "source": "ViT-H/14",
      "target": "CNN",
      "description": "ViT-H/14 improves upon traditional CNN architectures for image recognition."
    },
    {
      "type": "IMPROVES",
      "source": "ViT-L/16",
      "target": "CNN",
      "description": "ViT-L/16 improves upon traditional CNN architectures for image recognition."
    },
    {
      "type": "IMPROVES",
      "source": "Vi T-H/14",
      "target": "Vi T-L/16",
      "description": "Vi T-H/14 improves performance over Vi T-L/16 on challenging datasets."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vi T-L/16",
      "target": "Bi T-L",
      "description": "Vi T-L/16 outperforms Bi T-L on all tasks."
    },
    {
      "type": "USES",
      "source": "Noisy Student",
      "target": "ImageNet",
      "description": "Noisy Student is the state of the art on ImageNet."
    },
    {
      "type": "USES",
      "source": "Vi T-L/16",
      "target": "JFT-300 M",
      "description": "Vi T-L/16 is pre-trained on the JFT-300 M dataset."
    },
    {
      "type": "USES",
      "source": "Vi T-H/14",
      "target": "ImageNet",
      "description": "Vi T-H/14 is evaluated on the ImageNet dataset."
    },
    {
      "type": "USES",
      "source": "Vi T-H/14",
      "target": "CIFAR-100",
      "description": "Vi T-H/14 is evaluated on the CIFAR-100 dataset."
    },
    {
      "type": "USES",
      "source": "Vi T-H/14",
      "target": "VTAB suite",
      "description": "Vi T-H/14 is evaluated on the VTAB suite."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer (ViT)",
      "target": "ResNet",
      "description": "Vision Transformer models outperform ResNet-based baselines on image classification benchmarks."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "JFT-300M",
      "description": "Vision Transformer models are pre-trained on the JFT-300M dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "ImageNet",
      "description": "The performance of Vision Transformer models is compared to traditional datasets like ImageNet."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "CIFAR-10",
      "description": "The performance of Vision Transformer models is compared to CIFAR-10 dataset results."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "CIFAR-100",
      "description": "The performance of Vision Transformer models is compared to CIFAR-100 dataset results."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "Oxford-IIIT Pets",
      "description": "The performance of Vision Transformer models is compared to Oxford-IIIT Pets dataset results."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "Oxford Flowers-102",
      "description": "The performance of Vision Transformer models is compared to Oxford Flowers-102 dataset results."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer",
      "target": "ResNet",
      "description": "Vision Transformer models outperform ResNet-based baselines on all datasets."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "JFT-300 M",
      "description": "Vision Transformer models are pre-trained on the JFT-300 M dataset."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "Image Net-21 k",
      "description": "Vision Transformer models are also pre-trained on the smaller Image Net-21 k dataset."
    },
    {
      "type": "EVALUATES",
      "source": "Paper Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "VTAB",
      "description": "The paper evaluates the performance of Vision Transformer models on the VTAB benchmark."
    },
    {
      "type": "IMPROVES",
      "source": "Vi T-H/14",
      "target": "Bi T-R 152 x 4",
      "description": "Vi T-H/14 outperforms Bi T-R 152 x 4 on Natural and Structured tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vi T-L/16",
      "target": "VTAB",
      "description": "Vi T-L/16 model is evaluated against other models on the VTAB benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vi T-H/14",
      "target": "Bi T",
      "description": "Vi T-H/14 is compared to Bi T on various tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vi T-H/14",
      "target": "VIVI",
      "description": "Vi T-H/14 is compared to VIVI on various tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vi T-H/14",
      "target": "S 4 L",
      "description": "Vi T-H/14 is compared to S 4 L on various tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-H/14",
      "target": "BiT-R152x4",
      "description": "ViT-H/14 outperforms BiT-R152x4 on Natural and Structured tasks."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "JFT-300M",
      "description": "The Vision Transformer performs well when pre-trained on the JFT-300M dataset."
    },
    {
      "type": "USES",
      "source": "ViT-Large",
      "target": "ImageNet",
      "description": "ViT-Large models are pre-trained on ImageNet, leading to underperformance compared to ViT-Base."
    },
    {
      "type": "USES",
      "source": "ViT-Large",
      "target": "ImageNet-21k",
      "description": "ViT-Large models are pre-trained on ImageNet-21k, resulting in similar performance to ViT-Base."
    },
    {
      "type": "IMPROVES",
      "source": "Regularization",
      "target": "Performance",
      "description": "Regularization techniques are optimized to boost performance on smaller datasets."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-Large",
      "target": "ViT-Base",
      "description": "ViT-Large models underperform compared to ViT-Base models on smaller datasets."
    },
    {
      "type": "USES",
      "source": "ViT-Large",
      "target": "Image Net",
      "description": "ViT-Large models are evaluated using the Image Net dataset."
    },
    {
      "type": "USES",
      "source": "ViT-Base",
      "target": "Image Net",
      "description": "ViT-Base models are evaluated using the Image Net dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Image Net-21k",
      "target": "performance",
      "description": "Pre-training on Image Net-21k improves model performance."
    },
    {
      "type": "IMPROVES",
      "source": "JFT-300M",
      "target": "performance",
      "description": "Pre-training on JFT-300M shows the full benefit of larger models."
    },
    {
      "type": "IMPROVES",
      "source": "ViT",
      "target": "ResNet",
      "description": "ViT models perform better than ResNets when pre-trained on larger datasets."
    },
    {
      "type": "USES",
      "source": "ViT",
      "target": "ImageNet",
      "description": "ViT is evaluated on the ImageNet dataset."
    },
    {
      "type": "USES",
      "source": "ViT",
      "target": "JFT-300M",
      "description": "ViT is pre-trained on the JFT-300M dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT",
      "target": "ResNet",
      "description": "The performance of ViT is compared to ResNet across different dataset sizes."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vi T",
      "target": "Res Net",
      "description": "Vi T generally outperforms Res Nets with the same computational budget."
    },
    {
      "type": "IMPROVES",
      "source": "Hybrid",
      "target": "Vi T",
      "description": "Hybrid models improve upon pure Transformers for smaller model sizes."
    },
    {
      "type": "COMPARES_TO",
      "source": "Bi T",
      "target": "Vi T",
      "description": "Bi T CNNs outperform Vi T on Image Net, but Vi T overtakes with larger datasets."
    },
    {
      "type": "USES",
      "source": "Models",
      "target": "JFT-300 M",
      "description": "Models are trained on the JFT-300 M dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-B/32",
      "target": "ResNet 50",
      "description": "ViT-B/32 is compared to ResNet 50 in terms of performance on smaller datasets."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-B/32",
      "target": "ResNet 152 x 2",
      "description": "ViT-B/32 is compared to ResNet 152 x 2 regarding performance on larger datasets."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "JFT-300M",
      "description": "The Vision Transformer model is trained on the JFT-300M dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer (ViT)",
      "target": "validation accuracy",
      "description": "The Vision Transformer aims to improve validation accuracy on larger datasets."
    },
    {
      "type": "IMPROVES",
      "source": "ResNet",
      "target": "validation accuracy",
      "description": "ResNet models are noted to perform better on smaller datasets due to convolutional inductive bias."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "ImageNet",
      "description": "The paper evaluates the performance of the ViT model on the ImageNet dataset."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "VTAB",
      "description": "The paper evaluates the low-data results of the ViT model on the VTAB dataset."
    },
    {
      "type": "INTRODUCES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "ViT",
      "description": "The paper introduces the Vision Transformer model for image recognition."
    },
    {
      "type": "IMPROVES",
      "source": "ViT",
      "target": "few-shot results",
      "description": "The ViT model shows promising few-shot results, indicating its effectiveness in low-data scenarios."
    },
    {
      "type": "EXTENDS",
      "source": "ViT",
      "target": "low-data transfer",
      "description": "The ViT model extends the capabilities of image recognition by improving low-data transfer performance."
    },
    {
      "type": "USES",
      "source": "ResNet",
      "target": "JFT-300M",
      "description": "ResNet models are pre-trained on the JFT-300M dataset."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "JFT-300M",
      "description": "Vision Transformers are pre-trained on the JFT-300M dataset."
    },
    {
      "type": "BUILDS_ON",
      "source": "Hybrid Model",
      "target": "ResNet",
      "description": "Hybrid models build on the ResNet architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "Hybrid Model",
      "target": "Vision Transformer (ViT)",
      "description": "Hybrid models build on the Vision Transformer architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "ResNet",
      "description": "ViT dominates ResNets on the performance/compute trade-off."
    },
    {
      "type": "IMPROVES",
      "source": "hybrids",
      "target": "Vision Transformer (ViT)",
      "description": "Hybrids slightly outperform ViT at small computational budgets."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "performance/compute trade-off",
      "description": "ViT uses less compute to attain the same performance compared to ResNets."
    },
    {
      "type": "EXTENDS",
      "source": "Vision Transformer (ViT)",
      "target": "future scaling efforts",
      "description": "The performance of Vision Transformers motivates future scaling efforts."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "Attention Mechanism",
      "description": "The Vision Transformer utilizes an attention mechanism to process image data."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer",
      "target": "Position Embedding",
      "description": "The Vision Transformer improves image representation by adding position embeddings to patch representations."
    },
    {
      "type": "ANALYZES",
      "source": "Vision Transformer",
      "target": "Principal Components",
      "description": "The Vision Transformer analyzes principal components to understand its internal representations."
    },
    {
      "type": "USES",
      "source": "ViT",
      "target": "Self-attention",
      "description": "ViT uses self-attention to integrate information across the entire image."
    },
    {
      "type": "COMPARES_TO",
      "source": "Attention distance",
      "target": "CNN",
      "description": "Attention distance is compared to receptive field size in CNNs."
    },
    {
      "type": "IMPROVES",
      "source": "Position embeddings",
      "target": "ViT",
      "description": "Position embeddings improve the representation of 2D image topology in ViT."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Distance",
      "description": "The Transformer model utilizes attention distance to integrate information globally."
    },
    {
      "type": "COMPARES_TO",
      "source": "Attention Distance",
      "target": "Receptive Field Size",
      "description": "Attention distance is compared to receptive field size in CNNs."
    },
    {
      "type": "BUILDS_ON",
      "source": "Hybrid Models",
      "target": "ResNet",
      "description": "Hybrid models apply ResNet before the Transformer, suggesting a relationship in functionality."
    },
    {
      "type": "IMPROVES",
      "source": "Self-Supervised Pre-Training",
      "target": "Transformer",
      "description": "Self-supervised pre-training enhances the performance of Transformers."
    },
    {
      "type": "USES",
      "source": "ViT-L/16",
      "target": "Cosine Similarity",
      "description": "ViT-L/16 uses cosine similarity to measure the similarity of position embeddings."
    },
    {
      "type": "USES",
      "source": "ViT-L/32",
      "target": "Cosine Similarity",
      "description": "ViT-L/32 uses cosine similarity to measure the similarity of position embeddings."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Image Recognition",
      "description": "The Transformer architecture improves image recognition tasks through self-attention mechanisms."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "RGB Embedding Filters",
      "description": "The Transformer architecture builds on the concept of RGB embedding filters for image processing."
    },
    {
      "type": "EXTENDS",
      "source": "ViT-L/16",
      "target": "Mean Attention Distance",
      "description": "ViT-L/16 extends the concept of mean attention distance across different heads."
    },
    {
      "type": "IMPROVES",
      "source": "ViT-B/16",
      "target": "ImageNet",
      "description": "ViT-B/16 achieves 79.9% accuracy on ImageNet, improving by 2% over training from scratch."
    },
    {
      "type": "USES",
      "source": "ViT-B/16",
      "target": "Masked Patch Prediction",
      "description": "ViT-B/16 employs masked patch prediction for self-supervision."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-B/16",
      "target": "BERT",
      "description": "The masked patch prediction mimics the masked language modeling task used in BERT."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "Self-Supervised Pre-Training",
      "description": "The paper explores the application of Transformers to self-supervised pre-training."
    },
    {
      "type": "LEAVES_TO_FUTURE_WORK",
      "source": "Contrastive Pre-Training",
      "target": "Future Work",
      "description": "The exploration of contrastive pre-training is left for future research."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "self-attention",
      "description": "The Vision Transformer model employs self-attention mechanisms to process image data."
    },
    {
      "type": "BUILDS_ON",
      "source": "Vision Transformer",
      "target": "Transformer",
      "description": "The Vision Transformer is built on the Transformer architecture originally designed for NLP."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer",
      "target": "image classification",
      "description": "The Vision Transformer matches or exceeds the state of the art in image classification tasks."
    },
    {
      "type": "EXTENDS",
      "source": "Vision Transformer",
      "target": "detection",
      "description": "The paper discusses the challenge of applying the Vision Transformer to detection tasks."
    },
    {
      "type": "EXTENDS",
      "source": "Vision Transformer",
      "target": "segmentation",
      "description": "The paper discusses the challenge of applying the Vision Transformer to segmentation tasks."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer",
      "target": "self-supervised pre-training",
      "description": "Initial experiments show that self-supervised pre-training methods improve the performance of the Vision Transformer."
    },
    {
      "type": "IMPROVES",
      "source": "Self-supervised pre-training",
      "target": "ViT",
      "description": "Self-supervised pre-training methods show improvement in performance for ViT."
    },
    {
      "type": "COMPARES_TO",
      "source": "Self-supervised pre-training",
      "target": "Large-scale supervised pre-training",
      "description": "There is a large gap between self-supervised and large-scale supervised pre-training."
    },
    {
      "type": "BUILDS_ON",
      "source": "ViT",
      "target": "Self-supervised pre-training",
      "description": "Further scaling of ViT would likely lead to improved performance."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Attention Mechanism",
      "description": "The Transformer model builds on the attention mechanism to process data."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Mutual Information",
      "description": "The Transformer model utilizes mutual information for learning representations."
    },
    {
      "type": "BUILDS_ON",
      "source": "Attention Augmented Convolutional Networks",
      "target": "Transformer",
      "description": "Attention Augmented Convolutional Networks build on the Transformer architecture by integrating attention mechanisms."
    },
    {
      "type": "USES",
      "source": "End-to-End Object Detection with Transformers",
      "target": "Transformer",
      "description": "End-to-End Object Detection with Transformers uses the Transformer model for object detection tasks."
    },
    {
      "type": "EXTENDS",
      "source": "Generative Pretraining from Pixels",
      "target": "Transformer",
      "description": "Generative Pretraining from Pixels extends the capabilities of the Transformer model to image data."
    },
    {
      "type": "IMPROVES",
      "source": "Adaptive Input Representations",
      "target": "Transformer",
      "description": "Adaptive Input Representations improve the performance of the Transformer model in language tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Few-Shot Learning",
      "target": "Transformer",
      "description": "Few-Shot Learning is compared to the performance of the Transformer model in various tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT builds on the transformer architecture for language tasks."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The transformer model uses self-attention mechanisms to process input data."
    },
    {
      "type": "COMPARES_TO",
      "source": "Self-Attention",
      "target": "Convolutional Layers",
      "description": "The relationship between self-attention and convolutional layers is explored."
    },
    {
      "type": "INTRODUCES",
      "source": "UNITER",
      "target": "Contrastive Learning",
      "description": "UNITER introduces contrastive learning for image-text representation."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT builds on the transformer architecture for language understanding."
    },
    {
      "type": "IMPROVES",
      "source": "Deep Residual Learning",
      "target": "Convolutional Neural Networks",
      "description": "Deep residual learning improves the training of convolutional neural networks."
    },
    {
      "type": "USES",
      "source": "Momentum Contrast",
      "target": "Unsupervised Visual Representation Learning",
      "description": "Momentum contrast is used as a technique in unsupervised visual representation learning."
    },
    {
      "type": "INTRODUCES",
      "source": "Axial Attention",
      "target": "Multi-dimensional Transformers",
      "description": "Axial attention introduces a new way to apply attention in multi-dimensional transformers."
    },
    {
      "type": "COMPARES_TO",
      "source": "Convolutional Neural Networks",
      "target": "Robustness",
      "description": "The robustness of convolutional neural networks is compared to other models."
    },
    {
      "type": "BUILDS_ON",
      "source": "Ccnet",
      "target": "Transformer",
      "description": "Ccnet builds on the Transformer architecture to implement criss-cross attention."
    },
    {
      "type": "USES",
      "source": "Relation Networks",
      "target": "Transformer",
      "description": "Relation Networks utilize the Transformer model for object detection."
    },
    {
      "type": "USES",
      "source": "Local Relation Networks",
      "target": "Transformer",
      "description": "Local Relation Networks leverage the Transformer architecture for image recognition."
    },
    {
      "type": "IMPROVES",
      "source": "Contrastive Predictive Coding",
      "target": "Transformer",
      "description": "Contrastive Predictive Coding improves data efficiency in image recognition using Transformer."
    },
    {
      "type": "BUILDS_ON",
      "source": "Batch Normalization",
      "target": "Transformer",
      "description": "Batch normalization techniques are often used to improve the training of Transformer models."
    },
    {
      "type": "USES",
      "source": "Adam",
      "target": "Transformer",
      "description": "The Adam optimization method is commonly used to train Transformer models."
    },
    {
      "type": "EXTENDS",
      "source": "Big Transfer (BiT)",
      "target": "Transformer",
      "description": "Big Transfer extends the capabilities of the Transformer model for visual representation learning."
    },
    {
      "type": "COMPARES_TO",
      "source": "Deep Convolutional Neural Networks",
      "target": "Transformer",
      "description": "The paper compares the performance of Transformers to traditional deep convolutional neural networks."
    },
    {
      "type": "BUILDS_ON",
      "source": "Gshard",
      "target": "Transformer",
      "description": "Gshard builds on the Transformer architecture to implement conditional computation."
    },
    {
      "type": "USES",
      "source": "Visual BERT",
      "target": "Transformer",
      "description": "Visual BERT uses the Transformer architecture to process visual and textual data."
    },
    {
      "type": "USES",
      "source": "ViLBERT",
      "target": "Transformer",
      "description": "ViLBERT uses the Transformer architecture for integrating vision and language tasks."
    },
    {
      "type": "EXTENDS",
      "source": "Slot Attention",
      "target": "Object-centric Learning",
      "description": "Slot Attention extends the concept of object-centric learning by utilizing attention mechanisms."
    },
    {
      "type": "BUILDS_ON",
      "source": "Image Transformer",
      "target": "Transformer",
      "description": "The Image Transformer is built on the principles of the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "Weakly Supervised Pretraining",
      "target": "Transformer",
      "description": "Weakly supervised pretraining techniques are used to enhance the performance of Transformer models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Cats and Dogs",
      "target": "Image Transformer",
      "description": "The performance of the Image Transformer is compared to traditional models using the Cats and Dogs dataset."
    },
    {
      "type": "INTRODUCES",
      "source": "Weight Standardization",
      "target": "Transformer",
      "description": "Weight standardization is introduced as a technique to improve the training of Transformer models."
    },
    {
      "type": "BUILDS_ON",
      "source": "Weight Standardization",
      "target": "Transformer",
      "description": "Weight standardization techniques are applied to improve the performance of Transformer models."
    },
    {
      "type": "IMPROVES",
      "source": "Unsupervised Learning",
      "target": "Language Models",
      "description": "Unsupervised learning techniques enhance the capabilities of language models."
    },
    {
      "type": "USES",
      "source": "VideoBERT",
      "target": "Self-Attention",
      "description": "VideoBERT utilizes self-attention mechanisms for processing video and language data."
    },
    {
      "type": "COMPARES_TO",
      "source": "Data Effectiveness",
      "target": "Deep Learning Models",
      "description": "The effectiveness of data is compared against the performance of various deep learning models."
    },
    {
      "type": "BUILDS_ON",
      "source": "Self-supervised learning",
      "target": "Transformer",
      "description": "Self-supervised learning techniques can be applied to enhance the capabilities of Transformer models."
    },
    {
      "type": "IMPROVES",
      "source": "EfficientNet",
      "target": "Transformer",
      "description": "EfficientNet provides improvements in model efficiency that can be leveraged in Transformer architectures."
    },
    {
      "type": "USES",
      "source": "Axial-DeepLab",
      "target": "Transformer",
      "description": "Axial-DeepLab utilizes Transformer-like attention mechanisms for segmentation tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Attention is All You Need",
      "target": "Transformer",
      "description": "The paper 'Attention is All You Need' serves as a foundational comparison for the Transformer model."
    },
    {
      "type": "INTRODUCES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model for image recognition."
    },
    {
      "type": "USES",
      "source": "Axial-DeepLab",
      "target": "Axial Attention",
      "description": "Axial-DeepLab uses axial attention for panoptic segmentation."
    },
    {
      "type": "BUILDS_ON",
      "source": "Axial-DeepLab",
      "target": "Panoptic Segmentation Dataset",
      "description": "Axial-DeepLab builds on the Panoptic Segmentation Dataset for training."
    },
    {
      "type": "COMPARES_TO",
      "source": "Visual Transformers",
      "target": "Non-local Neural Networks",
      "description": "Visual Transformers are compared to Non-local Neural Networks in terms of performance."
    },
    {
      "type": "IMPROVES",
      "source": "Noisy Student",
      "target": "ImageNet",
      "description": "The Noisy Student technique improves classification performance on the ImageNet dataset."
    },
    {
      "type": "USES",
      "source": "Self-Supervised Learning",
      "target": "Visual Task Adaptation",
      "description": "Self-Supervised Learning techniques are used to enhance visual task adaptation."
    },
    {
      "type": "EXTENDS",
      "source": "Group Normalization",
      "target": "Transformer",
      "description": "Group Normalization is an extension that can be applied to Transformer models for better performance."
    },
    {
      "type": "USES",
      "source": "ViT-B/{16,32}",
      "target": "JFT-300M",
      "description": "ViT-B/{16,32} is trained using the JFT-300M dataset."
    },
    {
      "type": "USES",
      "source": "ViT-L/32",
      "target": "JFT-300M",
      "description": "ViT-L/32 is trained using the JFT-300M dataset."
    },
    {
      "type": "USES",
      "source": "ViT-L/16",
      "target": "JFT-300M",
      "description": "ViT-L/16 is trained using the JFT-300M dataset."
    },
    {
      "type": "USES",
      "source": "ViT-H/14",
      "target": "JFT-300M",
      "description": "ViT-H/14 is trained using the JFT-300M dataset."
    },
    {
      "type": "USES",
      "source": "R50",
      "target": "JFT-300M",
      "description": "R50 is trained using the JFT-300M dataset."
    },
    {
      "type": "USES",
      "source": "R101",
      "target": "JFT-300M",
      "description": "R101 is trained using the JFT-300M dataset."
    },
    {
      "type": "USES",
      "source": "R152",
      "target": "JFT-300M",
      "description": "R152 is trained using the JFT-300M dataset."
    },
    {
      "type": "USES",
      "source": "ViT-B/{16,32}",
      "target": "ImageNet-21k",
      "description": "ViT-B/{16,32} is trained using the ImageNet-21k dataset."
    },
    {
      "type": "USES",
      "source": "ViT-L/{16,32}",
      "target": "ImageNet-21k",
      "description": "ViT-L/{16,32} is trained using the ImageNet-21k dataset."
    },
    {
      "type": "USES",
      "source": "ViT-\u2217",
      "target": "ImageNet",
      "description": "ViT-* is trained using the ImageNet dataset."
    },
    {
      "type": "IMPROVES",
      "source": "ViT",
      "target": "Image Recognition",
      "description": "Vision Transformers improve image recognition tasks."
    },
    {
      "type": "EXTENDS",
      "source": "Self-Attention",
      "target": "Multihead Self-Attention",
      "description": "Multihead self-attention is an extension of the standard self-attention mechanism."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Multihead Self-Attention",
      "description": "The Vision Transformer model utilizes multihead self-attention as a core component."
    },
    {
      "type": "EVALUATES",
      "source": "Vision Transformer (ViT)",
      "target": "ImageNet",
      "description": "The performance of the Vision Transformer model is evaluated on the ImageNet dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Training Techniques",
      "target": "Gradient Clipping",
      "description": "Gradient clipping is applied to improve training stability."
    },
    {
      "type": "USES",
      "source": "ViT (Vision Transformer)",
      "target": "MSA (Multi-Head Self-Attention)",
      "description": "The ViT model uses multi-head self-attention as a core component for processing images."
    },
    {
      "type": "IMPROVES",
      "source": "Dropout",
      "target": "ViT (Vision Transformer)",
      "description": "Dropout is used to improve the performance of the ViT model by reducing overfitting."
    },
    {
      "type": "EVALUATES",
      "source": "ViT (Vision Transformer)",
      "target": "ImageNet",
      "description": "The performance of the ViT model is evaluated using the ImageNet dataset."
    },
    {
      "type": "INTRODUCES",
      "source": "ViT (Vision Transformer)",
      "target": "Fine-tuning",
      "description": "The paper introduces fine-tuning as a method for optimizing the ViT model on specific datasets."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Learning Rate Sweep",
      "description": "The Transformer model uses a learning rate sweep to optimize training."
    },
    {
      "type": "USES",
      "source": "Res Nets",
      "target": "Learning Rate Sweep",
      "description": "Res Nets utilize a learning rate sweep for fine-tuning."
    },
    {
      "type": "USES",
      "source": "Hybrid Models",
      "target": "Learning Rate Sweep",
      "description": "Hybrid models apply a learning rate sweep during training."
    },
    {
      "type": "COMPARES_TO",
      "source": "Image Net",
      "target": "CIFAR",
      "description": "The performance on Image Net is compared to that on CIFAR."
    },
    {
      "type": "USES",
      "source": "ViT (Vision Transformer)",
      "target": "Cosine Learning Rate Decay",
      "description": "ViT models utilize cosine learning rate decay during fine-tuning."
    },
    {
      "type": "USES",
      "source": "ViT (Vision Transformer)",
      "target": "Gradient Clipping",
      "description": "ViT models apply gradient clipping at a global norm of 1."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT (Vision Transformer)",
      "target": "ResNet",
      "description": "The paper compares the performance of ViT models with ResNet architectures."
    },
    {
      "type": "BUILDS_ON",
      "source": "Kolesnikov et al. (2020)",
      "target": "ViT (Vision Transformer)",
      "description": "The paper builds on the setup proposed by Kolesnikov et al. for fine-tuning."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Linear Layer",
      "description": "The Vision Transformer model uses linear layers for classification tasks."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer (ViT)",
      "target": "VTAB",
      "description": "The Vision Transformer model improves performance on the VTAB dataset."
    },
    {
      "type": "FOLLOWS",
      "source": "VTAB",
      "target": "Kolesnikov et al. (2020)",
      "description": "The protocol for evaluating on the VTAB dataset follows the methodology established by Kolesnikov et al."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "Masked Patch Prediction",
      "description": "The Vision Transformer employs masked patch prediction for self-supervision."
    },
    {
      "type": "USES",
      "source": "Masked Patch Prediction",
      "target": "JFT",
      "description": "The masked patch prediction technique is trained on the JFT dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Vision Transformer",
      "target": "Mean Color",
      "description": "The Vision Transformer predicts the mean color of corrupted patches as part of its training."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "Adam",
      "description": "The Vision Transformer uses the Adam optimizer for training."
    },
    {
      "type": "USES",
      "source": "Adam",
      "target": "cosine learning rate decay",
      "description": "Adam optimization algorithm uses cosine learning rate decay for adjusting the learning rate."
    },
    {
      "type": "COMPARES_TO",
      "source": "L2 regression",
      "target": "few-shot performance",
      "description": "L2 regression was compared to other prediction settings based on few-shot performance."
    },
    {
      "type": "INTRODUCES",
      "source": "masked patch prediction",
      "target": "JFT",
      "description": "The method of masked patch prediction does not require a large dataset like JFT for effective training."
    },
    {
      "type": "IMPROVES",
      "source": "masked patch prediction",
      "target": "few-shot performance",
      "description": "Masked patch prediction improves few-shot performance in image recognition tasks."
    },
    {
      "type": "IMPROVES",
      "source": "masked patch prediction",
      "target": "ViT",
      "description": "Masked patch prediction enhances the performance of the ViT model."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT",
      "target": "Image Net",
      "description": "ViT's performance is compared against the Image Net dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT",
      "target": "JFT-300M",
      "description": "ViT's performance is compared against the JFT-300M dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT",
      "target": "Image Net-21k",
      "description": "ViT's performance is compared against the Image Net-21k dataset."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Transformer",
      "description": "The Vision Transformer model utilizes the transformer architecture for image recognition."
    },
    {
      "type": "EVALUATES",
      "source": "Vision Transformer (ViT)",
      "target": "ImageNet",
      "description": "The performance of the Vision Transformer is evaluated on the ImageNet dataset."
    },
    {
      "type": "EVALUATES",
      "source": "Vision Transformer (ViT)",
      "target": "CIFAR-10",
      "description": "The performance of the Vision Transformer is evaluated on the CIFAR-10 dataset."
    },
    {
      "type": "EVALUATES",
      "source": "Vision Transformer (ViT)",
      "target": "CIFAR-100",
      "description": "The performance of the Vision Transformer is evaluated on the CIFAR-100 dataset."
    },
    {
      "type": "EVALUATES",
      "source": "Vision Transformer (ViT)",
      "target": "Oxford Flowers-102",
      "description": "The performance of the Vision Transformer is evaluated on the Oxford Flowers-102 dataset."
    },
    {
      "type": "EVALUATES",
      "source": "Vision Transformer (ViT)",
      "target": "Oxford-IIIT Pets",
      "description": "The performance of the Vision Transformer is evaluated on the Oxford-IIIT Pets dataset."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "Vision Transformer",
      "description": "The paper evaluates the performance of the Vision Transformer model on various datasets."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer",
      "target": "ResNet",
      "description": "The performance of Vision Transformer is compared to ResNet architectures."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "Image Net",
      "description": "The Vision Transformer model is pre-trained on the Image Net dataset."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "Image Net-21k",
      "description": "The Vision Transformer model is pre-trained on the Image Net-21k dataset."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "JFT 300 M",
      "description": "The Vision Transformer model is pre-trained on the JFT 300 M dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer",
      "target": "Oxford Flowers-102",
      "description": "The performance of the Vision Transformer is evaluated against the Oxford Flowers-102 dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer",
      "target": "Oxford-IIIT-Pets",
      "description": "The performance of the Vision Transformer is evaluated against the Oxford-IIIT-Pets dataset."
    },
    {
      "type": "IMPROVES",
      "source": "ViT-H",
      "target": "Transfer Accuracy",
      "description": "ViT-H achieves high transfer accuracy on various datasets."
    },
    {
      "type": "COMPARES_TO",
      "source": "ResNet-50",
      "target": "ResNet-101",
      "description": "Comparison of performance metrics between ResNet-50 and ResNet-101."
    },
    {
      "type": "COMPARES_TO",
      "source": "ResNet-50",
      "target": "ResNet-152",
      "description": "Comparison of performance metrics between ResNet-50 and ResNet-152."
    },
    {
      "type": "USES",
      "source": "R50",
      "target": "Vision Transformer (ViT)",
      "description": "R50 model utilizes Vision Transformer techniques."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT",
      "target": "ResNet",
      "description": "The paper compares the performance of ViT with ResNet."
    },
    {
      "type": "USES",
      "source": "ResNet",
      "target": "SGD",
      "description": "ResNet is typically trained using Stochastic Gradient Descent."
    },
    {
      "type": "USES",
      "source": "ViT",
      "target": "Adam",
      "description": "ViT uses Adam as an optimizer, which is unconventional for ResNet."
    },
    {
      "type": "IMPROVES",
      "source": "Adam",
      "target": "SGD",
      "description": "Adam pre-training outperforms SGD pre-training on most datasets."
    },
    {
      "type": "COMPARES_TO",
      "source": "ResNet-50",
      "target": "ResNet-152",
      "description": "Performance comparison of ResNet-50 and ResNet-152 models."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "Adam",
      "description": "The Vision Transformer model uses Adam for optimization."
    },
    {
      "type": "USES",
      "source": "Vision Transformer",
      "target": "SGD",
      "description": "The Vision Transformer model also uses SGD for optimization."
    },
    {
      "type": "EVALUATES",
      "source": "Paper",
      "target": "Vision Transformer",
      "description": "The paper evaluates the performance of the Vision Transformer."
    },
    {
      "type": "USES",
      "source": "Adam",
      "target": "Res Nets",
      "description": "Adam is used as the optimizer for pre-training Res Nets."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adam",
      "target": "SGD",
      "description": "Adam pre-training outperforms SGD pre-training on most datasets."
    },
    {
      "type": "BUILDS_ON",
      "source": "ViT",
      "target": "Transformer",
      "description": "ViT is based on the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "scaling the depth",
      "target": "5-shot performance",
      "description": "Scaling the depth of the ViT model results in significant improvements in 5-shot performance."
    },
    {
      "type": "EVALUATES",
      "source": "Transformer",
      "target": "ImageNet",
      "description": "The performance of the Transformer architecture is evaluated on the ImageNet dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Scaling",
      "target": "Performance",
      "description": "Scaling the depth of the network results in significant improvements in performance."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Class Token",
      "description": "The Transformer model uses a class token as an image representation."
    },
    {
      "type": "COMPARES_TO",
      "source": "Depth",
      "target": "Width",
      "description": "Scaling depth results in greater performance improvements compared to scaling width."
    },
    {
      "type": "INTRODUCES",
      "source": "Patch Size",
      "target": "Effective Sequence Length",
      "description": "Decreasing the patch size increases the effective sequence length, leading to robust improvements."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Class Token",
      "description": "The Transformer model uses a class token to represent images for classification."
    },
    {
      "type": "USES",
      "source": "Class Token",
      "target": "Multi-layer Perceptron (MLP)",
      "description": "The output of the class token is transformed into class predictions using an MLP."
    },
    {
      "type": "COMPARES_TO",
      "source": "GAP",
      "target": "ResNet",
      "description": "The performance of using GAP with image-patch embeddings is compared to ResNet's final feature map."
    },
    {
      "type": "USES",
      "source": "ViT-B/16",
      "target": "Positional Embedding",
      "description": "The ViT-B/16 model uses positional embeddings to encode spatial information."
    },
    {
      "type": "COMPARES_TO",
      "source": "Class-Token",
      "target": "Global Average Pooling",
      "description": "The paper compares the performance of class-token and global average pooling classifiers."
    },
    {
      "type": "IMPROVES",
      "source": "Positional Embedding",
      "target": "ViT-B/16",
      "description": "Different types of positional embeddings improve the performance of the ViT-B/16 model."
    },
    {
      "type": "INTRODUCES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "Transformer",
      "description": "The paper introduces the Transformer model for image recognition."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Positional Embedding",
      "description": "The Transformer model uses positional embeddings to provide spatial information."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "1-dimensional Positional Embedding",
      "description": "The model considers inputs as a sequence of patches using 1-dimensional positional embedding."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "2-dimensional Positional Embedding",
      "description": "The model uses 2-dimensional positional embedding to learn embeddings for X and Y axes."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Relative Positional Embeddings",
      "description": "The model employs relative positional embeddings to encode spatial information."
    },
    {
      "type": "USES",
      "source": "Relative Attention",
      "target": "Attention Mechanism",
      "description": "Relative Attention is used within the Attention Mechanism to enhance spatial information processing."
    },
    {
      "type": "IMPROVES",
      "source": "Positional Embeddings",
      "target": "Attention Mechanism",
      "description": "Positional Embeddings improve the performance of the Attention Mechanism by providing spatial context."
    },
    {
      "type": "INTRODUCES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "ViT (Vision Transformer)",
      "description": "The paper introduces the Vision Transformer model for image recognition."
    },
    {
      "type": "USES",
      "source": "ViT (Vision Transformer)",
      "target": "Positional Embeddings",
      "description": "The Vision Transformer uses positional embeddings to encode the position of image patches."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "Cosine Similarity",
      "description": "The paper evaluates the performance of the model using cosine similarity as a metric."
    },
    {
      "type": "IMPROVES",
      "source": "ViT (Vision Transformer)",
      "target": "Transformer Encoder",
      "description": "The Vision Transformer improves upon the traditional transformer encoder by adapting it for image data."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-B/16",
      "target": "Model with no positional embedding",
      "description": "The performance of the ViT-B/16 model is compared to that of models without positional embeddings."
    },
    {
      "type": "IMPROVES",
      "source": "Positional Embedding",
      "target": "ViT-B/16",
      "description": "Using positional embeddings improves the performance of the ViT-B/16 model."
    },
    {
      "type": "USES",
      "source": "ViT-B/16",
      "target": "Patch-Level Inputs",
      "description": "The ViT-B/16 model operates on patch-level inputs."
    },
    {
      "type": "USES",
      "source": "ViT-L/16",
      "target": "Attention mechanism",
      "description": "The ViT-L/16 model utilizes the attention mechanism to process image data."
    },
    {
      "type": "EXTENDS",
      "source": "Vision Transformer",
      "target": "Positional encoding",
      "description": "The Vision Transformer architecture extends the concept of positional encoding to adapt it for image data."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-L/16",
      "target": "R50",
      "description": "The performance of the ViT-L/16 model is compared to that of the R50 model in terms of attention distance."
    },
    {
      "type": "INTRODUCES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "Vision Transformer (ViT)",
      "description": "This paper introduces the Vision Transformer model for image recognition."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "FLOPs",
      "description": "The Vision Transformer model uses FLOPs as a metric to evaluate its computational performance."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "Vision Transformer (ViT)",
      "description": "The Vision Transformer extends the capabilities of the original Transformer model to the domain of image recognition."
    },
    {
      "type": "IMPROVES",
      "source": "Empirical Computational Costs",
      "target": "FLOPs",
      "description": "Empirical computational costs improve the understanding of real-world performance beyond theoretical FLOPs."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT",
      "target": "ResNet",
      "description": "ViT models have a clear advantage in terms of memory-efficiency over ResNet models."
    },
    {
      "type": "USES",
      "source": "ViT",
      "target": "TPUv3",
      "description": "ViT models are evaluated on TPUv3 accelerators for inference speed."
    },
    {
      "type": "IMPROVES",
      "source": "ViT",
      "target": "inference speed",
      "description": "ViT models show improved inference speed as input size increases."
    },
    {
      "type": "IMPROVES",
      "source": "ViT",
      "target": "batch size",
      "description": "Larger ViT models can fit larger batch sizes onto a core, improving scalability."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT models",
      "target": "ResNet",
      "description": "ViT models have speed comparable to similar ResNets."
    },
    {
      "type": "USES",
      "source": "Axial Attention",
      "target": "self-attention",
      "description": "Axial Attention performs multiple attention operations along a single axis of the input tensor."
    },
    {
      "type": "BUILDS_ON",
      "source": "Axial Res Net",
      "target": "Res Net 50",
      "description": "Axial Res Net is built on the architecture of Res Net 50 by replacing convolutions with axial self-attention."
    },
    {
      "type": "EXTENDS",
      "source": "ViT",
      "target": "Axial Transformer blocks",
      "description": "ViT is extended by incorporating Axial Transformer blocks for processing inputs."
    },
    {
      "type": "USES",
      "source": "Axial Res Net",
      "target": "Axial Attention",
      "description": "Axial Res Net uses axial attention to mix information along specific axes."
    },
    {
      "type": "IMPROVES",
      "source": "Axial-Vi T-B/32",
      "target": "Vi T-B",
      "description": "Axial-Vi T-B/32 performs better than its Vi T-B counterpart in terms of performance."
    },
    {
      "type": "IMPROVES",
      "source": "Axial-Vi T-B/16",
      "target": "Vi T-B",
      "description": "Axial-Vi T-B/16 performs better than its Vi T-B counterpart in terms of performance."
    },
    {
      "type": "USES",
      "source": "Axial Res Net",
      "target": "column-self-attention",
      "description": "Axial Res Net utilizes column-self-attention as part of its architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "Axial-Vi T-B/32",
      "target": "MLP",
      "description": "Axial-Vi T-B/32 builds on the MLP architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "Axial-Vi T-B/16",
      "target": "MLP",
      "description": "Axial-Vi T-B/16 builds on the MLP architecture."
    },
    {
      "type": "IMPROVES",
      "source": "Axial ViT",
      "target": "ViT",
      "description": "Axial ViT improves upon the standard ViT by using axial attention mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "Axial ViT",
      "target": "ResNet",
      "description": "The performance of Axial ViT is compared to ResNet models in terms of accuracy and speed."
    },
    {
      "type": "USES",
      "source": "Axial ViT",
      "target": "ImageNet",
      "description": "Axial ViT is evaluated on the ImageNet dataset for image classification tasks."
    },
    {
      "type": "EVALUATES",
      "source": "Paper Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "Axial ViT",
      "description": "The paper evaluates the performance of Axial ViT models."
    },
    {
      "type": "USES",
      "source": "ViT (Vision Transformer)",
      "target": "Self-Attention",
      "description": "ViT employs self-attention to process and integrate information from images."
    },
    {
      "type": "IMPROVES",
      "source": "Axial Transformer",
      "target": "Self-Attention",
      "description": "Axial Transformer improves the efficiency of self-attention by operating on smaller sequence lengths."
    },
    {
      "type": "COMPARES_TO",
      "source": "Attention Distance",
      "target": "Receptive Field Size",
      "description": "Attention distance is compared to receptive field size in CNNs to understand its variability across layers."
    },
    {
      "type": "BUILDS_ON",
      "source": "Axial Res Net",
      "target": "Axial Transformer",
      "description": "Axial Res Net builds on the Axial Transformer architecture for improved performance."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "ViT-H/14",
      "description": "The paper evaluates the ViT-H/14 model on the Object Net benchmark."
    },
    {
      "type": "USES",
      "source": "ViT-L/16",
      "target": "Attention Rollout",
      "description": "The ViT-L/16 model uses Attention Rollout to compute attention maps."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-H/14",
      "target": "Object Net",
      "description": "The performance of the ViT-H/14 model is compared against the Object Net benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-H/14",
      "target": "top-5 accuracy",
      "description": "The paper reports a top-5 accuracy of 82.1% for the ViT-H/14 model."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-H/14",
      "target": "top-1 accuracy",
      "description": "The paper reports a top-1 accuracy of 61.7% for the ViT-H/14 model."
    },
    {
      "type": "INTRODUCES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "Vision Transformer (ViT)",
      "description": "The paper introduces the Vision Transformer model for image recognition tasks."
    },
    {
      "type": "USES",
      "source": "Vision Transformer (ViT)",
      "target": "Transformer",
      "description": "The Vision Transformer model utilizes the Transformer architecture for processing images."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "ImageNet",
      "description": "The paper evaluates the performance of the Vision Transformer on the ImageNet dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Vision Transformer (ViT)",
      "target": "CNN",
      "description": "The performance of the Vision Transformer is compared to traditional Convolutional Neural Networks (CNNs) in the paper."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "ViT-H/14",
      "description": "The paper evaluates the performance of the ViT-H/14 model on various tasks."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "ViT-L/16",
      "description": "The paper evaluates the performance of the ViT-L/16 model on various tasks."
    },
    {
      "type": "EVALUATES",
      "source": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "target": "ViT-L/16 (I21k)",
      "description": "The paper evaluates the performance of the ViT-L/16 (I21k) model on various tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-H/14",
      "target": "ViT-L/16",
      "description": "The performance of ViT-H/14 is compared to ViT-L/16 across tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "ViT-L/16",
      "target": "ViT-L/16 (I21k)",
      "description": "The performance of ViT-L/16 is compared to ViT-L/16 (I21k) across tasks."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Denoising Score Matching",
      "description": "The model introduces a novel connection to denoising score matching."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Langevin Dynamics",
      "description": "The model uses Langevin dynamics in its training process."
    },
    {
      "type": "EVALUATES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "The model is evaluated on the CIFAR 10 dataset."
    },
    {
      "type": "EVALUATES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "LSUN",
      "description": "The model is evaluated on the LSUN dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Progressive GAN",
      "description": "The sample quality of the model is compared to that of Progressive GAN."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Progressive GAN",
      "description": "The sample quality of Denoising Diffusion Probabilistic Models is similar to that of Progressive GAN."
    },
    {
      "type": "IMPROVES",
      "source": "Energy-based Modeling",
      "target": "Generative Adversarial Networks (GANs)",
      "description": "Recent advances in energy-based modeling have produced images comparable to those generated by GANs."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Diffusion Probabilistic Models",
      "description": "This paper presents progress in diffusion probabilistic models."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Diffusion Probabilistic Models",
      "description": "This paper presents progress in diffusion probabilistic models."
    },
    {
      "type": "USES",
      "source": "Diffusion Model",
      "target": "Variational Inference",
      "description": "The diffusion model is trained using variational inference."
    },
    {
      "type": "BUILDS_ON",
      "source": "Diffusion Model",
      "target": "Markov Chain",
      "description": "The diffusion model is a parameterized Markov chain."
    },
    {
      "type": "USES",
      "source": "Diffusion Model",
      "target": "Gaussian Noise",
      "description": "The diffusion process adds Gaussian noise to the data."
    },
    {
      "type": "IMPROVES",
      "source": "Diffusion Model",
      "target": "Neural Network",
      "description": "The transitions of the sampling chain can be parameterized using a neural network."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Score Matching",
      "target": "Diffusion Models",
      "description": "Denoising score matching reveals an equivalence with diffusion models during training."
    },
    {
      "type": "INTRODUCES",
      "source": "Annealed Langevin Dynamics",
      "target": "Diffusion Models",
      "description": "Annealed Langevin dynamics is shown to be equivalent to a certain parameterization of diffusion models during sampling."
    },
    {
      "type": "COMPARES_TO",
      "source": "Diffusion Models",
      "target": "Likelihood-based Models",
      "description": "Diffusion models have log likelihoods that are not competitive compared to other likelihood-based models."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Log Likelihood",
      "description": "The models achieve better log likelihoods compared to estimates from energy-based models."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Progressive Decoding",
      "description": "The sampling procedure resembles progressive decoding."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Autoregressive Models",
      "description": "Diffusion models generalize the capabilities of autoregressive models."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Latent Variable Models",
      "description": "The models are a specific type of latent variable models."
    },
    {
      "type": "DEFINES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Markov Chain",
      "description": "The joint distribution is defined as a Markov chain."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Markov Chain",
      "description": "The reverse process is defined as a Markov chain with learned Gaussian transitions."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Gaussian Distribution",
      "description": "The model utilizes Gaussian distributions for the transitions in the Markov chain."
    },
    {
      "type": "EXTENDS",
      "source": "Forward Process",
      "target": "Reverse Process",
      "description": "The forward process is fixed to a Markov chain that adds noise, while the reverse process reconstructs the data."
    },
    {
      "type": "IMPROVES",
      "source": "Variational Inference",
      "target": "Negative Log Likelihood",
      "description": "Training is performed by optimizing the variational bound on negative log likelihood."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Reparameterization",
      "description": "The models can learn forward process variances through reparameterization."
    },
    {
      "type": "ENSURES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Gaussian Conditionals",
      "description": "The choice of Gaussian conditionals in the reverse process ensures expressiveness."
    },
    {
      "type": "EXPRESSES",
      "source": "Gaussian Conditionals",
      "target": "L",
      "description": "The loss function L is expressed in terms of the Gaussian conditionals."
    },
    {
      "type": "SAMPLES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "q(xt|x0)",
      "description": "The models utilize the distribution q(xt|x0) to sample data at arbitrary timesteps."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Stochastic Gradient Descent",
      "description": "The models optimize their training process using stochastic gradient descent."
    },
    {
      "type": "COMPARES_TO",
      "source": "KL Divergence",
      "target": "Gaussian Distribution",
      "description": "KL divergence is used to compare the Gaussian distributions in the context of the diffusion process."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Denoising Autoencoders",
      "description": "The models improve upon traditional denoising autoencoders by incorporating diffusion processes."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Diffusion Models",
      "description": "The paper introduces the concept of diffusion models in the context of denoising."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Denoising Score Matching",
      "description": "The model utilizes denoising score matching to guide the design choices."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Score Matching",
      "target": "Weighted Variational Bound Objective",
      "description": "Denoising score matching builds on the concept to derive a simplified objective."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Model Architecture",
      "description": "The model improves upon existing architectures by allowing flexible implementation."
    },
    {
      "type": "EXTENDS",
      "source": "Forward Process",
      "target": "Reverse Process",
      "description": "The forward process variances \u03b2t are learnable and extend to the reverse process."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Forward process",
      "description": "The model utilizes the forward process to define variances."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Reverse process",
      "description": "The model employs the reverse process to generate samples."
    },
    {
      "type": "COMPARES_TO",
      "source": "Forward process",
      "target": "Reverse process",
      "description": "The forward process is compared to the reverse process in terms of entropy."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Reparameterization",
      "description": "The model introduces reparameterization to allow learnable variances."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "\u00b5\u03b8(xt, t)",
      "description": "The model introduces a specific parameterization for predicting the mean of the denoised data."
    },
    {
      "type": "USES",
      "source": "\u00b5\u03b8(xt, t)",
      "target": "Forward Process Posterior",
      "description": "The model uses the forward process posterior to compute the mean."
    },
    {
      "type": "EVALUATES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Lt",
      "description": "The performance of the model is evaluated using the loss function Lt."
    },
    {
      "type": "EXTENDS",
      "source": "Lt",
      "target": "\u03c32t",
      "description": "The loss function is dependent on the variance parameter \u03c32t."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "\u03b1t",
      "description": "The model extends the concept of \u03b1t to control noise in the diffusion process."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "\u03b2t",
      "description": "The model extends the concept of \u03b2t related to the noise schedule."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "N(0, I)",
      "description": "The model uses a standard normal distribution for sampling."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "\u03f5\u03b8",
      "description": "The model improves the prediction of noise through the function approximator \u03f5\u03b8."
    },
    {
      "type": "BUILDS_ON",
      "source": "Sampling Algorithm",
      "target": "Training Algorithm",
      "description": "The sampling algorithm builds on the training algorithm to generate samples."
    },
    {
      "type": "USES",
      "source": "Sampling Algorithm",
      "target": "xt",
      "description": "The sampling algorithm computes xt\u22121 based on the current state xt."
    },
    {
      "type": "USES",
      "source": "Sampling Algorithm",
      "target": "\u03c3t",
      "description": "The sampling algorithm incorporates \u03c3t to add noise during sampling."
    },
    {
      "type": "USES",
      "source": "Denoising Score Matching",
      "target": "Variational Inference",
      "description": "Optimizing an objective resembling denoising score matching is equivalent to using variational inference."
    },
    {
      "type": "EXTENDS",
      "source": "Reverse Process Mean Function Approximator",
      "target": "\u03f5-Prediction Parameterization",
      "description": "The reverse process mean function approximator can be trained to predict \u03f5."
    },
    {
      "type": "COMPARES_TO",
      "source": "\u03f5-Prediction Parameterization",
      "target": "Langevin Dynamics",
      "description": "The \u03f5-prediction parameterization resembles Langevin dynamics."
    },
    {
      "type": "IMPROVES",
      "source": "\u03f5-Prediction Parameterization",
      "target": "Sample Quality",
      "description": "Predicting x 0 using this parameterization leads to worse sample quality early in experiments."
    },
    {
      "type": "IMPROVES",
      "source": "\u03f5-prediction parameterization",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "The \u03f5-prediction parameterization improves the sample quality of the diffusion model."
    },
    {
      "type": "USES",
      "source": "Neural network reverse process",
      "target": "Image data",
      "description": "The neural network reverse process operates on image data scaled to a specific range."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising score matching",
      "target": "Langevin dynamics",
      "description": "Denoising score matching builds on concepts from Langevin dynamics."
    },
    {
      "type": "USES",
      "source": "Discrete log likelihoods",
      "target": "Gaussian N(x 0; \u00b5\u03b8(x 1,1),\u03c32 1 I)",
      "description": "Discrete log likelihoods are derived from the Gaussian distribution used in the reverse process."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Gaussian Distribution",
      "description": "The model employs Gaussian distributions for its probabilistic framework."
    },
    {
      "type": "IMPROVES",
      "source": "Variational Bound",
      "target": "Codelength",
      "description": "The variational bound ensures a lossless codelength for discrete data."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Conditional Autoregressive Model",
      "description": "The model suggests the potential to incorporate more powerful decoders like conditional autoregressive models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "VAE Decoders",
      "description": "The model's approach is similar to discretized continuous distributions used in VAE decoders."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Reverse Process",
      "description": "The paper introduces the reverse process as part of the denoising diffusion framework."
    },
    {
      "type": "USES",
      "source": "Variational Bound",
      "target": "Log Likelihood",
      "description": "The variational bound is derived from terms related to the log likelihood."
    },
    {
      "type": "BUILDS_ON",
      "source": "Reverse Process",
      "target": "Decoder",
      "description": "The reverse process is built upon the decoder to generate samples."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "EBM",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with Energy-Based Models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "JEM",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with Joint Energy Models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Big GAN",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with Big GAN."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Style GAN 2 + ADA",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with Style GAN 2 + ADA."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Gated Pixel CNN",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with Gated Pixel CNN."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Sparse Transformer",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with Sparse Transformer."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Pixel IQN",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with Pixel IQN."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "NCSNv 2",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with NCSNv 2."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "NCSN",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with NCSN."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "SNGAN",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with SNGAN."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "SNGAN-DDLS",
      "description": "The paper compares the performance of Denoising Diffusion Probabilistic Models with SNGAN-DDLS."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Lsimple",
      "description": "The paper introduces the Lsimple loss function as a variant for training."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "NCSN",
      "description": "The model uses concepts from the NCSN denoising score matching model."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "FID",
      "description": "The paper evaluates the performance of the model using the FID metric."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "FID",
      "description": "The model aims to improve the FID score through better sampling quality."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Sample Quality",
      "description": "The reweighting in the simplified objective leads to better sample quality."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Weighted Variational Bound",
      "description": "The model utilizes a weighted variational bound to focus on more difficult denoising tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Weighted Variational Bound",
      "target": "Standard Variational Bound",
      "description": "The simplified objective compares to the standard variational bound in terms of reconstruction emphasis."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "NCSN",
      "description": "The model builds on the loss weighting used by the NCSN model."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "U-Net",
      "description": "The model employs a U-Net backbone for representing the reverse process."
    },
    {
      "type": "BUILDS_ON",
      "source": "U-Net",
      "target": "Pixel CNN++",
      "description": "The U-Net architecture is similar to an unmasked Pixel CNN++."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Transformer",
      "description": "The model specifies parameters across time using Transformer sinusoidal position embedding."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "sample quality",
      "description": "The model aims to improve sample quality through its design and processes."
    },
    {
      "type": "EVALUATES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "DKL",
      "description": "The experiments evaluate the Kullback-Leibler divergence to assess the quality of the generated samples."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Transformer",
      "description": "The model utilizes the Transformer architecture for its operations."
    },
    {
      "type": "EVALUATES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "The model's performance is evaluated using the CIFAR 10 dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "class conditional models",
      "description": "The model's sample quality is compared to that of class conditional models."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "FID score",
      "description": "The model achieves a better FID score compared to many existing models."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "Training on the true variational bound yields better codelengths for CIFAR 10."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Celeb A-HQ",
      "description": "Training on the true variational bound yields better codelengths for Celeb A-HQ."
    },
    {
      "type": "USES",
      "source": "Sending",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "The Sending algorithm is used to transmit data in the diffusion process."
    },
    {
      "type": "USES",
      "source": "Receiving",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "The Receiving algorithm is used to retrieve data in the diffusion process."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "LSUN",
      "description": "Sample quality is compared across different datasets including LSUN."
    },
    {
      "type": "IMPROVES",
      "source": "Predicting \u03f5",
      "target": "Sample Quality",
      "description": "Predicting \u03f5 performs better than predicting \u02dc\u00b5 when trained with a simplified objective."
    },
    {
      "type": "USES",
      "source": "Learning Reverse Process Variances",
      "target": "Variational Bound",
      "description": "Incorporates a parameterized diagonal \u03a3\u03b8(xt) into the variational bound."
    },
    {
      "type": "COMPARES_TO",
      "source": "Predicting \u02dc\u00b5",
      "target": "Predicting \u03f5",
      "description": "Predicting \u03f5 performs approximately as well as predicting \u02dc\u00b5 under certain training conditions."
    },
    {
      "type": "COMPARES_TO",
      "source": "CIFAR 10 models",
      "target": "Other Likelihood-based Models",
      "description": "The codelength gaps of CIFAR 10 models are comparable to those reported with other likelihood-based models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Model",
      "target": "Energy Based Models",
      "description": "The diffusion model's codelengths are better than the estimates reported for energy based models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Model",
      "target": "Likelihood-based Generative Models",
      "description": "The diffusion model's performance is not competitive with other types of likelihood-based generative models."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Model",
      "target": "Annealed Importance Sampling",
      "description": "The model utilizes annealed importance sampling for score matching."
    },
    {
      "type": "USES",
      "source": "Algorithm 3",
      "target": "minimal random coding",
      "description": "Algorithm 3 assumes access to minimal random coding to transmit samples."
    },
    {
      "type": "USES",
      "source": "Algorithm 4",
      "target": "minimal random coding",
      "description": "Algorithm 4 also assumes access to minimal random coding for sample transmission."
    },
    {
      "type": "COMPARES_TO",
      "source": "progressive lossy compression",
      "target": "rate-distortion behavior",
      "description": "Progressive lossy compression probes the rate-distortion behavior of the model."
    },
    {
      "type": "EVALUATES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "The model's performance is evaluated on the CIFAR 10 test set."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Root Mean Squared Error (RMSE)",
      "description": "The model calculates distortion using RMSE."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Rate-Distortion",
      "description": "The model analyzes the rate-distortion plot to assess performance."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "The model evaluates its performance using the CIFAR 10 dataset."
    },
    {
      "type": "MEASURES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "RMSE",
      "description": "The model's performance is measured using the RMSE metric."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Progressive Generation",
      "description": "The paper introduces the technique of progressive generation as part of the image generation process."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Variational Bound",
      "description": "The paper introduces the concept of variational bounds in the context of denoising diffusion models."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "The model uses the CIFAR 10 dataset for unconditional progressive generation."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Celeb A-HQ",
      "description": "The model uses the Celeb A-HQ dataset to generate high-quality images with shared attributes."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Diffusion Process",
      "description": "The paper introduces the concept of using diffusion processes for generative modeling."
    },
    {
      "type": "USES",
      "source": "Forward Process",
      "target": "p\u03b8(xt\u22121|xt)",
      "description": "The forward process is used to define the model that predicts the previous state in the diffusion process."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Blank Image",
      "description": "The model builds on the concept of a blank image as the final state in the diffusion process."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Gaussian diffusion model",
      "description": "The paper introduces the Gaussian diffusion model as a method for generating data."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Celeb A-HQ",
      "description": "The model uses the Celeb A-HQ dataset for training and evaluating image generation."
    },
    {
      "type": "IMPROVES",
      "source": "Gaussian diffusion model",
      "target": "Sample quality",
      "description": "The Gaussian diffusion model is speculated to improve sample quality compared to other noise addition methods."
    },
    {
      "type": "EXTENDS",
      "source": "Autoregressive model",
      "target": "Gaussian diffusion model",
      "description": "The Gaussian diffusion model extends the concept of autoregressive models by incorporating a generalized bit ordering."
    },
    {
      "type": "USES",
      "source": "Gaussian diffusion",
      "target": "Stochastic encoder",
      "description": "Gaussian diffusion is used in conjunction with a stochastic encoder to process images."
    },
    {
      "type": "USES",
      "source": "Interpolation",
      "target": "Reverse process",
      "description": "Interpolation utilizes the reverse process to decode linearly interpolated latent representations."
    },
    {
      "type": "COMPARES_TO",
      "source": "Gaussian diffusion",
      "target": "Masking noise",
      "description": "Gaussian diffusion is compared to masking noise in terms of its effectiveness in image processing."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Celeb A-HQ",
      "description": "The model utilizes the Celeb A-HQ dataset for training and evaluation."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "\u03f5-prediction reverse process",
      "description": "The model improves upon traditional methods by employing the \u03f5-prediction reverse process for better image reconstructions."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Diffusion Models",
      "description": "The paper compares diffusion models to other generative models like flows and VAEs."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Denoising Score Matching",
      "description": "The model establishes a connection between diffusion models and denoising score matching."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Variational Inference",
      "description": "The training procedure explicitly trains the Langevin dynamics sampler using variational inference."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Score Matching",
      "target": "Variational Inference",
      "description": "A certain weighted form of denoising score matching is compared to variational inference for training."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Score Matching",
      "target": "Langevin Dynamics",
      "description": "The connection implies an extension of denoising score matching to train a Langevin-like sampler."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Infusion Training",
      "description": "The paper discusses infusion training as a method for learning transition operators."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Variational Walkback",
      "description": "The paper introduces variational walkback as another method for learning transition operators."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Generative Stochastic Networks",
      "description": "Generative stochastic networks are mentioned as part of the methods for learning transition operators."
    },
    {
      "type": "BUILDS_ON",
      "source": "Score Matching",
      "target": "Energy-Based Modeling",
      "description": "Score matching is a technique that builds on the principles of energy-based modeling."
    },
    {
      "type": "USES",
      "source": "Rate-Distortion Curves",
      "target": "Variational Bound",
      "description": "Rate-distortion curves are computed in the evaluation of the variational bound."
    },
    {
      "type": "COMPARES_TO",
      "source": "Rate-Distortion Curves",
      "target": "Distortion Penalties",
      "description": "Rate-distortion curves can be compared to distortion penalties in annealed importance sampling."
    },
    {
      "type": "EXTENDS",
      "source": "Progressive Decoding",
      "target": "Convolutional DRAW",
      "description": "Progressive decoding arguments extend the concepts used in convolutional DRAW."
    },
    {
      "type": "IMPROVES",
      "source": "Progressive Decoding",
      "target": "Autoregressive Models",
      "description": "Progressive decoding may lead to improved sampling strategies for autoregressive models."
    },
    {
      "type": "IMPROVES",
      "source": "Diffusion Models",
      "target": "Generative Models",
      "description": "Diffusion models represent progress in improving the sample quality of generative models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Diffusion Models",
      "target": "GANs",
      "description": "Diffusion models are compared to GANs in terms of sample quality."
    },
    {
      "type": "COMPARES_TO",
      "source": "Diffusion Models",
      "target": "Autoregressive Models",
      "description": "Diffusion models are compared to autoregressive models."
    },
    {
      "type": "EXTENDS",
      "source": "Diffusion Models",
      "target": "Variational Inference",
      "description": "Diffusion models extend concepts from variational inference."
    },
    {
      "type": "EXTENDS",
      "source": "Diffusion Models",
      "target": "Denoising Score Matching",
      "description": "Diffusion models extend the use of denoising score matching."
    },
    {
      "type": "EXTENDS",
      "source": "Diffusion Models",
      "target": "Annealed Langevin Dynamics",
      "description": "Diffusion models extend the principles of annealed Langevin dynamics."
    },
    {
      "type": "EXTENDS",
      "source": "Diffusion Models",
      "target": "Progressive Lossy Compression",
      "description": "Diffusion models extend the techniques used in progressive lossy compression."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Generative Models",
      "description": "The paper presents advancements that enhance the utility of generative models."
    },
    {
      "type": "USES",
      "source": "Generative Models",
      "target": "Large Datasets",
      "description": "Generative models are trained on large datasets collected from the internet."
    },
    {
      "type": "COMPARES_TO",
      "source": "CNN",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "The paper discusses the differences between CNN-generated images and those generated by diffusion models."
    },
    {
      "type": "USES",
      "source": "Diffusion Models",
      "target": "Data Compression",
      "description": "Diffusion models may be useful for data compression."
    },
    {
      "type": "IMPROVES",
      "source": "Diffusion Models",
      "target": "Representation Learning",
      "description": "Diffusion models might contribute to representation learning on unlabeled raw data."
    },
    {
      "type": "EXTENDS",
      "source": "Diffusion Models",
      "target": "Image Classification",
      "description": "Diffusion models can be applied to a range of downstream tasks including image classification."
    },
    {
      "type": "EXTENDS",
      "source": "Diffusion Models",
      "target": "Reinforcement Learning",
      "description": "Diffusion models can be applied to a range of downstream tasks including reinforcement learning."
    },
    {
      "type": "APPLICATION_OF",
      "source": "Diffusion Models",
      "target": "Creative Uses",
      "description": "Diffusion models might become viable for creative uses in art, photography, and music."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Diffusion Models",
      "description": "Denoising diffusion probabilistic models utilize diffusion models as a foundational technique."
    },
    {
      "type": "SUPPORTED_BY",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "ONR PECASE",
      "description": "The research was supported by ONR PECASE funding."
    },
    {
      "type": "SUPPORTED_BY",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "NSF Graduate Research Fellowship",
      "description": "The research was supported by the NSF Graduate Research Fellowship."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Google\u2019s Tensor Flow Research Cloud (TFRC)",
      "description": "The research utilized Google\u2019s Tensor Flow Research Cloud for computational resources."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "GAN",
      "description": "Denoising Diffusion Probabilistic Models build on concepts from GANs."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Energy-based model",
      "description": "Denoising Diffusion Probabilistic Models utilize principles from energy-based models."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Neural Ordinary Differential Equations",
      "description": "Denoising Diffusion Probabilistic Models extend the ideas presented in Neural Ordinary Differential Equations."
    },
    {
      "type": "IMPROVES",
      "source": "Pixel SNAIL",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Pixel SNAIL improves upon the generative capabilities of Denoising Diffusion Probabilistic Models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Sparse Transformers",
      "description": "Denoising Diffusion Probabilistic Models are compared to Sparse Transformers in terms of performance."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Energy-Based Models",
      "description": "Denoising diffusion models build on the principles of energy-based models."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Residual Energy-Based Models",
      "description": "Denoising diffusion models utilize concepts from residual energy-based models for text generation."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Non-linear Independent Components Estimation (NICE)",
      "description": "The paper introduces the concept of NICE as a foundational technique for their model."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Real NVP",
      "description": "The paper discusses Real NVP as a relevant technique for density estimation."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Generative Adversarial Nets",
      "description": "Both are frameworks for generative modeling but utilize different approaches."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Flow Contrastive Estimation",
      "description": "Denoising diffusion models build on concepts from flow-based estimation techniques."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Variational Walkback",
      "description": "Denoising diffusion models extend ideas from variational methods."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "FFJORD",
      "description": "Denoising diffusion models leverage continuous dynamics concepts from FFJORD."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "International Conference on Learning Representations",
      "description": "The model was presented at this conference."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "continuous dynamics for scalable reversible generative models",
      "description": "The model builds on the concepts of continuous dynamics in generative modeling."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "GAN",
      "description": "Denoising diffusion models improve upon GANs by providing a more stable training process."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Diffusion Process",
      "description": "Denoising diffusion models are built on the principles of diffusion processes."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "beta-VAE",
      "description": "The performance of denoising diffusion models is compared to that of beta-VAEs."
    },
    {
      "type": "IMPROVES",
      "source": "Flow++",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Flow++ enhances generative modeling techniques that can be related to diffusion processes."
    },
    {
      "type": "COMPARES_TO",
      "source": "Video Pixel Networks",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Video Pixel Networks are compared to diffusion models in terms of generative capabilities."
    },
    {
      "type": "COMPARES_TO",
      "source": "Neural Audio Synthesis",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Neural Audio Synthesis techniques are evaluated against diffusion models for audio generation."
    },
    {
      "type": "IMPROVES",
      "source": "Progressive Growing of GANs",
      "target": "Generative Adversarial Networks (GANs)",
      "description": "Progressive Growing of GANs enhances the quality, stability, and variation of GANs."
    },
    {
      "type": "EXTENDS",
      "source": "Style-based Generator Architecture",
      "target": "Generative Adversarial Networks (GANs)",
      "description": "The Style-based Generator Architecture extends the capabilities of GANs by introducing style control."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Generative Adversarial Networks",
      "description": "Denoising diffusion models build on the principles of generative adversarial networks."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Stochastic Optimization",
      "description": "Denoising diffusion models utilize stochastic optimization techniques for training."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Glow",
      "description": "Denoising diffusion models are compared to Glow in terms of generative capabilities."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Auto-encoding Variational Bayes",
      "description": "Denoising diffusion models are evaluated against auto-encoding variational Bayes models."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Variational Inference",
      "description": "Denoising Diffusion Probabilistic Models build on the principles of variational inference."
    },
    {
      "type": "IMPROVES",
      "source": "Auto-encoding Variational Bayes",
      "target": "Variational Inference",
      "description": "Auto-encoding Variational Bayes improves the efficiency of variational inference."
    },
    {
      "type": "EXTENDS",
      "source": "Hamiltonian Monte Carlo",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Denoising Diffusion Probabilistic Models extend the concepts of Hamiltonian Monte Carlo for generative modeling."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Inverse Autoregressive Flow",
      "description": "Denoising Diffusion Probabilistic Models utilize Inverse Autoregressive Flow for improved sampling."
    },
    {
      "type": "COMPARES_TO",
      "source": "Energy-inspired Models",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Energy-inspired Models are compared to Denoising Diffusion Probabilistic Models in terms of sampling efficiency."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Generative Modeling",
      "description": "Denoising Diffusion Probabilistic Models introduce a novel approach to generative modeling."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Spectral Normalization",
      "description": "Denoising Diffusion Probabilistic Models may utilize spectral normalization to improve training stability."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "VQ-DRAW",
      "description": "Denoising Diffusion Probabilistic Models are compared to VQ-DRAW in terms of generative performance."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Energy-Based Models",
      "description": "Denoising Diffusion Probabilistic Models extend the concepts of energy-based models in their formulation."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Maximum Likelihood Learning",
      "description": "Denoising Diffusion Probabilistic Models utilize maximum likelihood learning for parameter estimation."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Energy-Based Models",
      "description": "Denoising Diffusion Probabilistic Models extend the concepts of energy-based models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Autoregressive Quantile Networks",
      "description": "Denoising Diffusion Probabilistic Models are compared to autoregressive quantile networks in terms of generative performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Wave Glow",
      "description": "Denoising Diffusion Probabilistic Models are evaluated against Wave Glow for speech synthesis tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "VQ-VAE",
      "description": "Denoising Diffusion Probabilistic Models are compared with VQ-VAE for generating diverse high-fidelity images."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Variational Inference",
      "description": "Denoising Diffusion Probabilistic Models build on the principles of variational inference."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "VQ-VAE-2",
      "description": "The performance of Denoising Diffusion Probabilistic Models is compared to VQ-VAE-2 in generating images."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Stochastic Backpropagation",
      "description": "Denoising Diffusion Probabilistic Models utilize stochastic backpropagation for training."
    },
    {
      "type": "EXTENDS",
      "source": "U-Net",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Denoising Diffusion Probabilistic Models extend the U-Net architecture for their applications."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Weight Normalization",
      "description": "Denoising Diffusion Probabilistic Models may utilize weight normalization to improve training efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Markov Chain Monte Carlo",
      "target": "Variational Inference",
      "description": "Markov Chain Monte Carlo and variational inference are compared as methods for approximating posterior distributions."
    },
    {
      "type": "IMPROVES",
      "source": "Pixel CNN++",
      "target": "Pixel CNN",
      "description": "Pixel CNN++ improves upon the original Pixel CNN model."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Generative modeling",
      "description": "Denoising Diffusion Probabilistic Models utilize generative modeling techniques."
    },
    {
      "type": "BUILDS_ON",
      "source": "A-NICE-MC",
      "target": "Generative modeling",
      "description": "A-NICE-MC builds on generative modeling principles."
    },
    {
      "type": "EXTENDS",
      "source": "Deep unsupervised learning",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Denoising Diffusion Probabilistic Models extend concepts from deep unsupervised learning."
    },
    {
      "type": "IMPROVES",
      "source": "Score-based generative models",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Denoising diffusion probabilistic models build upon the principles of score-based generative models."
    },
    {
      "type": "BUILDS_ON",
      "source": "Wave Net",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Denoising diffusion probabilistic models are influenced by the generative modeling techniques used in Wave Net."
    },
    {
      "type": "BUILDS_ON",
      "source": "Pixel Recurrent Neural Networks",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Denoising diffusion probabilistic models extend concepts from pixel recurrent neural networks."
    },
    {
      "type": "BUILDS_ON",
      "source": "Pixel CNN",
      "target": "Denoising Diffusion Probabilistic Models",
      "description": "Denoising diffusion probabilistic models incorporate ideas from Pixel CNN for image generation."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Score Matching",
      "description": "Denoising Diffusion Probabilistic Models utilize principles from score matching."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Attention Mechanism",
      "description": "Denoising Diffusion Probabilistic Models may incorporate attention mechanisms for improved performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Denoising Autoencoders",
      "description": "Denoising Diffusion Probabilistic Models are compared to denoising autoencoders in terms of performance."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Pixel CNN",
      "description": "Denoising Diffusion Probabilistic Models introduce concepts that build on the Pixel CNN architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Predictive sampling",
      "description": "Denoising diffusion models build on the concept of predictive sampling for generating data."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Stochastic normalizing flows",
      "description": "Denoising diffusion models utilize stochastic normalizing flows for efficient sampling."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Group normalization",
      "description": "Denoising diffusion models improve upon group normalization techniques for better performance."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Generative ConvNet",
      "description": "Denoising diffusion models extend the capabilities of generative convnets."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Learning descriptor networks",
      "description": "Denoising Diffusion Probabilistic Models may utilize descriptor networks for improved generative performance."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Energy-based spatial-temporal generative convnets",
      "description": "Denoising Diffusion Probabilistic Models may leverage energy-based techniques for dynamic pattern generation."
    },
    {
      "type": "BUILDS_ON",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Wide residual networks",
      "description": "Denoising Diffusion Probabilistic Models may build on the principles of wide residual networks for enhanced performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Progressive GAN",
      "description": "The performance of Denoising Diffusion Probabilistic Models is compared to Progressive GAN."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Style GAN",
      "description": "The performance of Denoising Diffusion Probabilistic Models is compared to Style GAN."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Style GAN 2",
      "description": "The performance of Denoising Diffusion Probabilistic Models is compared to Style GAN 2."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Variational Bound",
      "description": "Denoising Diffusion Probabilistic Models utilize the variational bound for theoretical interpretation."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Lossy Compression",
      "description": "Denoising Diffusion Probabilistic Models extend the concept of lossy compression in their framework."
    },
    {
      "type": "EVALUATES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "The model is evaluated on the CIFAR 10 dataset to assess its performance."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Rate-Distortion",
      "description": "The model utilizes rate-distortion values to analyze its performance."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "RMSE",
      "description": "The model employs RMSE as a metric to quantify distortion in generated images."
    },
    {
      "type": "BUILDS_ON",
      "source": "Pixel CNN++",
      "target": "U-Net",
      "description": "Pixel CNN++ architecture is based on the U-Net structure."
    },
    {
      "type": "IMPROVES",
      "source": "Pixel CNN++",
      "target": "Pixel CNN",
      "description": "Pixel CNN++ improves upon the original Pixel CNN model."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Divergence KL",
      "description": "Denoising Diffusion Probabilistic Models utilize KL divergence in their loss function."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Group Normalization",
      "description": "The implementation of Denoising Diffusion Probabilistic Models replaces weight normalization with group normalization."
    },
    {
      "type": "USES",
      "source": "CIFAR 10",
      "target": "Convolutional Residual Blocks",
      "description": "The CIFAR 10 model uses convolutional residual blocks for its architecture."
    },
    {
      "type": "USES",
      "source": "CIFAR 10",
      "target": "Self-Attention Blocks",
      "description": "The CIFAR 10 model incorporates self-attention blocks at the 16x16 resolution."
    },
    {
      "type": "IMPROVES",
      "source": "Convolutional Residual Blocks",
      "target": "Diffusion Probabilistic Models",
      "description": "Convolutional residual blocks improve the performance of diffusion probabilistic models."
    },
    {
      "type": "IMPROVES",
      "source": "Self-Attention Blocks",
      "target": "Diffusion Probabilistic Models",
      "description": "Self-attention blocks enhance the capabilities of diffusion probabilistic models."
    },
    {
      "type": "BUILDS_ON",
      "source": "Diffusion Probabilistic Models",
      "target": "Transformer",
      "description": "The model architecture builds on concepts from transformer models."
    },
    {
      "type": "COMPARES_TO",
      "source": "TPU v3-8",
      "target": "8 V100 GPUs",
      "description": "TPU v3-8 is compared to 8 V100 GPUs in terms of performance."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Celeb A-HQ",
      "description": "The model is trained using the Celeb A-HQ dataset."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "LSUN Bedroom",
      "description": "The model is trained using the LSUN Bedroom dataset."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "LSUN Cat",
      "description": "The model is trained using the LSUN Cat dataset."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "LSUN Church",
      "description": "The model is trained using the LSUN Church dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Hyperparameter search",
      "target": "CIFAR 10 sample quality",
      "description": "The hyperparameter search is performed to optimize the sample quality on the CIFAR 10 dataset."
    },
    {
      "type": "INTRODUCES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "\u03b2t schedule",
      "description": "The model introduces a \u03b2t schedule for controlling noise variance."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Dropout",
      "description": "Uses dropout to prevent overfitting during training."
    },
    {
      "type": "IMPROVES",
      "source": "Random Horizontal Flips",
      "target": "Sample Quality",
      "description": "Improves sample quality when applied during training."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Pixel CNN++",
      "description": "Compared to Pixel CNN++ in terms of sample quality."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Adam",
      "description": "Uses Adam optimizer for training."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adam",
      "target": "RMSProp",
      "description": "Compared Adam and RMSProp during the experimentation process."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "The model evaluates sample quality using the CIFAR 10 dataset."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "LSUN",
      "description": "The model calculates FID scores using the LSUN dataset."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Celeb A-HQ",
      "description": "The model utilizes the Celeb A-HQ dataset for image generation."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Inception Score",
      "description": "The model's performance is compared using the Inception Score metric."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "FID",
      "description": "The model's performance is evaluated using the FID metric."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "EMA",
      "description": "The model employs EMA on parameters with a specified decay factor."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "NCSN",
      "description": "The proposed model architecture and process definitions differ from NCSN in ways that improve sample quality."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "U-Net",
      "description": "The proposed model uses a U-Net architecture with self-attention."
    },
    {
      "type": "USES",
      "source": "NCSN",
      "target": "Re\ufb01ne Net",
      "description": "NCSN employs a Re\ufb01ne Net architecture with dilated convolutions."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Latent Variable Model",
      "description": "The proposed model improves sample quality by training the sampler as a latent variable model."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Sinusoidal Position Embedding",
      "description": "The proposed model conditions all layers on time step t using sinusoidal position embedding."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Diffusion Process",
      "description": "The proposed model employs a diffusion process that scales down data to maintain variance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "NCSN",
      "description": "The proposed model differs from NCSN in its forward process and scaling factors."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Langevin-like sampler",
      "description": "The model employs a Langevin-like sampler with coefficients derived from the forward process."
    },
    {
      "type": "IMPROVES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Markov chain",
      "description": "The model ensures reversibility through a Markov chain with conditional Gaussians."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Variational inference",
      "description": "The training procedure utilizes variational inference to match the data distribution."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Variational Inference",
      "description": "The model employs variational inference to train its sampler."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "NCSN",
      "description": "The paper contrasts its approach with NCSN, highlighting differences in sampler training."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Celeb A-HQ",
      "description": "The model is trained on the Celeb A-HQ dataset to generate images."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "The model is trained on the CIFAR 10 dataset to generate images."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "LSUN",
      "description": "The model is trained on the LSUN dataset to generate images."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Latent Structure",
      "description": "The model incorporates latent structure in its sampling process."
    },
    {
      "type": "EXTENDS",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Reverse Process Stochasticity",
      "description": "The model accounts for reverse process stochasticity during sampling."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Celeb A",
      "description": "The models utilize the Celeb A dataset for training and evaluation."
    },
    {
      "type": "ENCODES",
      "source": "Latent Variables",
      "target": "High-level Attributes",
      "description": "Latent variables encode high-level attributes despite their imperceptibility."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Interpolation",
      "description": "The models compare the effects of varying diffusion steps on the structure of images during interpolation."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "The model evaluates its performance using the CIFAR 10 dataset."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Celeb A-HQ",
      "description": "The model generates samples using the Celeb A-HQ dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Inception Score",
      "description": "The model's performance is compared using the Inception Score metric."
    },
    {
      "type": "COMPARES_TO",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "FID",
      "description": "The model's performance is compared using the FID metric."
    },
    {
      "type": "EVALUATES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "FID",
      "description": "The model's performance is evaluated using the FID metric."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "Celeb A-HQ",
      "description": "The model is trained on the Celeb A-HQ dataset."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "CIFAR 10",
      "description": "The model is trained on the CIFAR 10 dataset."
    },
    {
      "type": "USES",
      "source": "Denoising Diffusion Probabilistic Models",
      "target": "LSUN",
      "description": "The model is trained on the LSUN dataset."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "The paper introduces the technique of Low-Rank Adaptation."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "Transformer",
      "description": "Low-Rank Adaptation injects trainable matrices into each layer of the Transformer architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "Full Fine-Tuning",
      "description": "The paper compares the feasibility of using GPT-3 with full fine-tuning versus Low-Rank Adaptation."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Training Throughput",
      "description": "LoRA increases the training throughput while reducing the number of trainable parameters."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA is compared to GPT-3 fine-tuned with Adam, showing significant reductions in parameters and memory."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "RoBERTa",
      "description": "LoRA's performance is evaluated against RoBERTa."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "DeBERTa",
      "description": "LoRA's performance is evaluated against DeBERTa."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "GPT-2",
      "description": "LoRA's performance is evaluated against GPT-2."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Transformer",
      "description": "LoRA injects trainable rank decomposition matrices into each layer of the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "Fine-tuning",
      "description": "LoRA improves upon fine-tuning by reducing the number of parameters that need to be updated."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-2",
      "target": "GPT-3",
      "description": "GPT-3 is compared to GPT-2 in terms of model size and parameter count."
    },
    {
      "type": "USES",
      "source": "Natural Language Processing",
      "target": "RoBERTa",
      "description": "RoBERTa is used in various applications within natural language processing."
    },
    {
      "type": "USES",
      "source": "Natural Language Processing",
      "target": "DeBERTa",
      "description": "DeBERTa is utilized in natural language processing tasks."
    },
    {
      "type": "USES",
      "source": "Natural Language Processing",
      "target": "GPT-2",
      "description": "GPT-2 is employed in natural language processing applications."
    },
    {
      "type": "USES",
      "source": "Natural Language Processing",
      "target": "GPT-3",
      "description": "GPT-3 is applied in various natural language processing scenarios."
    },
    {
      "type": "IMPROVES",
      "source": "Fine-tuning",
      "target": "GPT-3",
      "description": "Fine-tuning significantly boosts the performance of GPT-3 compared to few-shot learning."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA adapts GPT-3 by modifying only a small number of parameters for new tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Existing Techniques",
      "description": "LoRA is compared to existing techniques that adapt parameters or learn external modules."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation (LoRA)",
      "description": "The paper introduces the LoRA technique for adapting large language models."
    },
    {
      "type": "USES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "GPT-3",
      "description": "The paper uses GPT-3 as a reference model for demonstrating the effectiveness of the proposed adaptation technique."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "Intrinsic Rank",
      "description": "LoRA is based on the hypothesis that changes in weights during adaptation have a low intrinsic rank."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "Over-Parametrized Models",
      "description": "LoRA takes inspiration from the concept that over-parametrized models can reside on a low intrinsic dimension."
    },
    {
      "type": "EXTENDS",
      "source": "Inference Latency",
      "target": "Model Depth",
      "description": "Inference latency is often introduced by extending model depth."
    },
    {
      "type": "EXTENDS",
      "source": "Inference Latency",
      "target": "Usable Sequence Length",
      "description": "Inference latency can also be affected by reducing the model's usable sequence length."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA is demonstrated using GPT-3 as an example."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Task Switching",
      "description": "LoRA reduces storage requirements and overhead for task switching."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Adaptive Optimizers",
      "description": "LoRA makes training more efficient by utilizing adaptive optimizers."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Dense Layers",
      "description": "LoRA optimizes rank decomposition matrices of dense layers during adaptation."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Adaptive Optimizers",
      "description": "LoRA improves the efficiency of adaptive optimizers by reducing the need to calculate gradients for most parameters."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Transformer",
      "description": "LoRA is designed to work with the Transformer architecture."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Inference Latency",
      "description": "LoRA introduces no inference latency compared to fully fine-tuned models."
    },
    {
      "type": "EXTENDS",
      "source": "LoRA",
      "target": "Prefix-Tuning",
      "description": "LoRA can be combined with prefix-tuning, extending its capabilities."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "autoregressive language model",
      "description": "LoRA is proposed as a method to adapt autoregressive language models."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Adam",
      "description": "LoRA uses the Adam optimization algorithm for model optimization."
    },
    {
      "type": "BUILDS_ON",
      "source": "LoRA",
      "target": "Transformer",
      "description": "LoRA builds on the Transformer architecture for its implementation."
    },
    {
      "type": "USES",
      "source": "P\u03a6(y|x)",
      "target": "Transformer",
      "description": "P\u03a6(y|x) is based on the Transformer architecture."
    },
    {
      "type": "EXTENDS",
      "source": "GPT",
      "target": "Transformer",
      "description": "GPT is an extension of the Transformer architecture for multi-task learning."
    },
    {
      "type": "REPRESENTS",
      "source": "Z",
      "target": "Task",
      "description": "Z represents training datasets for various downstream tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "NL 2 SQL",
      "target": "summarization",
      "description": "NL 2 SQL and summarization are examples of different downstream tasks."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "Full Fine-Tuning",
      "description": "Low-Rank Adaptation provides a more parameter-efficient method compared to Full Fine-Tuning."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "\u0398",
      "description": "Low-Rank Adaptation uses a smaller set of parameters \u0398 to encode the parameter increment \u2206\u03a6."
    },
    {
      "type": "USES",
      "source": "Full Fine-Tuning",
      "target": "\u03a60",
      "description": "Full Fine-Tuning initializes the model with pre-trained weights \u03a60."
    },
    {
      "type": "RELATES_TO",
      "source": "\u2206\u03a6",
      "target": "\u0398",
      "description": "The parameter increment \u2206\u03a6 is optimized over the smaller set of parameters \u0398."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "The paper introduces the low-rank adaptation technique for large language models."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "GPT-3",
      "description": "The low-rank adaptation technique is applied to the GPT-3 model."
    },
    {
      "type": "COMPARES_TO",
      "source": "Low-Rank Adaptation",
      "target": "Adapter Layers",
      "description": "The paper compares low-rank adaptation with the adapter layers technique for model adaptation."
    },
    {
      "type": "COMPARES_TO",
      "source": "Low-Rank Adaptation",
      "target": "Input Layer Activations Optimization",
      "description": "The paper compares low-rank adaptation with input layer activations optimization for model adaptation."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "The paper introduces the Low-Rank Adaptation technique for large language models."
    },
    {
      "type": "USES",
      "source": "Adapter Layers",
      "target": "Transformer",
      "description": "Adapter layers are used within the Transformer architecture to adapt models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adapter Layers",
      "target": "BERT",
      "description": "The paper compares the performance of adapter layers with BERT."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adapter Layers",
      "target": "GPT",
      "description": "The paper compares the performance of adapter layers with GPT."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "Inference Latency",
      "description": "Low-Rank Adaptation aims to improve inference latency in large models."
    },
    {
      "type": "USES",
      "source": "Adapter Layers",
      "target": "Low-Rank Adaptation",
      "description": "Adapter layers utilize low-rank adaptation techniques to minimize parameter count."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-2",
      "target": "Adapter Layers",
      "description": "The performance of adapter layers is compared to the inference speed of GPT-2."
    },
    {
      "type": "EXTENDS",
      "source": "Low-Rank Adaptation",
      "target": "Large Neural Networks",
      "description": "Low-rank adaptation extends the capabilities of large neural networks."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "Adapter Layers",
      "description": "Low-rank adaptation improves the efficiency of adapter layers."
    },
    {
      "type": "BUILDS_ON",
      "source": "LoRA",
      "target": "Large Language Models",
      "description": "LoRA is a technique designed to adapt large language models."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "All Reduce",
      "description": "LoRA requires synchronous GPU operations like All Reduce for model adaptation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Broadcast",
      "description": "LoRA requires synchronous GPU operations like Broadcast for model adaptation."
    },
    {
      "type": "COMPARES_TO",
      "source": "Prefix Tuning",
      "target": "LoRA",
      "description": "Prefix tuning is compared to LoRA in terms of optimization difficulty and performance."
    },
    {
      "type": "EXTENDS",
      "source": "Prefix Tuning",
      "target": "Large Language Models",
      "description": "Prefix tuning extends the capabilities of large language models by optimizing prompts."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Adapter Tuning",
      "description": "LoRA employs adapter tuning to adapt large language models."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Fine-Tune",
      "description": "LoRA is compared to traditional fine-tuning methods in terms of performance."
    },
    {
      "type": "INTRODUCES",
      "source": "Adapter L",
      "target": "Adapter Tuning",
      "description": "Adapter L is a variant of adapter tuning introduced in the paper."
    },
    {
      "type": "INTRODUCES",
      "source": "Adapter H",
      "target": "Adapter Tuning",
      "description": "Adapter H is another variant of adapter tuning introduced in the paper."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "Inference Latency",
      "description": "The paper evaluates the inference latency of models using LoRA."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "The paper introduces the concept of Low-Rank Adaptation for adapting large language models."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "Intrinsic Rank",
      "description": "Low-Rank Adaptation hypothesizes that weight updates during adaptation have a low intrinsic rank."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "Intrinsic Dimension",
      "description": "The technique builds on the concept of intrinsic dimension as shown by Aghajanyan et al. (2020)."
    },
    {
      "type": "USES",
      "source": "Neural Network",
      "target": "Matrix Multiplication",
      "description": "Neural networks perform matrix multiplication in their dense layers."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "The paper introduces the technique of Low-Rank Adaptation for large language models."
    },
    {
      "type": "USES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Adam",
      "description": "The paper uses the Adam optimization algorithm for training the model."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation improves the training efficiency of large language models."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "W0",
      "description": "The technique builds on the concept of a frozen initial weight matrix W0."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "\u0394W",
      "description": "The technique utilizes the low-rank update \u0394W as part of its adaptation process."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Full Fine-tuning",
      "description": "LoRA improves upon full fine-tuning by allowing adaptation without full-rank gradient updates."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Adapter-based methods",
      "description": "LoRA converges to the original model while adapter-based methods converge to an MLP."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Prefix-based methods",
      "description": "LoRA allows for longer input sequences compared to prefix-based methods."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "The paper introduces the technique of Low-Rank Adaptation for large language models."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "W",
      "description": "Low-Rank Adaptation uses the weight matrix W for model adaptation."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "W0",
      "description": "Low-Rank Adaptation uses the original weight matrix W0 as a base."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "BA",
      "description": "Low-Rank Adaptation incorporates the low-rank component BA for task adaptation."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "B\u2032A\u2032",
      "description": "Low-Rank Adaptation can switch to a different low-rank component B\u2032A\u2032 for new tasks."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Transformer",
      "description": "LoRA can be applied to the Transformer architecture to reduce trainable parameters."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Parameter Efficiency",
      "description": "LoRA improves parameter efficiency by adapting only specific weight matrices."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention Module",
      "description": "The Transformer architecture includes a self-attention module that utilizes weight matrices."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "MLP Module",
      "description": "The Transformer architecture includes an MLP module that is frozen during downstream tasks."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "VRAM usage",
      "description": "LoRA reduces VRAM usage significantly during training."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Checkpoint size",
      "description": "LoRA reduces the checkpoint size drastically, allowing for more efficient storage."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Transformer",
      "description": "LoRA is applied to the Transformer architecture for adaptation."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA is evaluated in the context of adapting the GPT-3 model."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA improves the training efficiency of GPT-3 by allowing for faster adaptation with fewer resources."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "VRAM",
      "description": "LoRA utilizes VRAM to store pre-trained weights for efficient model swapping."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "RoBERTa",
      "description": "LoRA is evaluated on the RoBERTa model for downstream task performance."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "DeBERTa",
      "description": "LoRA is evaluated on the DeBERTa model for downstream task performance."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "GPT-2",
      "description": "LoRA is evaluated on the GPT-2 model for downstream task performance."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA is evaluated on the GPT-3 model for large-scale experiments."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "GLUE",
      "description": "LoRA's performance is compared on the GLUE benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "WikiSQL",
      "description": "LoRA's performance is compared on the WikiSQL dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "SAMSum",
      "description": "LoRA's performance is compared on the SAMSum dataset."
    },
    {
      "type": "USES",
      "source": "Fine-Tuning (FT)",
      "target": "GPT-2",
      "description": "Fine-tuning is applied to adapt the GPT-2 model."
    },
    {
      "type": "USES",
      "source": "Fine-Tuning (FT)",
      "target": "GPT-3",
      "description": "Fine-tuning is applied to adapt the GPT-3 model."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Fine-Tuning (FT)",
      "description": "LoRA is compared to traditional fine-tuning methods."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Fine-Tuning (FT)",
      "description": "LoRA improves upon the fine-tuning technique by reducing the number of parameters updated."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Training Throughput",
      "description": "LoRA improves the training throughput compared to full fine-tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA's performance is compared to the full fine-tuning of GPT-3."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "RoBERTa",
      "description": "LoRA introduces a method for adapting RoBERTa with low-rank updates."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "MNLI",
      "description": "LoRA is evaluated on the MNLI dataset among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "SST-2",
      "description": "LoRA is evaluated on the SST-2 dataset among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "MRPC",
      "description": "LoRA is evaluated on the MRPC dataset among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "CoLA",
      "description": "LoRA is evaluated on the CoLA dataset among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "QNLI",
      "description": "LoRA is evaluated on the QNLI dataset among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "QQP",
      "description": "LoRA is evaluated on the QQP dataset among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "RTE",
      "description": "LoRA is evaluated on the RTE dataset among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "STS-B",
      "description": "LoRA is evaluated on the STS-B dataset among others."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "RoBERTa (FT)",
      "description": "LoRA is compared to the fine-tuned version of RoBERTa."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "RoBERTa (Bit Fit)",
      "description": "LoRA is compared to the Bit Fit version of RoBERTa."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "RoBERTa (Adpt D)",
      "description": "LoRA is compared to the Adpt D version of RoBERTa."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "RoBERTa (Adpt P)",
      "description": "LoRA is compared to the Adpt P version of RoBERTa."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "RoBERTa (Adpt H)",
      "description": "LoRA is compared to the Adpt H version of RoBERTa."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "Ro BERTalarge",
      "description": "LoRA improves the performance of Ro BERTalarge by enabling efficient adaptation."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "De BERTa XXL",
      "description": "LoRA enhances the performance of De BERTa XXL through low-rank updates."
    },
    {
      "type": "COMPARES_TO",
      "source": "Ro BERTalarge",
      "target": "Ro BERTabase",
      "description": "Ro BERTalarge is compared to Ro BERTabase in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "De BERTa XXL",
      "target": "Ro BERTalarge",
      "description": "De BERTa XXL is compared to Ro BERTalarge on the GLUE benchmark."
    },
    {
      "type": "EXTENDS",
      "source": "Pre\ufb01x-embedding tuning",
      "target": "Pre\ufb01x-layer tuning",
      "description": "Pre\ufb01x-layer tuning is an extension of pre\ufb01x-embedding tuning."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Large Language Models",
      "description": "LoRA is used to adapt large language models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Bias-only or Bit Fit",
      "target": "LoRA",
      "description": "Bias-only or Bit Fit serves as a baseline for comparison with LoRA."
    },
    {
      "type": "COMPARES_TO",
      "source": "Pre\ufb01x-embedding tuning",
      "target": "Pre\ufb01x-layer tuning",
      "description": "Pre\ufb01x-layer tuning is compared to pre\ufb01x-embedding tuning."
    },
    {
      "type": "EXTENDS",
      "source": "Prefix-layer tuning",
      "target": "Prefix-embedding tuning",
      "description": "Prefix-layer tuning is an extension of prefix-embedding tuning."
    },
    {
      "type": "USES",
      "source": "Adapter tuning",
      "target": "Transformer",
      "description": "Adapter tuning inserts layers into the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "Adapter L",
      "target": "Adapter H",
      "description": "Adapter L is a more efficient design that improves upon Adapter H."
    },
    {
      "type": "IMPROVES",
      "source": "Adapter H",
      "target": "Adapter L",
      "description": "Adapter L is a more efficient design compared to Adapter H."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adapter L",
      "target": "Adapter P",
      "description": "Adapter L is similar to Adapter P proposed by Pfeiffer et al. (2021)."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adapter L",
      "target": "Adapter D",
      "description": "Adapter D is another baseline that drops some adapter layers for efficiency."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Wq",
      "description": "LoRA applies low-rank adaptation to the weight matrix Wq."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Wv",
      "description": "LoRA applies low-rank adaptation to the weight matrix Wv."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Trainable Parameters",
      "description": "LoRA introduces the concept of adding trainable pairs of rank decomposition matrices to existing weight matrices."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Weight Matrices",
      "description": "LoRA uses existing weight matrices (Wq and Wv) to apply its adaptation technique."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Large Language Models",
      "description": "LoRA improves the adaptability of large language models by introducing low-rank adaptations."
    },
    {
      "type": "DETERMINES",
      "source": "Trainable Parameters",
      "target": "Rank",
      "description": "The number of trainable parameters is determined by the rank and the shape of the original weights."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "GPT-2 M",
      "description": "LoRA improves the performance of the GPT-2 M model."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "GPT-2 L",
      "description": "LoRA improves the performance of the GPT-2 L model."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-2 M (LoRA)",
      "target": "GPT-2 M (FT)",
      "description": "GPT-2 M (LoRA) is compared to GPT-2 M (FT) in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-2 L (LoRA)",
      "target": "GPT-2 L (FT)",
      "description": "GPT-2 L (LoRA) is compared to GPT-2 L (FT) in terms of performance metrics."
    },
    {
      "type": "IMPROVES",
      "source": "RoBERTa",
      "target": "BERT",
      "description": "RoBERTa optimizes the pre-training recipe of BERT and boosts its task performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "GPT-2",
      "description": "LoRA is compared against GPT-2 with different adaptation methods."
    },
    {
      "type": "USES",
      "source": "RoBERTa base",
      "target": "Hugging Face Transformers",
      "description": "The pre-trained RoBERTa models are taken from the Hugging Face Transformers library."
    },
    {
      "type": "COMPARES_TO",
      "source": "RoBERTa",
      "target": "GLUE",
      "description": "RoBERTa has been overtaken by larger models on the GLUE benchmark."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "RoBERTa base",
      "description": "LoRA is applied to the RoBERTa base model for adaptation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "RoBERTa large",
      "description": "LoRA is applied to the RoBERTa large model for adaptation."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Adapters",
      "description": "LoRA is compared to the adapter technique for performance evaluation."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "GLUE",
      "description": "LoRA's performance is evaluated on tasks from the GLUE benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Houlsby et al. (2019)",
      "description": "LoRA's performance is compared to the results from Houlsby et al. (2019)."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Pfeiffer et al. (2021)",
      "description": "LoRA's performance is compared to the results from Pfeiffer et al. (2021)."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "De BERTa XXL",
      "description": "Evaluates if LoRA can match the performance of a fully fine-tuned De BERTa XXL on GLUE."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "GPT-2",
      "description": "Investigates if LoRA is a competitive alternative to full fine-tuning on NLG models like GPT-2."
    },
    {
      "type": "USES",
      "source": "De BERTa",
      "target": "GLUE",
      "description": "De BERTa is evaluated on the GLUE benchmark."
    },
    {
      "type": "USES",
      "source": "De BERTa",
      "target": "Super GLUE",
      "description": "De BERTa performs competitively on the Super GLUE benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Li & Liang (2021)",
      "description": "The paper compares its setup and results to those of Li & Liang (2021)."
    },
    {
      "type": "USES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "E2E NLG Challenge",
      "description": "The paper presents results on the E2E NLG Challenge."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Web NLG",
      "description": "The paper evaluates its method on the Web NLG dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "DART",
      "description": "The paper evaluates its method on the DART dataset."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA improves the performance of GPT-3 over prior adaptation methods."
    },
    {
      "type": "USES",
      "source": "GPT-3",
      "target": "Wiki SQL",
      "description": "GPT-3 is evaluated on the Wiki SQL dataset."
    },
    {
      "type": "USES",
      "source": "GPT-3",
      "target": "MNLI-m",
      "description": "GPT-3 is evaluated on the MNLI-m dataset."
    },
    {
      "type": "USES",
      "source": "GPT-3",
      "target": "SAMSum",
      "description": "GPT-3 is evaluated on the SAMSum dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "full fine-tuning",
      "description": "LoRA is compared to full fine-tuning in terms of performance."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA is evaluated by scaling up to GPT-3 with 175 billion parameters."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "fine-tuning baseline",
      "description": "LoRA matches or exceeds the fine-tuning baseline on all three datasets."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Standard Deviation",
      "description": "LoRA uses standard deviation to report typical performance over random seeds."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Performance Drop",
      "description": "LoRA improves performance by avoiding significant drops when using an optimal number of special tokens."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Li & Liang (2021)",
      "description": "LoRA corroborates observations made by Li & Liang regarding performance drops."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Validation Accuracy",
      "description": "LoRA improves validation accuracy in low-data regimes."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Fine-Tune",
      "description": "LoRA is compared to Fine-Tune and other adaptation methods."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Prefix Embed",
      "description": "LoRA is compared to Prefix Embed in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Prefix Layer",
      "description": "LoRA is compared to Prefix Layer tuning methods."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Adapter(H)",
      "description": "LoRA is compared to Adapter(H) in terms of scalability."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Transformer",
      "description": "LoRA improves the scalability and task performance of Transformer-based models."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT builds on the Transformer architecture to achieve state-of-the-art performance in NLP."
    },
    {
      "type": "BUILDS_ON",
      "source": "GPT-2",
      "target": "Transformer",
      "description": "GPT-2 builds on the Transformer architecture for autoregressive language modeling."
    },
    {
      "type": "IMPROVES",
      "source": "Fine-Tuning",
      "target": "Model",
      "description": "Fine-tuning improves the performance of models on specific tasks."
    },
    {
      "type": "USES",
      "source": "GPT-3",
      "target": "Prompt Engineering",
      "description": "GPT-3 uses prompt engineering to adapt its behavior based on input prompts."
    },
    {
      "type": "BUILDS_ON",
      "source": "Fine-Tuning",
      "target": "Transformer",
      "description": "Fine-tuning builds on the Transformer architecture to enhance model performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "Task-Specific Data",
      "description": "GPT-3's performance is compared to training on task-specific data directly."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "Adapter Layers",
      "description": "Low-Rank Adaptation uses adapter layers to impose a low-rank constraint on weight updates."
    },
    {
      "type": "IMPROVES",
      "source": "Fine-tuning",
      "target": "Model Performance",
      "description": "Fine-tuning improves the performance of models on specific tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Fine-tuning",
      "target": "Parameter-Efficient Adaptation",
      "description": "Fine-tuning is often compared to parameter-efficient adaptation methods."
    },
    {
      "type": "EXTENDS",
      "source": "Adapter Layers",
      "target": "COMPACTER",
      "description": "COMPACTER is an extension of adapter layers that uses Kronecker products."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Adapter Layers",
      "description": "LoRA uses a similar bottleneck structure to impose a low-rank constraint on weight updates."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Parameter Efficiency",
      "description": "Combining LoRA with tensor product-based methods could potentially improve parameter efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Adapter Layers",
      "description": "LoRA can merge learned weights with main weights during inference, unlike adapter layers."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Li & Liang (2021)",
      "description": "The paper includes comparisons with the work of Li & Liang on prompt engineering."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "Low-Rank Structures",
      "description": "Low-Rank Adaptation leverages low-rank structures common in machine learning."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation is applied to large language models for optimization."
    },
    {
      "type": "EXTENDS",
      "source": "Prompt Engineering",
      "target": "Low-Rank Adaptation",
      "description": "Low-Rank Adaptation is described as a continuous and differentiable generalization of prompt engineering."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Structures",
      "target": "Machine Learning Problems",
      "description": "Many machine learning problems exhibit intrinsic low-rank structures."
    },
    {
      "type": "BUILDS_ON",
      "source": "Deep Learning Tasks",
      "target": "Low-Rank Properties",
      "description": "Deep learning tasks often result in learned neural networks that exhibit low-rank properties."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "Neural Network",
      "description": "Low-Rank Adaptation enhances the performance of neural networks on downstream tasks."
    },
    {
      "type": "USES",
      "source": "Low-Rank Constraint",
      "target": "Neural Network",
      "description": "Low-Rank Constraint is used to enforce low-rank properties during the training of neural networks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Neural Network",
      "target": "Neural Tangent Kernels",
      "description": "Neural networks outperform classical learning methods, including Neural Tangent Kernels, under certain conditions."
    },
    {
      "type": "EXTENDS",
      "source": "Low-Rank Adaptation",
      "target": "Frozen Model",
      "description": "Low-Rank Adaptation extends the concept of low-rank updates to frozen models for task adaptation."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "Adversarial Training",
      "description": "Low-rank adaptations can enhance the effectiveness of adversarial training."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "GPT-3",
      "description": "The low-rank adaptation technique is applied to the GPT-3 model to reduce trainable parameters."
    },
    {
      "type": "EXTENDS",
      "source": "Low-Rank Structure",
      "target": "Low-Rank Adaptation",
      "description": "The concept of low-rank structure underpins the low-rank adaptation technique."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA achieves a significant reduction in trainable parameters for GPT-3 without adversely affecting performance."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Transformer",
      "description": "LoRA is applied to the weight matrices of a pre-trained Transformer model."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Self-Attention Module",
      "description": "LoRA is applied to the weight matrices in the self-attention module of Transformers."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "W",
      "description": "The adaptation matrix \u2206W is compared to the original weight matrix W."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA aims to improve the performance of GPT-3 on downstream tasks."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA improves the performance of GPT-3 by adapting attention weights."
    },
    {
      "type": "USES",
      "source": "GPT-3",
      "target": "Wiki SQL",
      "description": "GPT-3 is evaluated on the Wiki SQL dataset."
    },
    {
      "type": "USES",
      "source": "GPT-3",
      "target": "Multi NLI",
      "description": "GPT-3 is evaluated on the Multi NLI dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Attention weights",
      "description": "LoRA compares the performance of different configurations of attention weights."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Validation accuracy",
      "description": "LoRA improves validation accuracy by adapting multiple weight matrices."
    },
    {
      "type": "COMPARES_TO",
      "source": "Wq",
      "target": "Wq, Wv",
      "description": "The performance of adapting Wq is compared to adapting both Wq and Wv."
    },
    {
      "type": "COMPARES_TO",
      "source": "Wq, Wv",
      "target": "Wq, Wk, Wv, Wo",
      "description": "The performance of adapting Wq and Wv is compared to adapting Wq, Wk, Wv, and Wo."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Weight Matrices",
      "description": "LoRA uses weight matrices Wq, Wv, Wk, and Wo for adaptation."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "Wiki SQL",
      "description": "LoRA is evaluated on the Wiki SQL dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "Multi NLI",
      "description": "LoRA is evaluated on the Multi NLI dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "Validation Accuracy",
      "description": "LoRA improves validation accuracy on datasets like Wiki SQL and Multi NLI."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "Update Matrix (\u2206W)",
      "description": "LoRA uses the update matrix \u2206W for adapting model weights."
    },
    {
      "type": "COMPARES_TO",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "GPT-2",
      "description": "LoRA's performance is compared to that of GPT-2 in the context of adaptation."
    },
    {
      "type": "EXTENDS",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "Intrinsic Rank",
      "description": "LoRA extends the concept of intrinsic rank in the context of model adaptation."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Large Language Models",
      "description": "LoRA improves the performance of large language models by allowing efficient fine-tuning."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "r",
      "description": "LoRA uses the parameter r to determine the rank for adaptation."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Retraining",
      "description": "LoRA is compared to retraining the entire model, indicating that it can be more efficient."
    },
    {
      "type": "EXTENDS",
      "source": "Pre-training",
      "target": "Downstream Task",
      "description": "The downstream task extends from the pre-training phase of the model."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation is applied to Large Language Models to improve their performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adaptation Matrices",
      "target": "Right-Singular Unitary Matrices",
      "description": "The adaptation matrices are compared based on their singular value decomposition results."
    },
    {
      "type": "MEASURES",
      "source": "Subspace Similarity",
      "target": "Grassmann Distance",
      "description": "Subspace similarity is measured using the Grassmann distance."
    },
    {
      "type": "FOCUSES_ON",
      "source": "Subspace Similarity",
      "target": "48th Layer",
      "description": "The analysis of subspace similarity is specifically focused on the 48th layer of the model."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Large Language Models",
      "description": "LoRA is introduced as a method to adapt large language models."
    },
    {
      "type": "COMPARES_TO",
      "source": "\u2206Wq",
      "target": "\u2206Wv",
      "description": "The paper compares the changes in query weights (\u2206Wq) and value weights (\u2206Wv) under different ranks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Ar=8",
      "target": "Ar=64",
      "description": "The paper compares the subspace similarity between configurations with different ranks (Ar=8 and Ar=64)."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "GPT-3",
      "description": "LoRA improves the performance of GPT-3 on downstream tasks by adapting its parameters."
    },
    {
      "type": "COMPARES_TO",
      "source": "Ar=8",
      "target": "Ar=64",
      "description": "The performance and characteristics of Ar=8 are compared to those of Ar=64."
    },
    {
      "type": "USES",
      "source": "Ar=8",
      "target": "GPT-3",
      "description": "Ar=8 uses the pre-trained GPT-3 model for adaptation."
    },
    {
      "type": "USES",
      "source": "Ar=64",
      "target": "GPT-3",
      "description": "Ar=64 uses the pre-trained GPT-3 model for adaptation."
    },
    {
      "type": "EXTENDS",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "Normalized Similarity",
      "description": "LoRA extends the concept of normalized similarity to evaluate subspace similarities."
    },
    {
      "type": "COMPARES_TO",
      "source": "\u2206Wq",
      "target": "\u2206Wv",
      "description": "\u2206Wq is observed to have a higher intrinsic rank compared to \u2206Wv."
    },
    {
      "type": "COMPARES_TO",
      "source": "\u2206W",
      "target": "W",
      "description": "The paper investigates the correlation between the adaptation matrix \u2206W and the original weight matrix W."
    },
    {
      "type": "COMPARES_TO",
      "source": "Gaussian Matrices",
      "target": "\u2206W",
      "description": "Gaussian matrices are used as a baseline for comparison with the adaptation matrix \u2206W."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "The paper introduces the technique of Low-Rank Adaptation for large language models."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "Frobenius norm",
      "description": "Low-Rank Adaptation uses the Frobenius norm to measure similarity between matrices."
    },
    {
      "type": "COMPARES_TO",
      "source": "\u2206W",
      "target": "W",
      "description": "The paper compares the low-rank adaptation \u2206W to the original weights W."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation builds on the foundation of large language models."
    },
    {
      "type": "COMPARES_TO",
      "source": "U",
      "target": "V",
      "description": "The paper compares the left and right singular-vector matrices U and V."
    },
    {
      "type": "COMPARES_TO",
      "source": "\u2206W",
      "target": "Random matrix",
      "description": "\u2206W shows a stronger correlation with W compared to a random matrix."
    },
    {
      "type": "IMPROVES",
      "source": "\u2206W",
      "target": "W",
      "description": "\u2206W amplifies features that are already present in W."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Frobenius norm",
      "description": "LoRA uses the Frobenius norm to evaluate the adaptation of weight matrices."
    },
    {
      "type": "EXTENDS",
      "source": "LoRA",
      "target": "Singular vectors",
      "description": "LoRA extends the concept of singular vectors by focusing on directions not emphasized in W."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Large Language Models",
      "description": "LoRA is proposed as an efficient adaptation strategy for fine-tuning large language models."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Ampli\ufb01cation Factor",
      "description": "LoRA utilizes the amplification factor to enhance important features for specific tasks."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Task-Switching",
      "description": "LoRA improves the ability to switch tasks quickly without introducing latency."
    },
    {
      "type": "COMPARES_TO",
      "source": "r = 64",
      "target": "r = 4",
      "description": "The paper compares the amplification factors for different values of r."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Transformer",
      "description": "LoRA is focused on adapting Transformer language models."
    },
    {
      "type": "EXTENDS",
      "source": "LoRA",
      "target": "Neural Networks",
      "description": "LoRA's principles are applicable to any neural networks with dense layers."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Large Language Models",
      "description": "LoRA improves the adaptation process of large language models by applying low-rank techniques."
    },
    {
      "type": "INFORMS",
      "source": "Rank-de\ufb01ciency",
      "target": "LoRA",
      "description": "The concept of rank-de\ufb01ciency informs the development of LoRA by suggesting that weight matrices could also be rank-de\ufb01cient."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation is a technique that builds on the architecture of large language models to improve their adaptability."
    },
    {
      "type": "USES",
      "source": "Adversarial Training",
      "target": "Robust Deep Learning",
      "description": "Adversarial training is used to enhance the robustness of deep learning models."
    },
    {
      "type": "EXTENDS",
      "source": "Convergence Theory",
      "target": "Over-parameterization",
      "description": "The convergence theory extends the understanding of how over-parameterization affects deep learning."
    },
    {
      "type": "INTRODUCES",
      "source": "Layer Normalization",
      "target": "Deep Learning",
      "description": "Layer normalization introduces a method to stabilize and accelerate the training of deep learning models."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "This paper introduces the technique of Low-Rank Adaptation for large language models."
    },
    {
      "type": "USES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Large Language Models",
      "description": "The paper discusses the adaptation of large language models using the proposed technique."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Semeval-2017",
      "description": "The paper evaluates the performance of the proposed method using the Semeval-2017 dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Deep Neural Networks",
      "description": "The paper compares the performance of low-rank adaptation with traditional deep neural network architectures."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "This paper introduces the Low-Rank Adaptation technique for large language models."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "BERT",
      "description": "The Low-Rank Adaptation technique builds on the BERT model for language understanding."
    },
    {
      "type": "USES",
      "source": "Deep Neural Networks",
      "target": "BERT",
      "description": "BERT utilizes deep neural networks as its underlying architecture."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Large Language Models",
      "description": "LoRA introduces a method for adapting large language models efficiently."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Bidirectional Transformers",
      "description": "LoRA uses bidirectional transformers as a foundation for its adaptation technique."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Paraphrase Quality",
      "description": "LoRA's performance is compared to existing methods in terms of paraphrase quality."
    },
    {
      "type": "EXTENDS",
      "source": "WebNLG",
      "target": "Large Language Models",
      "description": "WebNLG extends the capabilities of large language models by providing structured data for text generation."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation builds on the principles of large language models to enhance their performance."
    },
    {
      "type": "USES",
      "source": "Samsum corpus",
      "target": "Large Language Models",
      "description": "The Samsum corpus is used for training and evaluating large language models in the context of dialogue summarization."
    },
    {
      "type": "COMPARES_TO",
      "source": "Grassmann Discriminant Analysis",
      "target": "Kernel Methods",
      "description": "Grassmann Discriminant Analysis compares its performance against traditional kernel methods."
    },
    {
      "type": "INTRODUCES",
      "source": "Word-level Adversarial Re Programming (WARP)",
      "target": "Adversarial Techniques",
      "description": "WARP introduces a new approach to adversarial techniques by focusing on word-level modifications."
    },
    {
      "type": "IMPROVES",
      "source": "DeBERTa",
      "target": "BERT",
      "description": "DeBERTa improves upon BERT by introducing disentangled attention mechanisms."
    },
    {
      "type": "BUILDS_ON",
      "source": "Parameter-Efficient Transfer Learning",
      "target": "NLP",
      "description": "Parameter-Efficient Transfer Learning builds on the principles of transfer learning in natural language processing."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Low-Rank Expansions",
      "description": "LoRA uses low-rank expansions to adapt large language models efficiently."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Stochastic Optimization",
      "description": "LoRA introduces techniques that leverage stochastic optimization methods for model adaptation."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation is a technique specifically designed to enhance the performance of Large Language Models."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "Stochastic Optimization",
      "description": "Low-Rank Adaptation employs stochastic optimization methods for training."
    },
    {
      "type": "INTRODUCES",
      "source": "Gshard",
      "target": "Conditional Computation",
      "description": "Gshard introduces the concept of conditional computation for scaling models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Prompt Tuning",
      "target": "Low-Rank Adaptation",
      "description": "Prompt Tuning is compared to Low-Rank Adaptation in terms of parameter efficiency."
    },
    {
      "type": "EXTENDS",
      "source": "Prefix-Tuning",
      "target": "Prompt Tuning",
      "description": "Prefix-Tuning extends the ideas of Prompt Tuning by optimizing continuous prompts."
    },
    {
      "type": "USES",
      "source": "Learning overparameterized neural networks",
      "target": "Stochastic Gradient Descent",
      "description": "Learning overparameterized neural networks utilizes stochastic gradient descent for training."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Stochastic Gradient Descent",
      "description": "LoRA utilizes stochastic gradient descent for training."
    },
    {
      "type": "EXTENDS",
      "source": "LoRA",
      "target": "Low-Rank Adaptation",
      "description": "LoRA extends the concept of low-rank adaptation for large language models."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "This paper introduces the Low-Rank Adaptation technique for large language models."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "The Low-Rank Adaptation technique is applied to large language models."
    },
    {
      "type": "PRESENTED_AT",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "EMNLP 2020",
      "description": "The findings of this paper were presented at the EMNLP 2020 conference."
    },
    {
      "type": "ORGANIZED_BY",
      "source": "EMNLP 2020",
      "target": "Association for Computational Linguistics",
      "description": "The EMNLP 2020 conference was organized by the Association for Computational Linguistics."
    },
    {
      "type": "BUILDS_ON",
      "source": "LoRA",
      "target": "GPT",
      "description": "LoRA builds on the principles of GPT for adaptation."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Roberta",
      "description": "LoRA improves upon Roberta by providing a low-rank adaptation method."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Decoupled weight decay regularization",
      "description": "LoRA uses decoupled weight decay regularization to enhance performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Compacter",
      "description": "LoRA compares its efficiency with Compacter in low-rank adaptation."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "DART",
      "description": "LoRA introduces new methods that can be applied to DART for text generation."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation builds on the principles of large language models to enhance their performance."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "Adapter-fusion",
      "description": "Low-Rank Adaptation uses the principles of adapter-fusion for task composition."
    },
    {
      "type": "COMPARES_TO",
      "source": "Low-Rank Adaptation",
      "target": "Semi-orthogonal low-rank matrix factorization",
      "description": "Low-Rank Adaptation compares its effectiveness to semi-orthogonal low-rank matrix factorization techniques."
    },
    {
      "type": "INTRODUCES",
      "source": "Low-Rank Adaptation",
      "target": "Generalization guarantees",
      "description": "Low-Rank Adaptation introduces new generalization guarantees for neural networks."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation enhances the performance of large language models."
    },
    {
      "type": "USES",
      "source": "Large Language Models",
      "target": "SQuAD",
      "description": "Large language models are evaluated using the SQuAD dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Low-Rank Adaptation",
      "target": "Transformer",
      "description": "Low-Rank Adaptation techniques are compared to the Transformer architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "Low-Rank Adaptation",
      "target": "Low-Rank Matrix Factorization",
      "description": "Low-Rank Adaptation builds on the principles of low-rank matrix factorization."
    },
    {
      "type": "USES",
      "source": "Megatron-LM",
      "target": "Model Parallelism",
      "description": "Megatron-LM uses model parallelism to train large language models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adapters",
      "target": "Adapterdrop",
      "description": "Adapters are compared to Adapterdrop in terms of efficiency."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "This paper introduces the technique of Low-Rank Adaptation for large language models."
    },
    {
      "type": "PROPOSES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Large Language Models",
      "description": "The paper proposes a method to adapt large language models effectively."
    },
    {
      "type": "BUILDS_ON",
      "source": "LoRA",
      "target": "Transformer",
      "description": "LoRA is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "GLUE",
      "description": "LoRA utilizes the GLUE benchmark for evaluation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "SuperGLUE",
      "description": "LoRA utilizes the SuperGLUE benchmark for evaluation."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Neural Network Acceptability Judgments",
      "description": "LoRA is compared against models evaluated on the Neural Network Acceptability Judgments dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Broad-Coverage Challenge Corpus",
      "description": "LoRA is compared against models evaluated on the Broad-Coverage Challenge Corpus."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Low-Rank Adaptation",
      "description": "This paper introduces the Low-Rank Adaptation technique for large language models."
    },
    {
      "type": "USES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Large Language Models",
      "description": "The paper discusses the adaptation of large language models using the proposed technique."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Transformers",
      "description": "The paper compares the performance of the proposed technique with existing transformer models."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA: Low-Rank Adaptation of Large Language Models",
      "target": "Broad-Coverage Challenge Corpus",
      "description": "The paper evaluates the effectiveness of the proposed technique using the broad-coverage challenge corpus."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "Transformer",
      "description": "LoRA improves the adaptation of transformers by providing a low-rank approach."
    },
    {
      "type": "BUILDS_ON",
      "source": "BitFit",
      "target": "Transformer",
      "description": "BitFit builds on the transformer architecture for fine-tuning."
    },
    {
      "type": "USES",
      "source": "Low-Rank Matrix Factorization",
      "target": "Deep Neural Networks",
      "description": "Low-Rank Matrix Factorization is used to extract features from deep neural networks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Infinite-Width Neural Networks",
      "target": "Feature Learning",
      "description": "Infinite-Width Neural Networks are compared in the context of feature learning."
    },
    {
      "type": "IMPROVES",
      "source": "Fine-tuning",
      "target": "Large Language Models",
      "description": "Fine-tuning enhances the performance of large language models."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "Low-Rank Matrix Factorization",
      "description": "Low-Rank Adaptation employs low-rank matrix factorization techniques."
    },
    {
      "type": "COMPARES_TO",
      "source": "Few-shot Learning",
      "target": "Fine-tuning",
      "description": "Few-shot learning is compared to fine-tuning in terms of training sample requirements."
    },
    {
      "type": "IMPROVES",
      "source": "Fine-tuning",
      "target": "Model performance",
      "description": "Fine-tuning improves the model performance drastically compared to few-shot learning."
    },
    {
      "type": "USES",
      "source": "GPT-3",
      "target": "RTE",
      "description": "GPT-3's few-shot result is used for comparison on the RTE dataset."
    },
    {
      "type": "USES",
      "source": "GPT-3",
      "target": "MNLI",
      "description": "Two demonstrations per class and six in-context examples are used from the MNLI dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Fine-tuning",
      "target": "Few-shot learning",
      "description": "Fine-tuning is compared to few-shot learning in terms of model performance."
    },
    {
      "type": "IMPROVES",
      "source": "Fine-Tuning",
      "target": "Few-Shot Learning",
      "description": "Fine-tuning significantly outperforms few-shot learning on GPT-3."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Adapter Layers",
      "description": "LoRA proposes a method of adding modules in a parallel manner, unlike traditional adapter layers."
    },
    {
      "type": "USES",
      "source": "Adapter Layers",
      "target": "GPT-3",
      "description": "Adapter layers are used to adapt GPT-3 for specific tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Adapter Layers",
      "description": "LoRA is compared to adapter layers in terms of latency introduced."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "GPT-2",
      "description": "LoRA evaluates latency on GPT-2 medium."
    },
    {
      "type": "IMPROVES",
      "source": "Adapter L",
      "target": "Adapter H",
      "description": "Adapter L is a more efficient variant that improves upon the original Adapter H design."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Adapter H",
      "description": "LoRA utilizes Adapter H as one of its designs."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Adapter L",
      "description": "LoRA utilizes Adapter L as a more efficient design."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adapter H",
      "target": "no-adapter baseline",
      "description": "Adapter H's performance is compared to the no-adapter baseline."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adapter L",
      "target": "no-adapter baseline",
      "description": "Adapter L's performance is compared to the no-adapter baseline."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "GLUE Benchmark",
      "description": "LoRA is evaluated using the GLUE Benchmark for natural language understanding tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adapter H",
      "target": "no-adapter",
      "description": "Adapter H's performance is compared to the no-adapter baseline."
    },
    {
      "type": "COMPARES_TO",
      "source": "Adapter L",
      "target": "no-adapter",
      "description": "Adapter L's performance is compared to the no-adapter baseline."
    },
    {
      "type": "INTRODUCES",
      "source": "WikiSQL",
      "target": "Zhong et al. (2017)",
      "description": "WikiSQL is introduced in the work by Zhong et al. in 2017."
    },
    {
      "type": "INTRODUCES",
      "source": "SAMSum",
      "target": "Gliwa et al. (2019)",
      "description": "SAMSum is introduced in the work by Gliwa et al. in 2019."
    },
    {
      "type": "USES",
      "source": "GLUE",
      "target": "RoBERTa",
      "description": "GLUE benchmark is used to evaluate the RoBERTa model."
    },
    {
      "type": "USES",
      "source": "GLUE",
      "target": "DeBERTa",
      "description": "GLUE benchmark is used to evaluate the DeBERTa model."
    },
    {
      "type": "COMPARES_TO",
      "source": "DART",
      "target": "E2E",
      "description": "DART is a significantly larger and more complex data-to-text task compared to E2E."
    },
    {
      "type": "USES",
      "source": "E2E NLG Challenge",
      "target": "E2E",
      "description": "The E2E NLG Challenge uses the E2E dataset for training."
    },
    {
      "type": "USES",
      "source": "DART",
      "target": "Web NLG",
      "description": "Both datasets are used for data-to-text evaluation."
    },
    {
      "type": "IMPROVES",
      "source": "Adam W",
      "target": "RoBERTa",
      "description": "Adam W is used to improve the training of the RoBERTa model."
    },
    {
      "type": "COMPARES_TO",
      "source": "Web NLG",
      "target": "DART",
      "description": "Web NLG is compared to DART in terms of complexity and size."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "RoBERTa",
      "description": "LoRA uses the RoBERTa model as a base for adaptation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "AdamW",
      "description": "LoRA employs the AdamW optimization algorithm during training."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Houlsby et al. (2019)",
      "description": "LoRA's setup is compared to the setup in Houlsby et al. (2019)."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Pfeiffer et al. (2021)",
      "description": "LoRA's setup is compared to the setup in Pfeiffer et al. (2021)."
    },
    {
      "type": "BUILDS_ON",
      "source": "LoRA",
      "target": "Liu et al. (2019)",
      "description": "LoRA builds on the initialization method proposed by Liu et al. (2019)."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "AdamW",
      "description": "LoRA uses the AdamW optimization algorithm for training."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "linear learning rate decay",
      "description": "LoRA employs a linear learning rate decay schedule during training."
    },
    {
      "type": "INITIALIZES",
      "source": "LoRA",
      "target": "MNLI",
      "description": "LoRA modules are initialized to the best MNLI checkpoint when adapting to other tasks."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "MRPC",
      "description": "LoRA is evaluated on the MRPC dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "RTE",
      "description": "LoRA is evaluated on the RTE dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "STS-B",
      "description": "LoRA is evaluated on the STS-B dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "DEBERTA",
      "description": "LoRA's performance is compared to DEBERTA in the context of training."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "RoBERTa base",
      "description": "LoRA is applied to RoBERTa base for low-rank adaptation."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "RoBERTa large",
      "description": "LoRA is applied to RoBERTa large for low-rank adaptation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "AdamW",
      "description": "LoRA uses AdamW optimizer for training."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Learning Rate",
      "description": "LoRA employs specific learning rates for different configurations."
    },
    {
      "type": "BUILDS_ON",
      "source": "LoRA",
      "target": "Bottleneck",
      "description": "LoRA utilizes bottleneck architecture for efficient adaptation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "RoBERTa",
      "description": "LoRA is applied to the RoBERTa model for adaptation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "GPT-2",
      "description": "LoRA is applied to the GPT-2 model for adaptation."
    },
    {
      "type": "BUILDS_ON",
      "source": "RoBERTa",
      "target": "Transformer",
      "description": "RoBERTa is built on the Transformer architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "GPT-2",
      "target": "Transformer",
      "description": "GPT-2 is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "GPT-2",
      "target": "AdamW",
      "description": "GPT-2 uses the AdamW optimizer for training."
    },
    {
      "type": "USES",
      "source": "GPT-2",
      "target": "Adam W",
      "description": "GPT-2 models are trained using the Adam W optimization algorithm."
    },
    {
      "type": "USES",
      "source": "GPT-3",
      "target": "Adam W",
      "description": "GPT-3 experiments utilize the Adam W optimization algorithm for training."
    },
    {
      "type": "IMPROVES",
      "source": "GPT-3",
      "target": "GPT-2",
      "description": "GPT-3 improves upon the capabilities of GPT-2."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Hyperparameters",
      "description": "LoRA involves tuning hyperparameters such as batch size and learning rate."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "De BERTa XXL",
      "description": "LoRA is applied to the De BERTa XXL model for adaptation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "GPT-2",
      "description": "LoRA is applied to the GPT-2 model for adaptation."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Learning Rate",
      "description": "LoRA optimizes the learning rate for better performance."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Batch Size",
      "description": "LoRA adjusts the batch size for effective training."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Weight Decay",
      "description": "LoRA modifies weight decay to enhance model performance."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Dropout Probability",
      "description": "LoRA fine-tunes dropout probability to reduce overfitting."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "GPT-2",
      "description": "LoRA is applied to the GPT-2 model for adaptation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA is applied to the GPT-3 model for adaptation."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "E2E",
      "description": "LoRA is evaluated on the E2E dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "WebNLG",
      "description": "LoRA is evaluated on the WebNLG dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "DART",
      "description": "LoRA is evaluated on the DART dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "WikiSQL",
      "description": "LoRA is evaluated on the WikiSQL dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "MNLI",
      "description": "LoRA is evaluated on the MNLI dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "SAMSum",
      "description": "LoRA is evaluated on the SAMSum dataset."
    },
    {
      "type": "BUILDS_ON",
      "source": "LoRA",
      "target": "Prefix Tuning",
      "description": "LoRA can be combined with prefix tuning approaches."
    },
    {
      "type": "USES",
      "source": "LoRA+Prefix Embed",
      "target": "Prefix Embedding Tuning",
      "description": "LoRA+PE combines LoRA with prefix embedding tuning."
    },
    {
      "type": "USES",
      "source": "LoRA+Prefix Layer",
      "target": "Prefix Layer Tuning",
      "description": "LoRA+PL combines LoRA with prefix layer tuning."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA+Prefix Embed",
      "target": "Wiki SQL",
      "description": "Evaluates the combination of LoRA and prefix embedding tuning on Wiki SQL."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA+Prefix Layer",
      "target": "MNLI",
      "description": "Evaluates the combination of LoRA and prefix layer tuning on MNLI."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "LoRA+PE",
      "description": "LoRA+PE significantly outperforms LoRA on Wiki SQL."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA+PE",
      "target": "LoRA",
      "description": "LoRA+PE is compared to LoRA and pre\ufb01x-embedding tuning."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "pre\ufb01x-embedding tuning",
      "description": "LoRA is somewhat orthogonal to pre\ufb01x-embedding tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Prefix-Embedding Tuning (PE)",
      "description": "LoRA is somewhat orthogonal to prefix-embedding tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Human Baseline",
      "description": "LoRA achieves performance comparable to the human baseline."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA+PL",
      "target": "LoRA",
      "description": "LoRA+PL performs slightly worse than LoRA."
    },
    {
      "type": "USES",
      "source": "LoRA+PL",
      "target": "Learning Rate",
      "description": "LoRA+PL is sensitive to the choice of learning rate."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "DART",
      "description": "LoRA is evaluated on the DART dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "Web NLG",
      "description": "LoRA is evaluated on the Web NLG dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Prefix-based Approaches",
      "description": "LoRA performs better than or at least on-par with prefix-based approaches."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "GPT-2 Medium",
      "description": "LoRA improves the performance of GPT-2 Medium compared to prefix-based approaches."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "GPT-2 Large",
      "description": "LoRA improves the performance of GPT-2 Large compared to prefix-based approaches."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "prefix-based approaches",
      "description": "LoRA is compared to prefix-based approaches in terms of performance."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "BLEU",
      "description": "LoRA's performance is evaluated using the BLEU metric."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "MET",
      "description": "LoRA's performance is evaluated using the MET metric."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "TER",
      "description": "LoRA's performance is evaluated using the TER metric."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Fine-Tune",
      "description": "LoRA improves upon the traditional fine-tuning method by using low-rank adaptation."
    },
    {
      "type": "USES",
      "source": "GPT-2 Medium",
      "target": "Fine-Tune",
      "description": "GPT-2 Medium uses fine-tuning to adapt to specific tasks."
    },
    {
      "type": "USES",
      "source": "GPT-2 Large",
      "target": "Fine-Tune",
      "description": "GPT-2 Large uses fine-tuning to adapt to specific tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Adapter L",
      "description": "LoRA is compared to Adapter L in terms of performance on the Web NLG dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Pre\ufb01x",
      "description": "LoRA is compared to the Pre\ufb01x adaptation method on the Web NLG dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "FTTop 2",
      "description": "LoRA is compared to FTTop 2 in the context of adapting large language models."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Fine-Tune",
      "description": "LoRA is compared to Fine-Tune in terms of performance on MNLI-100."
    },
    {
      "type": "COMPARES_TO",
      "source": "Pre\ufb01x Embed",
      "target": "random chance",
      "description": "Pre\ufb01x Embed performs slightly better than random chance on MNLI-100."
    },
    {
      "type": "COMPARES_TO",
      "source": "Pre\ufb01x Layer",
      "target": "Pre\ufb01x Embed",
      "description": "Pre\ufb01x Layer performs better than Pre\ufb01x Embed on MNLI-100."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Pre\ufb01x Layer",
      "description": "LoRA performs better than Pre\ufb01x Layer on MNLI-100."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Fine-Tuning",
      "description": "LoRA achieves better performance than fine-tuning on MNLI-100 and MNLI-Full."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Pre\ufb01x Embed",
      "description": "LoRA is compared to Pre\ufb01x Embed in terms of performance on MNLI datasets."
    },
    {
      "type": "COMPARES_TO",
      "source": "Pre\ufb01x Embed",
      "target": "Fine-Tuning",
      "description": "Pre\ufb01x Embed is compared to Fine-Tuning and LoRA on MNLI-100."
    },
    {
      "type": "USES",
      "source": "Subspace Similarity",
      "target": "\u03c6(A, B, i, j)",
      "description": "Subspace similarity is measured using the function \u03c6(A, B, i, j)."
    },
    {
      "type": "COMPARES_TO",
      "source": "Subspace Similarity",
      "target": "Projection Metric",
      "description": "Subspace similarity is a reverse of the standard Projection Metric."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Large Language Models",
      "description": "LoRA introduces a method for adapting large language models using low-rank techniques."
    },
    {
      "type": "EXTENDS",
      "source": "LoRA",
      "target": "LoRA+PE",
      "description": "LoRA+PE extends LoRA by incorporating prefix embeddings."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "Wiki SQL",
      "description": "LoRA is evaluated using the Wiki SQL dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "MNLI-m",
      "description": "LoRA is evaluated using the MNLI-m dataset."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Prefix Embed",
      "description": "LoRA uses the Prefix Embed architecture for model adaptation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Prefix Layer",
      "description": "LoRA uses the Prefix Layer architecture for model adaptation."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Adapter H",
      "description": "LoRA uses the Adapter H architecture for model adaptation."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Validation Accuracy",
      "description": "LoRA improves validation accuracy compared to other tuning methods."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Prefix Embedding Tuning",
      "description": "LoRA is compared to Prefix Embedding Tuning in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Prefix Layer Tuning",
      "description": "LoRA is compared to Prefix Layer Tuning in terms of performance."
    },
    {
      "type": "USES",
      "source": "GPT-3",
      "target": "LoRA",
      "description": "GPT-3 utilizes LoRA for adaptation."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "MNLI",
      "description": "LoRA is evaluated on the MNLI dataset."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA improves the sample-efficiency of GPT-3 compared to other methods."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "MNLI",
      "description": "LoRA is evaluated on the MNLI dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "fine-tuning",
      "description": "LoRA is compared to fine-tuning in terms of sample-efficiency."
    },
    {
      "type": "INTRODUCES",
      "source": "Projection Metric",
      "target": "LoRA",
      "description": "The Projection Metric is introduced as a way to evaluate the performance of LoRA."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA is introduced as a method for adapting GPT-3."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "MNLI",
      "description": "LoRA is evaluated using the MNLI dataset."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Learning Rate",
      "description": "LoRA utilizes learning rate as a hyperparameter."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Batch Size",
      "description": "LoRA utilizes batch size as a hyperparameter."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "Epoch",
      "description": "LoRA utilizes the number of epochs as a hyperparameter."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "Other Adaptation Methods",
      "description": "LoRA is compared to other adaptation methods in the context of low-rank updates."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "GPT-2",
      "description": "Low-Rank Adaptation improves the performance of GPT-2 by optimizing the rank during training."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "GPT-3",
      "description": "Low-Rank Adaptation improves the performance of GPT-3 by optimizing the rank during training."
    },
    {
      "type": "USES",
      "source": "GPT-2",
      "target": "E2E NLG Challenge",
      "description": "GPT-2 uses the E2E NLG Challenge dataset for training and evaluation."
    },
    {
      "type": "COMPARES_TO",
      "source": "Optimal Rank",
      "target": "GPT-2",
      "description": "The optimal rank for adaptation is compared between GPT-2 and GPT-3."
    },
    {
      "type": "COMPARES_TO",
      "source": "Optimal Rank",
      "target": "GPT-3",
      "description": "The optimal rank for adaptation is compared between GPT-2 and GPT-3."
    },
    {
      "type": "CORRELATES_WITH",
      "source": "W",
      "target": "\u2206W",
      "description": "The correlation between W and \u2206W is analyzed with varying rank."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation enhances the performance of large language models by focusing on task-specific directions."
    },
    {
      "type": "COMPARES_TO",
      "source": "\u2206W",
      "target": "W",
      "description": "The relationship between \u2206W and W is analyzed in terms of their subspace similarity."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Low-Rank Adaptation (LoRA)",
      "description": "The paper introduces the concept of Low-Rank Adaptation for large language models."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "SVD Decomposition",
      "description": "LoRA uses SVD decomposition to analyze weight updates in the model."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "Amplification Factor",
      "description": "The paper evaluates the amplification factor to measure the effectiveness of task-specific directions."
    },
    {
      "type": "COMPARES_TO",
      "source": "Amplification Factor",
      "target": "96-layer Transformer",
      "description": "The amplification factor is analyzed across different layers of the 96-layer Transformer."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation",
      "target": "Large Language Models",
      "description": "Low-Rank Adaptation improves the performance of Large Language Models on specific tasks."
    },
    {
      "type": "USES",
      "source": "Low-Rank Adaptation",
      "target": "\u2206W",
      "description": "Low-Rank Adaptation uses the change in model parameters (\u2206W) to adapt the model."
    },
    {
      "type": "COMPARES_TO",
      "source": "r",
      "target": "Amplification Factor",
      "description": "The rank (r) is compared to the amplification factor to assess the effectiveness of the adaptation."
    },
    {
      "type": "INTRODUCES",
      "source": "LoRA",
      "target": "Transformer",
      "description": "LoRA introduces a low-rank adaptation method for improving the performance of Transformer models."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "BLEU",
      "description": "LoRA evaluates its performance using the BLEU metric among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "NIST",
      "description": "LoRA evaluates its performance using the NIST metric among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "METEOR",
      "description": "LoRA evaluates its performance using the METEOR metric among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "ROUGE L",
      "description": "LoRA evaluates its performance using the ROUGE L metric among others."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "CIDEr",
      "description": "LoRA evaluates its performance using the CIDEr metric among others."
    },
    {
      "type": "USES",
      "source": "LoRA",
      "target": "GPT-2 Medium",
      "description": "LoRA uses GPT-2 Medium for adaptation tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LoRA",
      "target": "GPT-3",
      "description": "LoRA's performance is compared to that of GPT-3."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "Validation Loss",
      "description": "LoRA improves validation loss metrics when adapting models."
    },
    {
      "type": "IMPROVES",
      "source": "LoRA",
      "target": "BLEU",
      "description": "LoRA improves BLEU scores for generated text."
    },
    {
      "type": "EVALUATES",
      "source": "LoRA",
      "target": "E2E NLG Challenge",
      "description": "LoRA is evaluated on the E2E NLG Challenge dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Low-Rank Adaptation (LoRA)",
      "target": "Large Language Models",
      "description": "LoRA improves the adaptation process of large language models by applying low-rank updates."
    },
    {
      "type": "COMPARES_TO",
      "source": "Wq",
      "target": "\u2206Wq",
      "description": "Wq is compared to \u2206Wq to analyze the effects of the adaptation."
    },
    {
      "type": "USES",
      "source": "Normalized Subspace Similarity",
      "target": "Wq",
      "description": "Normalized subspace similarity is used to evaluate the singular directions of Wq."
    },
    {
      "type": "USES",
      "source": "Normalized Subspace Similarity",
      "target": "\u2206Wq",
      "description": "Normalized subspace similarity is used to evaluate the singular directions of \u2206Wq."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Self-Attention",
      "description": "Flash Attention improves the efficiency of self-attention by being IO-aware."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "IO-Aware Attention",
      "description": "Flash Attention uses the principle of IO-awareness to enhance attention computation."
    },
    {
      "type": "COMPARES_TO",
      "source": "Approximate Attention",
      "target": "Flash Attention",
      "description": "Approximate Attention methods are compared to Flash Attention in terms of speed and quality."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "BERT-large",
      "description": "Flash Attention trains BERT-large 15% faster than existing baselines."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "GPT-2",
      "description": "Flash Attention achieves a 3x speedup on GPT-2."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "long-range arena",
      "description": "Flash Attention provides a 2.4x speedup on long-range arena tasks."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "block-sparse attention",
      "description": "Flash Attention is extended to block-sparse attention for faster approximate attention."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Transformer",
      "description": "Flash Attention enables longer context in Transformers, yielding higher quality models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "GPT-2",
      "description": "Flash Attention achieves a 0.7 better perplexity on GPT-2."
    },
    {
      "type": "INTRODUCES",
      "source": "Flash Attention",
      "target": "Path-X",
      "description": "Flash Attention allows Transformers to achieve better-than-chance performance on the Path-X challenge."
    },
    {
      "type": "INTRODUCES",
      "source": "Flash Attention",
      "target": "Path-256",
      "description": "Flash Attention enables Transformers to achieve a 63.1% accuracy on the Path-256 challenge."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention",
      "description": "FlashAttention aims to enhance the efficiency of the attention mechanism."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "IO-Awareness",
      "description": "FlashAttention incorporates IO-awareness to optimize memory access."
    },
    {
      "type": "COMPARES_TO",
      "source": "Approximate Attention Methods",
      "target": "Standard Attention",
      "description": "Approximate attention methods are compared to standard attention in terms of performance."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "IO-Awareness",
      "description": "FlashAttention enhances the efficiency of attention mechanisms by incorporating IO-awareness."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "GPU",
      "description": "FlashAttention is designed to work efficiently with GPU architectures."
    },
    {
      "type": "COMPARES_TO",
      "source": "SRAM",
      "target": "HBM",
      "description": "SRAM is compared to HBM in terms of speed and efficiency in memory operations."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Attention on GPT-2",
      "description": "Flash Attention enhances the attention mechanism used in GPT-2 by optimizing memory and computation efficiency."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "SRAM",
      "description": "Flash Attention utilizes SRAM for fast data access during attention computation."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "HBM",
      "description": "Flash Attention writes output back to HBM after computation."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "PyTorch implementation of attention",
      "description": "Flash Attention is compared to the standard PyTorch implementation of attention in terms of speed."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Attention Computation",
      "description": "Flash Attention improves the efficiency of attention computation by reducing memory accesses."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "PyTorch Implementation of Attention",
      "description": "Flash Attention is compared to the PyTorch implementation of attention, showing a significant speedup."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "HBM",
      "description": "Flash Attention avoids reading and writing the large attention matrix to and from HBM."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "IO-aware algorithms",
      "description": "Flash Attention extends the concept of IO-aware algorithms to optimize attention mechanisms."
    },
    {
      "type": "INTRODUCES",
      "source": "Flash Attention",
      "target": "Softmax Reduction",
      "description": "Flash Attention introduces a method to compute softmax reduction without accessing the whole input."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Intermediate Attention Matrix",
      "description": "Flash Attention avoids storing the large intermediate attention matrix for the backward pass."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "HBM",
      "description": "Flash Attention improves memory access efficiency by reducing the need to read and write to HBM."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "CUDA",
      "description": "Flash Attention is implemented in CUDA to achieve fine-grained control over memory access."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "Flash Attention runs faster and uses less memory compared to standard attention."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "CUDA",
      "description": "Flash Attention is implemented in CUDA for fine-grained control over memory access."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "Flash Attention requires fewer HBM accesses compared to standard attention."
    },
    {
      "type": "ANALYZES",
      "source": "Flash Attention",
      "target": "IO complexity",
      "description": "The paper analyzes the IO complexity of Flash Attention."
    },
    {
      "type": "IMPROVES",
      "source": "Block-Sparse Flash Attention",
      "target": "Flash Attention",
      "description": "Block-sparse Flash Attention is faster and has better IO complexity compared to Flash Attention."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Approximate Attention Algorithms",
      "description": "Flash Attention serves as a primitive to realize the potential of approximate attention algorithms."
    },
    {
      "type": "EXTENDS",
      "source": "Block-Sparse Flash Attention",
      "target": "Multi-GPU",
      "description": "Block-sparse Flash Attention can be extended to operate on multi-GPU architectures."
    },
    {
      "type": "EXTENDS",
      "source": "Block-Sparse Flash Attention",
      "target": "Kernel Regression",
      "description": "Block-sparse Flash Attention can be extended to support kernel regression operations."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Model Quality",
      "description": "Flash Attention improves model quality by enabling modeling of longer context."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Prior Attention Implementations",
      "description": "Flash Attention is benchmarked against prior attention implementations in terms of runtime and memory footprint."
    },
    {
      "type": "BUILDS_ON",
      "source": "Flash Attention",
      "target": "Transformer",
      "description": "Flash Attention is designed to enhance the training speed of Transformer models."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "BERT-large",
      "description": "Flash Attention trains BERT-large 15% faster than the training speed record in MLPerf 1.1."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "GPT-2",
      "description": "Flash Attention shows a 0.7 improvement in perplexity on GPT-2."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Long-Range Arena",
      "description": "Flash Attention trains models on long-range tasks 2.4 times faster than baselines."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "GPT-2",
      "description": "Flash Attention improves the perplexity of GPT-2 by 0.7."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Path-X",
      "description": "Flash Attention enables a Transformer to achieve better-than-chance performance on the Path-X challenge."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "Block-sparse Flash Attention",
      "description": "Block-sparse Flash Attention is an extension of Flash Attention that allows for longer sequences."
    },
    {
      "type": "IMPROVES",
      "source": "Block-sparse Flash Attention",
      "target": "Path-256",
      "description": "Block-sparse Flash Attention enables a model to achieve better-than-chance performance on Path-256."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Approximate Attention",
      "description": "Flash Attention is faster and more memory-efficient than existing approximate attention methods."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer, which becomes faster for longer sequences."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "GPU",
      "description": "Flash Attention utilizes GPU hardware to achieve its performance benefits."
    },
    {
      "type": "BUILDS_ON",
      "source": "Flash Attention",
      "target": "GPU Memory Hierarchy",
      "description": "Flash Attention's performance is influenced by the GPU memory hierarchy."
    },
    {
      "type": "USES",
      "source": "A100 GPU",
      "target": "GPU Memory Hierarchy",
      "description": "The A100 GPU's performance is based on the principles of GPU memory hierarchy."
    },
    {
      "type": "USES",
      "source": "A100 GPU",
      "target": "High Bandwidth Memory (HBM)",
      "description": "The A100 GPU utilizes high bandwidth memory for data processing."
    },
    {
      "type": "USES",
      "source": "A100 GPU",
      "target": "On-chip SRAM",
      "description": "The A100 GPU uses on-chip SRAM for faster data access during computations."
    },
    {
      "type": "COMPARES_TO",
      "source": "On-chip SRAM",
      "target": "High Bandwidth Memory (HBM)",
      "description": "On-chip SRAM is faster but smaller compared to HBM."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Memory-bound operations",
      "description": "FlashAttention improves the efficiency of memory-bound operations."
    },
    {
      "type": "COMPARES_TO",
      "source": "Compute-bound",
      "target": "Memory-bound",
      "description": "Compute-bound operations are contrasted with memory-bound operations based on their performance characteristics."
    },
    {
      "type": "IMPROVES",
      "source": "Kernel fusion",
      "target": "Memory-bound",
      "description": "Kernel fusion is used to improve the performance of memory-bound operations."
    },
    {
      "type": "IMPROVES",
      "source": "Kernel Fusion",
      "target": "Memory-Bound Operations",
      "description": "Kernel fusion improves the efficiency of memory-bound operations by reducing the number of memory accesses."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Kernel Fusion",
      "description": "FlashAttention utilizes kernel fusion to enhance performance."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Standard Attention Implementation",
      "description": "FlashAttention enhances the efficiency of standard attention implementations by reducing memory usage and improving speed."
    },
    {
      "type": "USES",
      "source": "Standard Attention Implementation",
      "target": "Attention Mechanism",
      "description": "Standard attention implementation utilizes the attention mechanism to compute outputs."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Standard Attention Implementation",
      "description": "FlashAttention is compared to standard attention implementations in terms of performance and memory efficiency."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Standard Attention Implementation",
      "description": "Flash Attention provides a more efficient approach to computing attention compared to the standard implementation."
    },
    {
      "type": "USES",
      "source": "Standard Attention Implementation",
      "target": "HBM",
      "description": "The standard attention implementation relies on High Bandwidth Memory for data access."
    },
    {
      "type": "USES",
      "source": "Standard Attention Implementation",
      "target": "Masking",
      "description": "Masking is applied in the standard attention implementation to control attention scores."
    },
    {
      "type": "USES",
      "source": "Standard Attention Implementation",
      "target": "Dropout",
      "description": "Dropout is applied in the standard attention implementation to enhance generalization."
    },
    {
      "type": "USES",
      "source": "Standard Attention Implementation",
      "target": "Softmax",
      "description": "Softmax is used in the standard attention implementation to normalize attention scores."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "Flash Attention improves upon standard attention by requiring fewer HBM accesses."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "Block-Sparse Attention",
      "description": "Flash Attention can be extended to handle block-sparse attention."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Tiling",
      "description": "FlashAttention uses tiling to compute attention by blocks."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Recomputation",
      "description": "FlashAttention employs recomputation to manage memory access efficiently."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Sub-quadratic HBM accesses",
      "description": "FlashAttention aims to improve performance by achieving sub-quadratic HBM accesses."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Attention Output",
      "description": "FlashAttention introduces a method to compute the attention output efficiently."
    },
    {
      "type": "COMPARES_TO",
      "source": "Softmax",
      "target": "Tiling",
      "description": "Softmax is compared to tiling in terms of numerical stability during attention computation."
    },
    {
      "type": "USES",
      "source": "softmax with scaling",
      "target": "numerical stability",
      "description": "The softmax with scaling technique is used to ensure numerical stability in computations."
    },
    {
      "type": "USES",
      "source": "softmax",
      "target": "exponential function",
      "description": "The softmax function utilizes the exponential function to compute probabilities."
    },
    {
      "type": "BUILDS_ON",
      "source": "softmax with scaling",
      "target": "max",
      "description": "The softmax with scaling technique builds on the max function to enhance numerical stability."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention enhances the efficiency and memory usage of traditional attention mechanisms."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Softmax",
      "description": "FlashAttention employs the softmax function to compute attention scores."
    },
    {
      "type": "EXTENDS",
      "source": "Recomputation",
      "target": "Attention Mechanism",
      "description": "Recomputation extends the capabilities of attention mechanisms by reducing memory usage during training."
    },
    {
      "type": "USES",
      "source": "Attention Mechanism",
      "target": "Q-K-V",
      "description": "The attention mechanism utilizes the Query-Key-Value architecture for processing inputs."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Selective Gradient Checkpointing",
      "description": "FlashAttention improves upon selective gradient checkpointing by speeding up the backward pass despite increased FLOPs."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Kernel Fusion",
      "description": "FlashAttention utilizes kernel fusion to implement its algorithm efficiently in a single CUDA kernel."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "HBM",
      "description": "FlashAttention uses HBM for loading inputs and writing results to optimize memory access."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "HBM",
      "description": "Flash Attention utilizes HBM for storing input matrices."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "on-chip SRAM",
      "description": "Flash Attention uses on-chip SRAM for temporary storage during computations."
    },
    {
      "type": "BUILDS_ON",
      "source": "Flash Attention",
      "target": "Q-K-V",
      "description": "Flash Attention builds on the Q-K-V model for attention mechanisms."
    },
    {
      "type": "COMPUTES",
      "source": "Flash Attention",
      "target": "S",
      "description": "Flash Attention computes the matrix S as part of the attention mechanism."
    },
    {
      "type": "COMPUTES",
      "source": "Flash Attention",
      "target": "m",
      "description": "Flash Attention computes the metric m during the attention process."
    },
    {
      "type": "COMPUTES",
      "source": "Flash Attention",
      "target": "\u2113",
      "description": "Flash Attention computes the metric \u2113 during the attention process."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Standard Attention",
      "description": "FlashAttention shows significant reduction in HBM accesses compared to standard attention."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Softmax",
      "description": "FlashAttention uses the softmax function to compute the output."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "IO Complexity",
      "description": "FlashAttention extends the analysis of IO complexity in attention mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "Flash Attention is compared to Standard Attention in terms of runtime and memory efficiency."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "HBM Accesses",
      "description": "Flash Attention asymptotically improves HBM accesses over all SRAM sizes."
    },
    {
      "type": "USES",
      "source": "GPT-2",
      "target": "Flash Attention",
      "description": "GPT-2 utilizes Flash Attention for improved performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Block-Sparse Flash Attention",
      "target": "Flash Attention",
      "description": "Block-Sparse Flash Attention is faster than Flash Attention based on sparsity."
    },
    {
      "type": "IMPROVES",
      "source": "block-sparse Flash Attention",
      "target": "Flash Attention",
      "description": "Block-sparse Flash Attention is faster than Flash Attention by a factor proportional to the sparsity."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "HBM accesses",
      "description": "Flash Attention requires fewer HBM accesses compared to standard attention implementations."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "standard attention",
      "description": "Flash Attention is compared to standard attention in terms of HBM accesses and execution speed."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "SRAM",
      "description": "Flash Attention utilizes SRAM to load blocks of K and V for computation."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Exact Attention",
      "description": "Flash Attention reduces the number of HBM accesses compared to standard attention methods."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "Flash Attention is compared to standard attention in terms of HBM access efficiency."
    },
    {
      "type": "BUILDS_ON",
      "source": "Lower-bound",
      "target": "Exact Attention",
      "description": "The lower-bound concept is used to establish limits on HBM accesses for exact attention."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "standard attention",
      "description": "FlashAttention has fewer HBM accesses, resulting in faster runtime compared to standard attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "standard attention",
      "description": "FlashAttention is compared to standard attention in terms of FLOP count and HBM accesses."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "block size",
      "description": "The performance of FlashAttention varies with different block sizes."
    },
    {
      "type": "EVALUATES",
      "source": "FlashAttention",
      "target": "HBM accesses",
      "description": "The paper evaluates how HBM accesses impact the runtime of FlashAttention."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "Block-Sparse Flash Attention",
      "description": "Block-Sparse Flash Attention is an extension of Flash Attention that reduces IO complexity."
    },
    {
      "type": "USES",
      "source": "Block-Sparse Flash Attention",
      "target": "Mask matrix (~M)",
      "description": "Block-Sparse Flash Attention uses a mask matrix to determine which elements to include in the attention computation."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Q, K, V",
      "description": "Flash Attention uses the input matrices Q, K, and V to compute attention scores."
    },
    {
      "type": "COMPUTES",
      "source": "Flash Attention",
      "target": "S",
      "description": "Flash Attention computes the score matrix S from the dot product of Q and K."
    },
    {
      "type": "COMPUTES",
      "source": "Flash Attention",
      "target": "P",
      "description": "Flash Attention computes the softmax-normalized score matrix P from S."
    },
    {
      "type": "COMPUTES",
      "source": "Flash Attention",
      "target": "O",
      "description": "Flash Attention computes the output matrix O from the product of P and V."
    },
    {
      "type": "IMPROVES",
      "source": "Block-sparse Flash Attention",
      "target": "IO complexity",
      "description": "Applying block-sparsity yields a direct improvement to the larger term in the IO complexity."
    },
    {
      "type": "USES",
      "source": "Block-sparse Flash Attention",
      "target": "Block sparsity mask",
      "description": "The algorithm utilizes a block sparsity mask to determine which blocks of the attention matrix to compute."
    },
    {
      "type": "COMPARES_TO",
      "source": "sparsity",
      "target": "\u0398(\ud835\udc41\ud835\udc51)",
      "description": "The relationship between the fraction of nonzero blocks and the complexity notation."
    },
    {
      "type": "EVALUATES",
      "source": "Block-sparse Flash Attention",
      "target": "Butterfly sparsity",
      "description": "The algorithm is evaluated using fixed butterfly sparsity in downstream experiments."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Training Speed",
      "description": "Flash Attention improves the training speed of transformer models."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Attention Runtime",
      "description": "Flash Attention improves the runtime of attention mechanisms."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Memory Benchmarks",
      "description": "Flash Attention enhances memory efficiency during model training."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "BERT",
      "description": "Flash Attention outperforms the MLPerf 1.1 speed record for BERT."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "GPT-2",
      "description": "Flash Attention speeds up GPT-2 compared to standard implementations."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention achieves speed improvements over Megatron."
    },
    {
      "type": "EVALUATES",
      "source": "LRA benchmark",
      "target": "Flash Attention",
      "description": "The performance of Flash Attention is evaluated on the LRA benchmark."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "GPT-2",
      "description": "Flash Attention speeds up GPT-2 training significantly compared to Megatron."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Transformer",
      "description": "Flash Attention scales Transformers to longer sequences, yielding higher quality."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron in terms of training speed and perplexity."
    },
    {
      "type": "EVALUATES",
      "source": "Flash Attention",
      "target": "Long-Range Arena (LRA)",
      "description": "Flash Attention is evaluated on the LRA benchmark."
    },
    {
      "type": "EVALUATES",
      "source": "Flash Attention",
      "target": "Path-X",
      "description": "Flash Attention achieves better-than-random performance on the Path-X task."
    },
    {
      "type": "EVALUATES",
      "source": "Flash Attention",
      "target": "Path-256",
      "description": "Block-sparse Flash Attention achieves better-than-random performance on the Path-256 task."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "BERT",
      "description": "Flash Attention yields the fastest single-node BERT training speed."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "Flash Attention is up to 3 times faster than standard attention for common sequence lengths."
    },
    {
      "type": "COMPARES_TO",
      "source": "Block-sparse Flash Attention",
      "target": "Existing Approximate Attention Baselines",
      "description": "Block-sparse Flash Attention is faster than all existing approximate attention baselines."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Flash Attention",
      "description": "BERT utilizes Flash Attention for improved training speed."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "BERT-large",
      "description": "Flash Attention yields faster training times for BERT-large compared to Nvidia MLPerf 1.1."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "GPT-2",
      "description": "Flash Attention yields faster training times for GPT-2 on the Open Webtext dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Hugging Face",
      "description": "Flash Attention shows up to 3x end-to-end speedup compared to Hugging Face."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron-LM",
      "description": "Flash Attention shows 1.7x speedup compared to Megatron-LM."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "GPT-2 small",
      "description": "Flash Attention improves the training speed of GPT-2 small compared to other implementations."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "GPT-2 medium",
      "description": "Flash Attention improves the training speed of GPT-2 medium compared to other implementations."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Huggingface",
      "description": "Flash Attention is compared to Huggingface implementation in terms of training speed."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron-LM",
      "description": "Flash Attention is compared to Megatron-LM in terms of training speed."
    },
    {
      "type": "USES",
      "source": "GPT-2 small",
      "target": "Open Web Text",
      "description": "GPT-2 small uses Open Web Text dataset for training."
    },
    {
      "type": "USES",
      "source": "GPT-2 medium",
      "target": "Open Web Text",
      "description": "GPT-2 medium uses Open Web Text dataset for training."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "standard attention",
      "description": "Flash Attention is compared to standard attention in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Block-sparse Flash Attention",
      "target": "approximate attention methods",
      "description": "Block-sparse Flash Attention is faster than all tested approximate attention methods."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "attention mechanisms",
      "description": "The Transformer architecture utilizes attention mechanisms for its operations."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Transformer",
      "description": "Flash Attention improves the speed and memory efficiency of the Transformer model."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block-sparse Flash Attention",
      "description": "Flash Attention is compared to Block-sparse Flash Attention in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer regarding efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linear Attention",
      "description": "Flash Attention is compared to Linear Attention in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Performer",
      "description": "Flash Attention is compared to Performer regarding efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer regarding efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of performance."
    },
    {
      "type": "EXTENDS",
      "source": "GPT-2",
      "target": "Flash Attention",
      "description": "Flash Attention extends the capabilities of GPT-2 by allowing longer context lengths."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-2",
      "target": "Megatron-LM",
      "description": "GPT-2 is compared to Megatron-LM in terms of speed and perplexity."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "MIMIC-III",
      "description": "Using Flash Attention improves performance on the MIMIC-III dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "ECt HR",
      "description": "Using Flash Attention improves performance on the ECt HR dataset."
    },
    {
      "type": "USES",
      "source": "GPT-2 small - Flash Attention",
      "target": "Flash Attention",
      "description": "The GPT-2 small model utilizes Flash Attention for improved performance."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "LRA accuracy",
      "description": "FlashAttention improves the accuracy results by optimizing the attention mechanism."
    },
    {
      "type": "USES",
      "source": "unit patient discharge summaries",
      "target": "LRA accuracy",
      "description": "The unit patient discharge summaries are used to evaluate LRA accuracy."
    },
    {
      "type": "COMPARES_TO",
      "source": "reproduced baselines",
      "target": "original comparison",
      "description": "The reproduced baselines perform better than those reported in the original comparison."
    },
    {
      "type": "IMPROVES",
      "source": "RoBERTa",
      "target": "Transformer",
      "description": "RoBERTa improves upon the original Transformer architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Block-Sparse Flash Attention",
      "description": "FlashAttention is compared to Block-Sparse Flash Attention in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "PyTorch Attention",
      "description": "FlashAttention is compared to the standard PyTorch Attention mechanism."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Megatron Attention",
      "description": "FlashAttention is compared to Megatron Attention in terms of efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Linformer Attention",
      "description": "FlashAttention is compared to Linformer Attention regarding memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "OpenAI Sparse Attention",
      "description": "FlashAttention is compared to OpenAI Sparse Attention in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "RoBERTa",
      "description": "Flash Attention is evaluated against the performance of the RoBERTa model."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Transformer",
      "description": "Flash Attention improves the efficiency of the Transformer architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "MIMIC-III",
      "target": "ECt HR",
      "description": "The performance of models is compared between the MIMIC-III and ECt HR datasets."
    },
    {
      "type": "EVALUATES",
      "source": "Flash Attention",
      "target": "micro F1",
      "description": "Flash Attention is evaluated using the micro F1 metric on various sequence lengths."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of performance on specific tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Performer",
      "description": "Flash Attention is compared to Performer regarding efficiency and performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of model performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer to assess improvements in attention mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "SMYRF",
      "description": "Flash Attention is compared to SMYRF in terms of memory efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block-sparse Flash Attention",
      "description": "Flash Attention is compared to Block-sparse Flash Attention in terms of performance on benchmarks."
    },
    {
      "type": "USES",
      "source": "Path-X",
      "target": "Transformer",
      "description": "Path-X uses the Transformer model to classify paths in images."
    },
    {
      "type": "USES",
      "source": "Path-256",
      "target": "Transformer",
      "description": "Path-256 uses the Transformer model to classify paths in larger images."
    },
    {
      "type": "INTRODUCES",
      "source": "Flash Attention",
      "target": "Path-X",
      "description": "Flash Attention introduces the ability to solve the Path-X benchmark."
    },
    {
      "type": "INTRODUCES",
      "source": "Flash Attention",
      "target": "Path-256",
      "description": "Flash Attention introduces the ability to solve the Path-256 benchmark."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Accuracy",
      "description": "Flash Attention achieves 61.4 accuracy on Path-X."
    },
    {
      "type": "IMPROVES",
      "source": "Block-sparse Flash Attention",
      "target": "Accuracy",
      "description": "Block-sparse Flash Attention achieves 63.1 accuracy on Path-256."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "A100 GPU",
      "description": "Flash Attention is benchmarked on an A100 GPU with 40 GB HBM."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Exact Attention",
      "description": "Flash Attention is compared against reference implementations for exact attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Approximate Attention",
      "description": "Flash Attention is compared against reference implementations for approximate attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Sparse Attention",
      "description": "Flash Attention is compared against reference implementations for sparse attention."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Exact Attention",
      "description": "Flash Attention runs significantly faster than exact attention baselines."
    },
    {
      "type": "IMPROVES",
      "source": "Block-sparse Flash Attention",
      "target": "Exact Attention",
      "description": "Block-sparse Flash Attention is faster than all implementations of exact attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Approximate Attention",
      "description": "Flash Attention runs faster than approximate attention for short sequences."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Sparse Attention",
      "description": "Flash Attention runs faster than sparse attention for short sequences."
    },
    {
      "type": "COMPARES_TO",
      "source": "Approximate Attention",
      "target": "Flash Attention",
      "description": "Approximate attention runtimes begin to cross over with Flash Attention at sequences between 512 and 1024."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Exact Attention",
      "description": "Flash Attention is up to 20 times more memory efficient than exact attention baselines."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Approximate Attention",
      "description": "Flash Attention is more memory-efficient than approximate attention baselines."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is still 2 times more efficient than Linformer before running out of memory."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block-Sparse Flash Attention",
      "description": "Both techniques have the same memory footprint."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention",
      "description": "FlashAttention aims to enhance the efficiency and memory usage of the attention mechanism."
    },
    {
      "type": "USES",
      "source": "IO-aware implementations",
      "target": "CUDA kernel",
      "description": "IO-aware implementations require the use of CUDA kernels for performance optimization."
    },
    {
      "type": "EXTENDS",
      "source": "IO-aware implementations",
      "target": "Deep Network",
      "description": "The IO-aware approach is believed to extend beyond just attention to other layers in deep networks."
    },
    {
      "type": "COMPARES_TO",
      "source": "IO-aware implementations",
      "target": "Halide",
      "description": "The proposed method is compared to Halide in terms of compiling high-level languages to optimized implementations."
    },
    {
      "type": "IMPROVES",
      "source": "IO-aware implementation of attention",
      "target": "Transformer",
      "description": "The IO-aware implementation enhances the efficiency of attention computation in Transformers."
    },
    {
      "type": "EXTENDS",
      "source": "IO-Aware Deep Learning",
      "target": "Multi-GPU IO-Aware Methods",
      "description": "The IO-aware approach can be applied to multi-GPU methods to improve performance."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "FMHA",
      "description": "The implementation of FlashAttention builds upon the FMHA code."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "FMHA",
      "description": "FlashAttention uses FMHA code as a starting point for its implementation."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "CUDA",
      "description": "FlashAttention utilizes CUDA for its implementation."
    },
    {
      "type": "SUPPORTS",
      "source": "NIH",
      "target": "FlashAttention",
      "description": "NIH provides funding support for the research related to FlashAttention."
    },
    {
      "type": "SUPPORTS",
      "source": "NSF",
      "target": "FlashAttention",
      "description": "NSF provides funding support for the research related to FlashAttention."
    },
    {
      "type": "SUPPORTS",
      "source": "ARL",
      "target": "FlashAttention",
      "description": "ARL provides funding support for the research related to FlashAttention."
    },
    {
      "type": "SUPPORTS",
      "source": "ONR",
      "target": "FlashAttention",
      "description": "ONR provides funding support for the research related to FlashAttention."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Exact Attention",
      "description": "FlashAttention utilizes exact attention mechanisms to enhance performance."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "IO-Aware Architecture",
      "description": "FlashAttention improves the efficiency of IO-aware architectures in attention computation."
    },
    {
      "type": "FUNDED_BY",
      "source": "FlashAttention",
      "target": "ONR",
      "description": "FlashAttention research is funded by the Office of Naval Research."
    },
    {
      "type": "FUNDED_BY",
      "source": "FlashAttention",
      "target": "Department of Defense (DoD)",
      "description": "FlashAttention research is supported by the Department of Defense through the NDSEG Program."
    },
    {
      "type": "COLLABORATES_WITH",
      "source": "Stanford DAWN project",
      "target": "Facebook",
      "description": "The Stanford DAWN project collaborates with Facebook for research initiatives."
    },
    {
      "type": "COLLABORATES_WITH",
      "source": "Stanford DAWN project",
      "target": "Google",
      "description": "The Stanford DAWN project collaborates with Google for research initiatives."
    },
    {
      "type": "COLLABORATES_WITH",
      "source": "Stanford DAWN project",
      "target": "VMWare",
      "description": "The Stanford DAWN project collaborates with VMWare for research initiatives."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention improves the efficiency of the attention mechanism in transformer models."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "IO-awareness",
      "description": "FlashAttention utilizes IO-awareness to enhance performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Lambda Networks",
      "description": "FlashAttention is compared to Lambda Networks in terms of handling long-range interactions."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Longformer",
      "description": "FlashAttention is compared to Longformer for its capability in processing long documents."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention enhances the efficiency and memory usage of traditional attention mechanisms."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention is applied within the Transformer architecture to optimize attention computation."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention enhances the traditional attention mechanism by making it faster and more memory-efficient."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Memory Efficiency",
      "description": "FlashAttention utilizes memory efficiency as a key performance metric."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Speed",
      "description": "FlashAttention aims to improve the speed of processing in attention mechanisms."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention introduces a new method for implementing attention in Transformer models."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Sparse Attention",
      "description": "FlashAttention improves upon sparse attention mechanisms by enhancing speed and memory efficiency."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "Low-Rank Attention",
      "description": "FlashAttention extends the concept of low-rank attention to achieve better performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Performer",
      "description": "FlashAttention compares its efficiency and performance against the Performer model."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention enhances the efficiency of the Transformer model's attention mechanism."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "Transformer-XL",
      "description": "Transformer-XL extends the capabilities of the original Transformer model for longer contexts."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Transformer-XL",
      "description": "FlashAttention is compared to Transformer-XL in terms of efficiency and performance."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Butterfly Factorizations",
      "description": "FlashAttention builds on the principles of Butterfly Factorizations to enhance performance."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Kaleidoscope",
      "description": "FlashAttention utilizes the efficient representation provided by Kaleidoscope."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "Pixelated Butterfly",
      "description": "FlashAttention extends the concepts of Pixelated Butterfly for improved training efficiency."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Monarch",
      "description": "FlashAttention introduces the use of Monarch matrices for enhanced accuracy."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Memory Efficiency",
      "description": "FlashAttention enhances memory efficiency in attention mechanisms."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "BERT",
      "description": "FlashAttention builds on the principles established by BERT for attention mechanisms."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Asymmetric Clustering",
      "description": "FlashAttention utilizes asymmetric clustering to achieve efficient attention."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "Structured Matrices",
      "description": "FlashAttention extends the concept of structured matrices for improved training efficiency."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "BERT",
      "description": "FlashAttention builds on the principles established by BERT in the context of attention mechanisms."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Layer-wise Optimal Brain Surgeon",
      "description": "FlashAttention improves upon techniques for optimizing neural network performance."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Transformers for Image Recognition",
      "description": "FlashAttention uses transformer architecture principles for efficient attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Structured Matrices",
      "description": "FlashAttention compares its efficiency with structured matrices in mathematical contexts."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "Fast Geometric Learning",
      "description": "FlashAttention extends the concepts of fast geometric learning to attention mechanisms."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention enhances the efficiency of traditional attention mechanisms."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "IO-Awareness",
      "description": "FlashAttention incorporates IO-awareness to optimize performance."
    },
    {
      "type": "EXTENDS",
      "source": "Lottery Ticket Hypothesis",
      "target": "Stabilizing the lottery ticket hypothesis",
      "description": "The research extends the lottery ticket hypothesis by exploring its stabilization."
    },
    {
      "type": "COMPARES_TO",
      "source": "Lottery Ticket Hypothesis",
      "target": "Linear mode connectivity",
      "description": "The lottery ticket hypothesis is compared to the concept of linear mode connectivity."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "target": "FlashAttention",
      "description": "The paper introduces the FlashAttention technique."
    },
    {
      "type": "USES",
      "source": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "target": "OpenWebText Corpus",
      "description": "The paper may utilize the OpenWebText Corpus for evaluation or training."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "target": "State-Space Models",
      "description": "The paper compares its technique with state-space models in the context of audio generation."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Memory Efficiency",
      "description": "FlashAttention enhances memory efficiency in attention mechanisms."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Speed",
      "description": "FlashAttention increases the speed of attention computations."
    },
    {
      "type": "BUILDS_ON",
      "source": "Hippo",
      "target": "Structured State Space Layers",
      "description": "Hippo builds on the concepts of structured state space layers for memory efficiency."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention enhances the efficiency of the Transformer model's attention mechanism."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "Memory hierarchy",
      "description": "FlashAttention leverages memory hierarchy design principles for improved performance."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "Deep compression",
      "description": "FlashAttention extends the concept of deep compression by focusing on attention mechanisms."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Structured state spaces",
      "description": "FlashAttention utilizes structured state spaces for efficient sequence modeling."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention improves the efficiency of the Transformer model's attention mechanism."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Data Movement Optimization",
      "description": "FlashAttention is compared to data movement optimization techniques in the context of transformer models."
    },
    {
      "type": "DISSECTS",
      "source": "Zhe Jia",
      "target": "Ampere GPU",
      "description": "Zhe Jia dissects the Ampere GPU architecture via microbenchmarking."
    },
    {
      "type": "DISSECTS",
      "source": "Zhe Jia",
      "target": "NVIDIA Volta GPU",
      "description": "Zhe Jia dissects the NVIDIA Volta GPU architecture via microbenchmarking."
    },
    {
      "type": "DISSECTS",
      "source": "Zhe Jia",
      "target": "Graphcore IPU",
      "description": "Zhe Jia dissects the Graphcore IPU architecture via microbenchmarking."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention improves the efficiency of the Transformer model's attention mechanism."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Tensor Processing Unit (TPU)",
      "description": "FlashAttention is designed to leverage the capabilities of Tensor Processing Units for enhanced performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "MIMIC-III",
      "description": "FlashAttention is evaluated against existing methods, including those that utilize the MIMIC-III dataset."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Memory Efficiency",
      "description": "FlashAttention enhances memory efficiency compared to traditional attention mechanisms."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Speed",
      "description": "FlashAttention increases the speed of attention computation."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "RNN",
      "description": "Transformers are compared to RNNs in terms of performance and efficiency."
    },
    {
      "type": "BUILDS_ON",
      "source": "Reformer",
      "target": "Transformer",
      "description": "Reformer builds on the Transformer architecture to improve efficiency."
    },
    {
      "type": "IMPROVES",
      "source": "Albert",
      "target": "BERT",
      "description": "Albert improves upon BERT by reducing model size while maintaining performance."
    },
    {
      "type": "USES",
      "source": "Runtime Neural Pruning",
      "target": "Neural Networks",
      "description": "Runtime Neural Pruning uses techniques to optimize neural networks during execution."
    },
    {
      "type": "COMPARES_TO",
      "source": "Performers",
      "target": "Transformers",
      "description": "Performers compare their efficiency against traditional Transformers."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Roberta",
      "description": "FlashAttention provides a more efficient attention mechanism that can enhance the performance of models like Roberta."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Luna",
      "description": "FlashAttention is compared to Luna in terms of efficiency and performance in attention mechanisms."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Nvidia A100",
      "description": "FlashAttention utilizes the capabilities of the Nvidia A100 architecture."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Nvidia H100",
      "description": "FlashAttention utilizes the capabilities of the Nvidia H100 architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Online normalizer calculation for softmax",
      "description": "FlashAttention is compared to the online normalizer calculation technique."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "Mlperf",
      "description": "FlashAttention builds on the Mlperf benchmark for performance evaluation."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention introduces a new method for implementing attention in Transformer models."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "PyTorch",
      "description": "FlashAttention is implemented using the PyTorch library."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Self-attention does not need O(n^2) memory",
      "description": "FlashAttention compares its memory efficiency with the findings of the paper on self-attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Language models are unsupervised multitask learners",
      "description": "FlashAttention is evaluated against the performance of language models discussed in this paper."
    },
    {
      "type": "PRESENTS_AT",
      "source": "FlashAttention",
      "target": "Annual Meeting of the Association for Computational Linguistics",
      "description": "FlashAttention is presented at the conference."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention enhances the efficiency and memory usage of the standard attention mechanism."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Compressive Transformers",
      "description": "FlashAttention is compared to Compressive Transformers in terms of performance on long-range sequence tasks."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention enhances the efficiency of the traditional attention mechanism."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Sparse Computation",
      "description": "FlashAttention employs sparse computation techniques to reduce memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention is compared to the Transformer model in terms of performance and efficiency."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Routing Transformers",
      "description": "FlashAttention enhances the efficiency of attention mechanisms used in Routing Transformers."
    },
    {
      "type": "BUILDS_ON",
      "source": "Movement Pruning",
      "target": "FlashAttention",
      "description": "FlashAttention builds on the principles of adaptive sparsity introduced by Movement Pruning."
    },
    {
      "type": "USES",
      "source": "Megatron-LM",
      "target": "FlashAttention",
      "description": "Megatron-LM utilizes FlashAttention for efficient attention computation."
    },
    {
      "type": "EXTENDS",
      "source": "Structured Transforms",
      "target": "FlashAttention",
      "description": "FlashAttention extends the concepts of structured transforms for improved performance."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention enhances the efficiency and memory usage of the Transformer model."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Adaptive Attention Span",
      "description": "FlashAttention incorporates adaptive attention span techniques to optimize performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Long Range Arena",
      "target": "FlashAttention",
      "description": "Long Range Arena serves as a benchmark to compare the performance of FlashAttention."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Attention",
      "description": "FlashAttention introduces a new approach to the attention mechanism."
    },
    {
      "type": "BUILDS_ON",
      "source": "DeepNet",
      "target": "Transformer",
      "description": "DeepNet builds on the transformer architecture to scale it to 1,000 layers."
    },
    {
      "type": "COMPARES_TO",
      "source": "Linformer",
      "target": "Attention",
      "description": "Linformer compares its linear complexity self-attention mechanism to traditional attention."
    },
    {
      "type": "USES",
      "source": "Data Locality Optimization",
      "target": "Roo\ufb02ine Model",
      "description": "Data locality optimization techniques are used to enhance the Roo\ufb02ine model's performance."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention enhances the efficiency of the Transformer model's attention mechanism."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "Optimal space lower bounds",
      "description": "FlashAttention is based on principles that involve optimal space usage."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Lightweight and dynamic convolutions",
      "description": "FlashAttention employs lightweight and dynamic convolutions to optimize performance."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention introduces a new approach to attention mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Nystr\u00f6mformer",
      "description": "FlashAttention compares its efficiency with the Nystr\u00f6mformer model."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Tokens-to-token vit",
      "description": "FlashAttention evaluates its performance against Tokens-to-token vit."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Big Bird",
      "description": "FlashAttention compares its capabilities with the Big Bird model."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Transformer",
      "description": "FlashAttention enhances the efficiency and memory usage of traditional transformer attention mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Big Bird",
      "description": "FlashAttention is compared to Big Bird in terms of handling longer sequences."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Attention Free Transformer",
      "description": "FlashAttention is compared to the Attention Free Transformer regarding attention mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Long-Short Transformer",
      "description": "FlashAttention is compared to Long-Short Transformer in terms of efficiency for language and vision."
    },
    {
      "type": "BUILDS_ON",
      "source": "IO-Aware Runtime Optimization",
      "target": "I/O Complexity",
      "description": "IO-Aware Runtime Optimization is based on the analysis of I/O complexity."
    },
    {
      "type": "BUILDS_ON",
      "source": "IO-Aware Runtime Optimization",
      "target": "Memory Hierarchies",
      "description": "The concept builds on the understanding of memory hierarchies."
    },
    {
      "type": "IMPROVES",
      "source": "Structured Matrices",
      "target": "Computational Complexity",
      "description": "Structured Matrices aim to reduce the computational complexity in machine learning models."
    },
    {
      "type": "USES",
      "source": "Block-Sparse Attention",
      "target": "Butterfly Matrices",
      "description": "Block-sparse attention uses butterfly matrices to achieve efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Structured Matrices",
      "target": "Traditional Matrices",
      "description": "Structured matrices are more efficient in theory compared to traditional matrices."
    },
    {
      "type": "INTRODUCES",
      "source": "Butterfly Matrices",
      "target": "Structured Matrices",
      "description": "Butterfly matrices are introduced as a means to express structured matrices efficiently."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Sparse Training",
      "description": "Flash Attention enhances the efficiency of sparse model training."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "Lottery Tickets Hypothesis",
      "description": "Flash Attention can be viewed as a fixed lottery ticket in the context of attention mechanisms."
    },
    {
      "type": "BUILDS_ON",
      "source": "Flash Attention",
      "target": "Butterfly Matrices",
      "description": "Flash Attention utilizes the butterfly pattern for its sparsity structure."
    },
    {
      "type": "COMPARES_TO",
      "source": "Sparse Training",
      "target": "Hardware Lottery",
      "description": "Sparse Training is contrasted with the hardware lottery phenomenon in terms of efficiency."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Long-range Arena",
      "description": "Flash Attention performs almost as well as dense attention on Long-range Arena tasks."
    },
    {
      "type": "USES",
      "source": "Reformer",
      "target": "sparse attention",
      "description": "Reformer uses hashing for sparse attention to reduce computational costs."
    },
    {
      "type": "USES",
      "source": "Smyrf",
      "target": "sparse attention",
      "description": "Smyrf employs sparse attention techniques to enhance efficiency."
    },
    {
      "type": "USES",
      "source": "Performer",
      "target": "low-rank approximation",
      "description": "Performer utilizes low-rank approximation to improve attention mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "Longformer",
      "target": "sparse and low-rank approximations",
      "description": "Longformer combines sparse and low-rank approximations for better accuracy."
    },
    {
      "type": "COMPARES_TO",
      "source": "Big Bird",
      "target": "sparse attention",
      "description": "Big Bird employs sparse attention mechanisms for handling long sequences."
    },
    {
      "type": "COMPARES_TO",
      "source": "Scatterbrain",
      "target": "sparse attention",
      "description": "Scatterbrain uses sparse attention techniques to manage long-range dependencies."
    },
    {
      "type": "COMPARES_TO",
      "source": "Long-short transformer",
      "target": "efficiency",
      "description": "Long-short transformer is designed to improve efficiency for long sequences."
    },
    {
      "type": "COMPARES_TO",
      "source": "Combiner",
      "target": "various attention mechanisms",
      "description": "Combiner integrates various attention mechanisms for enhanced performance."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "natural language processing",
      "description": "Transformer architecture is foundational for models in natural language processing."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "computer vision",
      "description": "Transformer architecture is also widely used in computer vision tasks."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "Transformer-XL",
      "description": "Transformer-XL extends the capabilities of the Transformer model by allowing longer context."
    },
    {
      "type": "EXTENDS",
      "source": "Hi PPO",
      "target": "S 4",
      "description": "S 4 is an extension of Hi PPO that enhances its capabilities."
    },
    {
      "type": "COMPARES_TO",
      "source": "Lambda Networks",
      "target": "Attention",
      "description": "Lambda Networks are compared to traditional attention mechanisms as an alternative."
    },
    {
      "type": "COMPARES_TO",
      "source": "AFT",
      "target": "Attention",
      "description": "AFT is compared to attention mechanisms in the context of modeling tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "FLASH",
      "target": "Attention",
      "description": "FLASH is compared to attention mechanisms in image classification and language modeling."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Memory Efficiency",
      "description": "Flash Attention improves memory efficiency by requiring extra memory linear instead of quadratic in the sequence length."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Attention Mechanism",
      "description": "Flash Attention uses the attention mechanism but optimizes its computation for better performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Lambda Networks",
      "description": "Flash Attention is compared to Lambda Networks as part of the broader context of attention replacement attempts."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "AFT",
      "description": "Flash Attention is compared to AFT in the context of replacing attention mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "FLASH",
      "description": "Flash Attention is compared to FLASH in the context of replacing attention mechanisms."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Memory footprint",
      "description": "FlashAttention reduces the memory footprint during attention computation."
    },
    {
      "type": "USES",
      "source": "Attention mechanism",
      "target": "Softmax normalization",
      "description": "The attention mechanism uses softmax normalization to compute attention outputs."
    },
    {
      "type": "BUILDS_ON",
      "source": "Softmax normalization",
      "target": "Attention mechanism",
      "description": "Softmax normalization is a foundational technique used in the attention mechanism."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "Attention mechanism",
      "description": "FlashAttention extends the traditional attention mechanism by improving efficiency."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention enhances the standard attention mechanism by making it faster and more memory-efficient."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Softmax",
      "description": "FlashAttention employs the softmax function to normalize attention scores."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "Gradient Checkpointing",
      "description": "FlashAttention extends the concept of gradient checkpointing to achieve memory efficiency in the backward pass."
    },
    {
      "type": "USES",
      "source": "Gradient Checkpointing",
      "target": "Backward Pass",
      "description": "Gradient checkpointing is used to optimize the memory efficiency of the backward pass."
    },
    {
      "type": "IMPROVES",
      "source": "Backward Pass",
      "target": "Memory Efficiency",
      "description": "The backward pass is computed in a memory-efficient manner."
    },
    {
      "type": "USES",
      "source": "Output Gradient",
      "target": "Reverse-mode Autodiff",
      "description": "The output gradient is computed using reverse-mode autodiff."
    },
    {
      "type": "USES",
      "source": "Softmax Function",
      "target": "Input Gradients",
      "description": "The softmax function is used to compute input gradients."
    },
    {
      "type": "USES",
      "source": "Jacobian",
      "target": "Softmax Function",
      "description": "The Jacobian is derived from the softmax function to compute gradients."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Softmax",
      "description": "FlashAttention utilizes the softmax function to compute attention scores."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Jacobian",
      "description": "FlashAttention employs the Jacobian to derive gradients for optimization."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Memory Efficiency",
      "description": "FlashAttention enhances memory efficiency in attention mechanisms."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Speed",
      "description": "FlashAttention increases the speed of attention computation."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention enhances the efficiency and memory usage of traditional attention mechanisms."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Pointwise Multiplication",
      "description": "FlashAttention employs pointwise multiplication as part of its computation process."
    },
    {
      "type": "EVALUATES",
      "source": "FlashAttention",
      "target": "Memory Complexity",
      "description": "FlashAttention evaluates memory complexity in terms of O(n) and O(d) for its operations."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Softmax Scaling",
      "description": "Flash Attention uses softmax scaling to normalize attention scores."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Masking Function",
      "description": "Flash Attention employs a masking function to handle variable-length input sequences."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Dropout",
      "description": "Flash Attention applies dropout to the attention probabilities to prevent overfitting."
    },
    {
      "type": "BUILDS_ON",
      "source": "Flash Attention",
      "target": "Algorithm 2",
      "description": "Flash Attention builds on the principles outlined in Algorithm 2 for its forward pass."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention introduces a new approach to the attention mechanism that is faster and more memory-efficient."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "HBM",
      "description": "Flash Attention uses High Bandwidth Memory to store input matrices."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "on-chip SRAM",
      "description": "Flash Attention utilizes on-chip SRAM for temporary data storage during computations."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Attention Mechanism",
      "description": "Flash Attention improves the efficiency of the standard attention mechanism."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention introduces a new approach to the standard attention mechanism."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Memory Efficiency",
      "description": "FlashAttention improves memory efficiency compared to traditional attention mechanisms."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Computational Speed",
      "description": "FlashAttention enhances computational speed in processing attention."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Input Sequences",
      "description": "FlashAttention uses input sequences (Q, K, V) for its computations."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Standard Attention Backward Pass",
      "description": "FlashAttention improves the standard attention backward pass by reducing memory usage."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "HBM",
      "description": "FlashAttention uses High Bandwidth Memory for loading and storing matrices during computation."
    },
    {
      "type": "USES",
      "source": "Standard Attention Backward Pass",
      "target": "Q-K-V",
      "description": "The standard attention backward pass utilizes the Q, K, and V matrices for gradient computation."
    },
    {
      "type": "USES",
      "source": "Standard Attention Backward Pass",
      "target": "P",
      "description": "The standard attention backward pass uses the P matrix to compute attention scores."
    },
    {
      "type": "IMPROVES",
      "source": "Standard Attention Backward Pass",
      "target": "Dropout Mask",
      "description": "The standard attention backward pass improves memory efficiency by not storing the dropout mask."
    },
    {
      "type": "USES",
      "source": "Standard Attention Backward Pass",
      "target": "Pseudo-Random Number Generator",
      "description": "The standard attention backward pass uses a pseudo-random number generator to regenerate the dropout mask."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Memory Usage",
      "description": "FlashAttention improves memory usage by allowing only O(1N) extra memory."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Dropout Mask",
      "description": "FlashAttention uses the dropout mask generated from the forward pass during the backward pass."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Softmax Gradient",
      "description": "FlashAttention computes the softmax gradient using Eq. (4)."
    },
    {
      "type": "USES",
      "source": "Softmax Gradient",
      "target": "Dot Product",
      "description": "The computation of the softmax gradient involves calculating the dot product between vectors."
    },
    {
      "type": "INTRODUCES",
      "source": "Flash Attention",
      "target": "Flash Attention Backward Pass",
      "description": "Flash Attention introduces the backward pass algorithm to efficiently compute gradients."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Q-K-V-O",
      "description": "Flash Attention uses matrices Q, K, V, and O for its computations."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "\u2113-m",
      "description": "Flash Attention uses vectors \u2113 and m for its operations."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "softmax scaling constant (\u03c4)",
      "description": "Flash Attention uses the softmax scaling constant for its softmax calculations."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "dropout probability (p_drop)",
      "description": "Flash Attention uses the dropout probability to manage overfitting during training."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "pseudo-random number generator state (R)",
      "description": "Flash Attention uses the pseudo-random number generator state from the forward pass."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "on-chip SRAM",
      "description": "Flash Attention utilizes on-chip SRAM for efficient memory access during computations."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "HBM",
      "description": "FlashAttention utilizes HBM for efficient data storage during computations."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "SRAM",
      "description": "FlashAttention uses SRAM for on-chip data processing."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "dropout mask",
      "description": "FlashAttention improves the application of dropout masks in attention mechanisms."
    },
    {
      "type": "COMPUTES",
      "source": "FlashAttention",
      "target": "S",
      "description": "FlashAttention computes the attention scores matrix S on-chip."
    },
    {
      "type": "COMPUTES",
      "source": "FlashAttention",
      "target": "P",
      "description": "FlashAttention computes the processed attention probabilities matrix P using dropout."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention enhances the efficiency of the traditional attention mechanism."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "IO-complexity",
      "description": "FlashAttention incorporates IO-complexity analysis to optimize performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Backward Pass",
      "target": "Forward Pass",
      "description": "The backward pass is analyzed in comparison to the forward pass in terms of computational complexity."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "FLOPs",
      "description": "FlashAttention's performance is evaluated in terms of FLOPs during both forward and backward passes."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "Flash Attention reduces the IO-complexity of the backward pass compared to Standard Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "The paper compares the IO-complexity of Flash Attention's backward pass to that of Standard Attention."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "HBM accesses",
      "description": "Flash Attention's backward pass requires a specific number of HBM accesses."
    },
    {
      "type": "USES",
      "source": "Standard Attention",
      "target": "HBM accesses",
      "description": "Standard Attention's backward pass requires a specific number of HBM accesses."
    },
    {
      "type": "EXTENDS",
      "source": "Theorem 2",
      "target": "Theorem 5",
      "description": "Theorem 5 extends the analysis of IO-complexity to the backward pass based on Theorem 2."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Rabe and Staats Algorithm",
      "description": "Both algorithms operate on blocks of the attention matrix but focus on different aspects: Flash Attention on memory accesses and Rabe and Staats on memory footprint."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Tiling",
      "description": "Flash Attention uses the tiling technique to operate on blocks of the attention matrix."
    },
    {
      "type": "USES",
      "source": "Rabe and Staats Algorithm",
      "target": "Tiling",
      "description": "Rabe and Staats Algorithm uses the tiling technique to operate on blocks of the attention matrix."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Memory Accesses",
      "description": "Flash Attention improves performance by reducing memory accesses."
    },
    {
      "type": "IMPROVES",
      "source": "Rabe and Staats Algorithm",
      "target": "Memory Footprint",
      "description": "Rabe and Staats Algorithm improves performance by reducing the total memory footprint."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "Flash Attention is faster than Standard Attention by a factor of 2-4."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Rabe and Staats",
      "description": "Flash Attention is compared to Rabe and Staats, which is around the same speed or slightly slower than Standard Attention."
    },
    {
      "type": "USES",
      "source": "Rabe and Staats",
      "target": "Softmax Normalization Statistics",
      "description": "Rabe and Staats uses softmax normalization statistics to summarize information from blocks."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Incremental Updates",
      "description": "Flash Attention uses incremental updates to process outputs after each block."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Gradient Checkpointing",
      "description": "FlashAttention simplifies the backward pass compared to gradient checkpointing, reducing memory requirements."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Rabe and Staats",
      "description": "FlashAttention is compared to Rabe and Staats in terms of memory efficiency and computational speed."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Algorithm 1",
      "description": "FlashAttention utilizes Algorithm 1 for its computation steps."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "FLOPs",
      "description": "FlashAttention's performance is evaluated based on the number of FLOPs required for its operations."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "softmax",
      "description": "FlashAttention utilizes the softmax function to normalize attention scores."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Q",
      "description": "FlashAttention computes attention scores using the Query matrix Q."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "K",
      "description": "FlashAttention computes attention scores using the Key matrix K."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "V",
      "description": "FlashAttention computes the output using the Value matrix V."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "FLOPs",
      "description": "FlashAttention aims to reduce the number of FLOPs required for attention computation."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "HBM",
      "description": "FlashAttention extends the use of HBM for efficient memory management."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Algorithm 1",
      "description": "FlashAttention introduces Algorithm 1 for computing attention efficiently."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "rowmax",
      "description": "FlashAttention uses the rowmax metric to compute maximum values in the attention mechanism."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "rowsum",
      "description": "FlashAttention uses the rowsum metric to compute sums in the attention mechanism."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "HBM",
      "description": "FlashAttention performs computations in High Bandwidth Memory (HBM)."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "S",
      "description": "FlashAttention uses the matrix S for its computations."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "V",
      "description": "FlashAttention uses the variable V in its attention calculations."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention introduces a new approach to the attention mechanism that is faster and more memory-efficient."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Row-max",
      "description": "FlashAttention uses row-max values for normalization in its calculations."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Slice",
      "description": "FlashAttention operates on slices of matrices to perform attention calculations."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention improves the efficiency and memory usage of traditional attention mechanisms."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Softmax",
      "description": "FlashAttention employs the softmax function to generate probability distributions from attention scores."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Attention",
      "description": "FlashAttention introduces a new method for implementing attention that is faster and more memory-efficient."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Q",
      "description": "FlashAttention uses the query matrix Q as part of its computation."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "K",
      "description": "FlashAttention uses the key matrix K as part of its computation."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "V",
      "description": "FlashAttention uses the value matrix V as part of its computation."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "IO complexity",
      "description": "FlashAttention builds on the concept of IO complexity to optimize attention computation."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "HBM",
      "description": "FlashAttention compares its performance in terms of memory access with HBM."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "IO complexity",
      "description": "FlashAttention improves the IO complexity of attention mechanisms by optimizing memory access patterns."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "HBM",
      "description": "FlashAttention uses High Bandwidth Memory for efficient data access during computation."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "softmax",
      "description": "FlashAttention employs the softmax function to compute probabilities from input scores."
    },
    {
      "type": "REQUIRES",
      "source": "standard attention implementation",
      "target": "global memory",
      "description": "Standard attention implementation requires global memory accesses for reading inputs."
    },
    {
      "type": "REQUIRES",
      "source": "FlashAttention",
      "target": "on-chip memory",
      "description": "FlashAttention requires on-chip memory to store blocks of K and V."
    },
    {
      "type": "CONSTRAINTS",
      "source": "B_c",
      "target": "M",
      "description": "The block size B_c must be constrained by the size M to fit into on-chip memory."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "On-chip Memory",
      "description": "FlashAttention requires on-chip memory to efficiently process blocks of data."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Block Sizes",
      "description": "FlashAttention optimizes the block sizes to enhance memory efficiency."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Exact Attention",
      "description": "FlashAttention introduces a new approach to computing exact attention efficiently."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "IO complexity",
      "description": "FlashAttention uses IO complexity to analyze the efficiency of attention computation."
    },
    {
      "type": "COMPARES_TO",
      "source": "Exact Attention",
      "target": "Standard Attention",
      "description": "Exact Attention is compared to standard attention mechanisms in terms of HBM accesses."
    },
    {
      "type": "EXTENDS",
      "source": "Theorem 5",
      "target": "Theorem 2",
      "description": "Theorem 5 extends the analysis of IO complexity from attention forward to attention backward."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "Flash Attention improves upon the standard attention mechanism by reducing IO complexity."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "IO Complexity",
      "description": "Flash Attention utilizes IO complexity analysis to optimize memory access patterns."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Standard Attention",
      "description": "The paper compares the IO complexity of Flash Attention with that of standard attention."
    },
    {
      "type": "EXTENDS",
      "source": "Theorem 2",
      "target": "Flash Attention",
      "description": "The analysis in Flash Attention extends the findings of Theorem 2."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "HBM accesses",
      "description": "FlashAttention introduces a method to optimize the number of HBM accesses in attention mechanisms."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Theorem 2",
      "description": "FlashAttention uses Theorem 2 to establish constraints on block sizes."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Block sizes",
      "description": "FlashAttention improves the efficiency of block sizes in the attention mechanism."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Tc",
      "description": "FlashAttention improves the computation time Tc in the attention mechanism."
    },
    {
      "type": "RELATES_TO",
      "source": "N",
      "target": "d",
      "description": "N is related to d as it represents the number of elements in the context of dimensionality."
    },
    {
      "type": "INTRODUCES",
      "source": "FlashAttention",
      "target": "Block-Sparse Flash Attention",
      "description": "FlashAttention introduces the Block-Sparse Flash Attention technique for efficient attention computation."
    },
    {
      "type": "USES",
      "source": "Block-Sparse Flash Attention",
      "target": "masking function",
      "description": "Block-Sparse Flash Attention uses a masking function to apply sparsity to attention scores."
    },
    {
      "type": "USES",
      "source": "Block-Sparse Flash Attention",
      "target": "dropout probability",
      "description": "Block-Sparse Flash Attention uses dropout probability to enhance training robustness."
    },
    {
      "type": "USES",
      "source": "Block-Sparse Flash Attention",
      "target": "Matrices Q, K, V",
      "description": "Block-Sparse Flash Attention uses matrices Q, K, and V as inputs for the attention mechanism."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "softmax scaling constant",
      "description": "FlashAttention improves the efficiency of the softmax scaling constant in attention calculations."
    },
    {
      "type": "BUILDS_ON",
      "source": "Block-Sparse Flash Attention",
      "target": "block sizes",
      "description": "Block-Sparse Flash Attention builds on the concept of block sizes to enhance performance."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "Block-sparse Flash Attention",
      "description": "Block-sparse Flash Attention is an extension of FlashAttention that optimizes performance by skipping zero blocks."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Attention Mechanism",
      "description": "FlashAttention utilizes the attention mechanism to compute attention scores efficiently."
    },
    {
      "type": "EVALUATES",
      "source": "Block-sparse Flash Attention",
      "target": "IO-complexity",
      "description": "The paper evaluates the IO-complexity of the block-sparse Flash Attention algorithm."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "IO-complexity",
      "description": "Flash Attention improves the IO-complexity by optimizing memory access patterns."
    },
    {
      "type": "USES",
      "source": "Multi-GPU Attention",
      "target": "Flash Attention",
      "description": "Multi-GPU Attention uses Flash Attention to enhance performance across multiple GPUs."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "Large Language Models",
      "description": "Flash Attention extends the capabilities of Large Language Models by enabling efficient attention computation."
    },
    {
      "type": "IMPROVES",
      "source": "Sparse weight matrices",
      "target": "MLP layers",
      "description": "Sparse weight matrices can improve the efficiency of MLP layers."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Attention matrix",
      "description": "FlashAttention relies on the attention matrix being a function of a low-rank matrix."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "IO-awareness",
      "description": "FlashAttention incorporates IO-awareness to enhance performance."
    },
    {
      "type": "EXTENDS",
      "source": "Kernel machine learning",
      "target": "FlashAttention",
      "description": "FlashAttention extends the principles of kernel machine learning by optimizing attention computation."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Attention Matrix",
      "description": "Flash Attention uses the attention matrix to compute attention scores efficiently."
    },
    {
      "type": "USES",
      "source": "BERT-large",
      "target": "LAMB",
      "description": "BERT-large uses the LAMB optimizer for training."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Kernel Machine Learning",
      "description": "Flash Attention compares its approach to kernel machine learning in terms of memory access."
    },
    {
      "type": "IMPROVES",
      "source": "Ke Ops",
      "target": "Kernel Machine Learning",
      "description": "Ke Ops improves kernel machine learning operations by reducing memory reads/writes."
    },
    {
      "type": "BUILDS_ON",
      "source": "Flash Attention",
      "target": "Kernel Matrix",
      "description": "Flash Attention builds on the concept of kernel matrices to optimize attention calculations."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "LAMB optimizer",
      "description": "FlashAttention uses the LAMB optimizer for training."
    },
    {
      "type": "EVALUATES",
      "source": "FlashAttention",
      "target": "MLPerf 1.1",
      "description": "FlashAttention evaluates its performance using the MLPerf 1.1 dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Nvidia's reported training speed",
      "description": "FlashAttention compares its training speed with the reported speed from Nvidia."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "FP 16 precision",
      "description": "FlashAttention uses FP 16 precision for training."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "Apex AMP",
      "description": "FlashAttention builds on the capabilities of Apex AMP for mixed precision training."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "GPT-2",
      "description": "FlashAttention compares its results with the standard implementations of GPT-2."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "A100-80 GB GPUs",
      "description": "FlashAttention uses A100-80 GB GPUs for training."
    },
    {
      "type": "BUILDS_ON",
      "source": "FlashAttention",
      "target": "Megatron-LM",
      "description": "FlashAttention builds on the training recipe from the Megatron-LM repository."
    },
    {
      "type": "USES",
      "source": "GPT-2",
      "target": "Huggingface Transformers",
      "description": "GPT-2 uses the standard implementations from the Huggingface Transformers library."
    },
    {
      "type": "USES",
      "source": "GPT-2",
      "target": "Nvidia\u2019s Megatron-LM",
      "description": "GPT-2 uses the standard implementations from Nvidia\u2019s Megatron-LM repository."
    },
    {
      "type": "IMPROVES",
      "source": "Gradient Accumulation",
      "target": "GPU Memory Usage",
      "description": "Gradient accumulation improves the ability to fit larger batch sizes into available GPU memory."
    },
    {
      "type": "USES",
      "source": "GPT-2",
      "target": "Adam W",
      "description": "GPT-2 uses the Adam W optimizer for training."
    },
    {
      "type": "USES",
      "source": "GPT-2",
      "target": "Openwebtext",
      "description": "GPT-2 is trained on the Openwebtext dataset."
    },
    {
      "type": "USES",
      "source": "Mixed-Precision Training",
      "target": "Py Torch AMP",
      "description": "Mixed-precision training is implemented using Py Torch AMP."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Hugging Face implementation",
      "description": "Flash Attention behaves similarly to the baseline Hugging Face implementation in terms of validation perplexity."
    },
    {
      "type": "USES",
      "source": "GPT-2 small",
      "target": "Flash Attention",
      "description": "GPT-2 small is trained using the Flash Attention technique."
    },
    {
      "type": "USES",
      "source": "GPT-2 medium",
      "target": "Flash Attention",
      "description": "GPT-2 medium is trained using the Flash Attention technique."
    },
    {
      "type": "FOLLOWS",
      "source": "MIMIC-III",
      "target": "Dai et al.",
      "description": "The hyperparameters for MIMIC-III are based on the work of Dai et al."
    },
    {
      "type": "FOLLOWS",
      "source": "ECt HR",
      "target": "Dai et al.",
      "description": "The hyperparameters for ECt HR are based on the work of Dai et al."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-2-small Flash Attention",
      "target": "GPT-2-small Hugging Face",
      "description": "Flash Attention yields the same validation curves as the baseline implementation from Hugging Face."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-2-medium Flash Attention",
      "target": "GPT-2-medium Hugging Face",
      "description": "Flash Attention yields the same validation curves as the baseline implementation from Hugging Face."
    },
    {
      "type": "BUILDS_ON",
      "source": "Flash Attention",
      "target": "Hugging Face",
      "description": "Flash Attention builds on the implementations provided by Hugging Face."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Long-range arena",
      "description": "Flash Attention is evaluated using the Long-range arena benchmark."
    },
    {
      "type": "USES",
      "source": "Mixed-Precision Training",
      "target": "Performer",
      "description": "Performer is not stable when using mixed-precision training."
    },
    {
      "type": "USES",
      "source": "Mixed-Precision Training",
      "target": "Local Attention",
      "description": "Local Attention's implementation does not support mixed-precision training."
    },
    {
      "type": "BUILDS_ON",
      "source": "Path-X",
      "target": "Path-64",
      "description": "Path-X is fine-tuned after pretraining on Path-64."
    },
    {
      "type": "BUILDS_ON",
      "source": "Path-256",
      "target": "Path-64",
      "description": "Path-256 is also related to the pretraining on Path-64."
    },
    {
      "type": "COMPARES_TO",
      "source": "Attention Methods",
      "target": "Wallclock-Time Speedup",
      "description": "Different attention methods are compared based on their wallclock-time speedup across tasks."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Path-X",
      "description": "FlashAttention improves the performance of the Path-X model."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Apex FMHA",
      "description": "FlashAttention is compared to Apex FMHA, which was the fastest implementation of attention at the time."
    },
    {
      "type": "USES",
      "source": "MLPerf",
      "target": "Apex FMHA",
      "description": "Almost all MLPerf submissions for BERT training benchmark use Apex FMHA for their model code."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "FMHA",
      "description": "Flash Attention is compared to FMHA in terms of runtime performance across different sequence lengths."
    },
    {
      "type": "USES",
      "source": "FMHA",
      "target": "BERT-large",
      "description": "FMHA targets BERT models and specifically supports a head dimension of 64."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "A100-SXM",
      "description": "Flash Attention is measured on an A100-SXM GPU."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "FMHA",
      "description": "FlashAttention improves upon FMHA by supporting longer sequences and using memory-saving techniques."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Tiling",
      "description": "FlashAttention uses tiling to manage memory for long sequences."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Recomputation",
      "description": "FlashAttention uses recomputation to save memory during the backward pass."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "FMHA",
      "description": "FlashAttention is compared to FMHA in terms of performance for short sequences."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "FMHA",
      "description": "FlashAttention is compared to FMHA in terms of runtime performance."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "A100 GPU",
      "description": "FlashAttention is profiled on an A100 GPU to measure speedup."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Speedup",
      "description": "FlashAttention shows a speedup over standard PyTorch attention."
    },
    {
      "type": "EVALUATES",
      "source": "FlashAttention",
      "target": "PyTorch",
      "description": "FlashAttention is evaluated within the PyTorch framework."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Standard PyTorch Attention",
      "description": "FlashAttention is compared to standard PyTorch attention in terms of speedup."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "A100",
      "description": "FlashAttention is evaluated on the A100 GPU."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "RTX 3090",
      "description": "FlashAttention is evaluated on the RTX 3090 GPU."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "T4",
      "description": "FlashAttention is evaluated on the T4 GPU."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "Speedup",
      "description": "FlashAttention shows significant speedup in performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "RTX 3090",
      "target": "A100",
      "description": "The performance of RTX 3090 is compared to A100 in terms of memory bandwidth."
    },
    {
      "type": "COMPARES_TO",
      "source": "T4",
      "target": "A100",
      "description": "The performance of T4 is compared to A100, noting the smaller SRAM size."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "T4",
      "description": "FlashAttention is implemented on T4 GPUs, which affects the block sizes and speedup observed."
    },
    {
      "type": "IMPROVES",
      "source": "FlashAttention",
      "target": "speedup",
      "description": "FlashAttention aims to improve speedup during the forward pass on T4 GPUs."
    },
    {
      "type": "EXTENDS",
      "source": "FlashAttention",
      "target": "IO complexity",
      "description": "FlashAttention extends the analysis of IO complexity in the context of GPU performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "PyTorch",
      "description": "FlashAttention is compared against the reference implementation of exact attention from PyTorch."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Hugging Face",
      "description": "FlashAttention is compared against the reference implementation of exact attention from Hugging Face."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Megatron",
      "description": "FlashAttention is compared against the reference implementation of exact attention from Megatron."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Reformer",
      "description": "FlashAttention is compared against the reference implementation of approximate attention from Reformer."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Local Attention",
      "description": "FlashAttention is compared against the reference implementation of approximate attention from Local Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Linformer Attention",
      "description": "FlashAttention is compared against the reference implementation of approximate attention from Linformer Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Smyrf",
      "description": "FlashAttention is compared against the reference implementation of approximate attention from Smyrf."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Long Short Former (LSFormer)",
      "description": "FlashAttention is compared against the reference implementation of approximate attention from LSFormer."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Block-Sparse Attention",
      "description": "FlashAttention is compared against the reference implementation of sparse attention from Block-Sparse Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Longformer",
      "description": "FlashAttention is compared against the reference implementation of sparse attention from Longformer."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Big Bird Attention",
      "description": "FlashAttention is compared against the reference implementation of sparse attention from Big Bird Attention."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Dropout",
      "description": "FlashAttention uses dropout as a regularization technique during training."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "Masking",
      "description": "FlashAttention uses masking to control which positions are attended to in the input sequences."
    },
    {
      "type": "EVALUATES",
      "source": "FlashAttention",
      "target": "Runtime",
      "description": "FlashAttention evaluates its performance based on runtime measurements."
    },
    {
      "type": "EVALUATES",
      "source": "FlashAttention",
      "target": "Memory Footprint",
      "description": "FlashAttention evaluates its memory usage during the attention computation."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Py Torch Attention",
      "description": "FlashAttention is compared to Py Torch Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Megatron",
      "description": "FlashAttention is compared to Megatron in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Reformer",
      "description": "FlashAttention is compared to Reformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Local Attention",
      "description": "FlashAttention is compared to Local Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Linformer",
      "description": "FlashAttention is compared to Linformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Smyrf",
      "description": "FlashAttention is compared to Smyrf in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of performance."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Dropout",
      "description": "Flash Attention uses dropout in its measurements."
    },
    {
      "type": "USES",
      "source": "Flash Attention",
      "target": "Masking",
      "description": "Flash Attention uses masking in its measurements."
    },
    {
      "type": "USES",
      "source": "FlashAttention",
      "target": "FP 16",
      "description": "FlashAttention uses FP 16 for all measurements."
    },
    {
      "type": "USES",
      "source": "Local Attention",
      "target": "FP 32",
      "description": "Local Attention implementation only supports FP 32."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Megatron",
      "description": "FlashAttention is compared to Megatron regarding sequence length support."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Block-Sparse",
      "description": "FlashAttention is compared to Block-Sparse regarding sequence length support."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Longformer",
      "description": "FlashAttention is compared to Longformer regarding sequence length support."
    },
    {
      "type": "COMPARES_TO",
      "source": "FlashAttention",
      "target": "Big Bird",
      "description": "FlashAttention is compared to Big Bird regarding sequence length support."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Block-Sparse Flash Attention",
      "target": "Flash Attention",
      "description": "Block-Sparse Flash Attention is a variant of Flash Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block-Sparse Flash Attention",
      "description": "Both are attention mechanisms evaluated for performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared against standard PyTorch Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared against the Megatron model."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared against the Reformer model."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared against Local Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared against Linformer."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared against Smyrf."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared against LSformer."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared against Block Sparse."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared against Longformer."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared against Big Bird."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse Flash Attention",
      "description": "Flash Attention is compared to its block sparse variant."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird regarding efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of runtime."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Block-Sparse Flash Attention",
      "description": "Block-Sparse Flash Attention improves upon Flash Attention by incorporating block sparsity."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of performance metrics."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "Block-Sparse Flash Attention",
      "description": "Block-Sparse Flash Attention extends the capabilities of Flash Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Block-Sparse Flash Attention",
      "target": "Flash Attention",
      "description": "Block-Sparse Flash Attention is compared to Flash Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block-Sparse Flash Attention",
      "description": "Both techniques are compared based on their forward pass runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron regarding efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is evaluated against Reformer for performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared with Local Attention based on runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is evaluated against Smyrf for performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared with LSformer regarding runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse in terms of efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is evaluated against Longformer for performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared with Big Bird based on runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse in terms of runtime efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer regarding performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention for runtime performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron in terms of backward pass runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer regarding runtime efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer regarding efficiency metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of runtime performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer regarding efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Block-Sparse Flash Attention",
      "target": "Flash Attention",
      "description": "Block-Sparse Flash Attention is compared to Flash Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of performance metrics."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "Block-Sparse Flash Attention",
      "description": "Block-Sparse Flash Attention is an extension of Flash Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention in terms of runtime performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron regarding runtime efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention based on runtime metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer regarding performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer based on runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse attention mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of runtime performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird regarding efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Block-Sparse Flash Attention",
      "target": "Flash Attention",
      "description": "Block-Sparse Flash Attention is a variant of Flash Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block-Sparse Flash Attention",
      "description": "Both are attention mechanisms evaluated for runtime efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron regarding performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer for efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer for performance metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf regarding efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse for efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer regarding performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of runtime."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of runtime."
    },
    {
      "type": "IMPROVES",
      "source": "Flash Attention",
      "target": "Block Sparse Flash Attention",
      "description": "Block Sparse Flash Attention improves upon Flash Attention by utilizing block sparse techniques."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf in terms of memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse in terms of memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer in terms of memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Block-Sparse Flash Attention",
      "target": "Flash Attention",
      "description": "Block-Sparse Flash Attention is an extension of Flash Attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Py Torch Attention",
      "description": "Flash Attention is compared to Py Torch Attention in terms of memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Megatron",
      "description": "Flash Attention is compared to Megatron in terms of memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer in terms of memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention in terms of memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Reformer",
      "description": "Flash Attention is compared to Reformer in terms of efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Local Attention",
      "description": "Flash Attention is compared to Local Attention regarding performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Linformer",
      "description": "Flash Attention is compared to Linformer in terms of complexity."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Smyrf",
      "description": "Flash Attention is compared to Smyrf for attention efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "LSformer",
      "description": "Flash Attention is compared to LSformer in terms of attention mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Block Sparse",
      "description": "Flash Attention is compared to Block Sparse regarding memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Longformer",
      "description": "Flash Attention is compared to Longformer for handling long sequences."
    },
    {
      "type": "COMPARES_TO",
      "source": "Flash Attention",
      "target": "Big Bird",
      "description": "Flash Attention is compared to Big Bird in terms of scalability."
    },
    {
      "type": "EXTENDS",
      "source": "Flash Attention",
      "target": "Block-Sparse Flash Attention",
      "description": "Block-Sparse Flash Attention extends the capabilities of Flash Attention."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Pre-training",
      "description": "BERT introduces a new approach to pre-training deep bidirectional representations."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT uses the Transformer architecture to create its language representations."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Question Answering",
      "description": "BERT improves the performance of question answering tasks by fine-tuning the pre-trained model."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Language Inference",
      "description": "BERT improves the performance of language inference tasks by fine-tuning the pre-trained model."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "GLUE score",
      "description": "BERT improves the GLUE score to 80.5%, a 7.7% absolute improvement."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Multi NLI accuracy",
      "description": "BERT improves Multi NLI accuracy to 86.7%, a 4.6% absolute improvement."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "SQuAD v1.1 Test F1",
      "description": "BERT improves SQuAD v1.1 Test F1 to 93.2, a 1.5 point absolute improvement."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "SQuAD v2.0 Test F1",
      "description": "BERT improves SQuAD v2.0 Test F1 to 83.1, a 5.1 point absolute improvement."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Language model pre-training",
      "description": "BERT utilizes language model pre-training to enhance its performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Natural Language Inference",
      "description": "BERT is evaluated on the natural language inference task."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Paraphrasing",
      "description": "BERT is evaluated on the paraphrasing task."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Natural Language Inference",
      "description": "BERT is used for natural language inference tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Paraphrasing",
      "description": "BERT is used for paraphrasing tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Named Entity Recognition",
      "description": "BERT is used for named entity recognition tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Question Answering",
      "description": "BERT is used for question answering tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Feature-based approach",
      "target": "Fine-tuning approach",
      "description": "The feature-based approach uses task-specific architectures, while the fine-tuning approach introduces minimal task-specific parameters."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "ELMo",
      "description": "BERT introduces the concept of using pre-trained representations in a different manner compared to ELMo."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Generative Pre-trained Transformer (Open AI GPT)",
      "description": "BERT introduces a new method of pre-training language representations."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Open AI GPT",
      "description": "BERT and Open AI GPT both aim to learn general language representations but differ in their architecture."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT improves upon fine-tuning approaches by utilizing bidirectional representations."
    },
    {
      "type": "USES",
      "source": "Open AI GPT",
      "target": "Left-to-right architecture",
      "description": "Open AI GPT employs a left-to-right architecture in its self-attention layers."
    },
    {
      "type": "LIMITS",
      "source": "Unidirectional language models",
      "target": "Fine-tuning",
      "description": "Unidirectional language models restrict the effectiveness of fine-tuning approaches."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT builds on the transformer architecture to enhance language understanding."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Fine-tuning based approaches",
      "description": "BERT improves fine-tuning based approaches by alleviating the unidirectionality constraint."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Masked Language Model (MLM)",
      "description": "BERT uses the masked language model as a pre-training objective."
    },
    {
      "type": "INSPIRED_BY",
      "source": "Masked Language Model (MLM)",
      "target": "Cloze task",
      "description": "The masked language model is inspired by the Cloze task."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Question Answering",
      "description": "BERT is particularly beneficial for token-level tasks such as question answering."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Masked Language Model (MLM)",
      "description": "BERT uses the MLM objective to enable pre-trained deep bidirectional representations."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Unidirectional Language Models",
      "description": "BERT contrasts with unidirectional language models used by Radford et al. (2018)."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Peters et al. (2018)",
      "description": "BERT is contrasted with Peters et al. (2018) which uses a shallow concatenation of language models."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "NLP tasks",
      "description": "BERT advances the state of the art for eleven NLP tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Bidirectional Transformers",
      "description": "BERT is based on the architecture of Bidirectional Transformers."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Pre-training",
      "description": "BERT utilizes pre-training to enhance its performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Task-specific architectures",
      "description": "BERT outperforms many task-specific architectures."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "General Language Representations",
      "description": "BERT extends the research on general language representations."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Word2Vec",
      "description": "BERT builds upon the concepts introduced by Word2Vec."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "GloVe",
      "description": "BERT builds upon the concepts introduced by GloVe."
    },
    {
      "type": "IMPROVES",
      "source": "Pre-trained word embeddings",
      "target": "Embeddings learned from scratch",
      "description": "Pre-trained word embeddings offer significant improvements over embeddings learned from scratch."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Left-to-right language modeling",
      "description": "BERT utilizes left-to-right language modeling objectives for pretraining."
    },
    {
      "type": "EXTENDS",
      "source": "Left-to-right language modeling",
      "target": "Ranking candidate next sentences",
      "description": "Left-to-right language modeling has been generalized to train sentence representations using ranking objectives."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Word2Vec",
      "description": "BERT builds on the concepts introduced by Word2Vec and other neural methods."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "GloVe",
      "description": "BERT builds on the concepts introduced by GloVe and other neural methods."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Sentence embeddings",
      "description": "BERT introduces new methods for generating sentence embeddings."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Paragraph embeddings",
      "description": "BERT introduces new methods for generating paragraph embeddings."
    },
    {
      "type": "IMPROVES",
      "source": "ELMo",
      "target": "NLP Benchmarks",
      "description": "ELMo advances the state of the art for several major NLP benchmarks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Contextual Word Embeddings",
      "description": "BERT integrates contextual word embeddings with existing task-specific architectures."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "ELMo",
      "description": "BERT builds on the concepts introduced by ELMo for contextual representations."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Next Sentence Prediction",
      "description": "BERT uses next sentence prediction as part of its training objectives."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Denoising Autoencoder",
      "description": "BERT employs denoising autoencoder derived objectives in its training."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is based on the transformer architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "ELMo",
      "description": "BERT is compared to ELMo, highlighting its deep bidirectionality."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Cloze Task",
      "description": "BERT utilizes the cloze task to enhance its training process."
    },
    {
      "type": "BUILDS_ON",
      "source": "Sentence/Document Encoders",
      "target": "LSTM",
      "description": "Sentence/document encoders build on the concepts of LSTMs for contextual representations."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Open AI GPT",
      "description": "BERT builds on the advancements made by Open AI GPT in language modeling."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "GLUE benchmark",
      "description": "BERT is evaluated on the GLUE benchmark to demonstrate its performance on sentence-level tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Pre-training",
      "description": "BERT utilizes pre-training techniques to enhance its language understanding capabilities."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT employs fine-tuning to adapt its pre-trained model for specific tasks."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Masked Language Model (MLM)",
      "description": "BERT introduces the masked language model as part of its pre-training objectives."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Next Sentence Prediction (NSP)",
      "description": "BERT introduces next sentence prediction as a pre-training task."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "SQuAD",
      "description": "BERT is evaluated on the SQuAD dataset for question answering tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "MNLI",
      "description": "BERT is evaluated on the MNLI dataset for natural language inference tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT builds on the transformer architecture to achieve its language understanding capabilities."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Deep Bidirectional Transformers",
      "description": "BERT is based on the architecture of deep bidirectional transformers."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Transfer Learning",
      "description": "BERT employs transfer learning techniques for its pre-training and fine-tuning phases."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transfer Learning",
      "target": "Natural Language Inference",
      "description": "Transfer learning has been shown to be effective in tasks like natural language inference."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transfer Learning",
      "target": "Machine Translation",
      "description": "Transfer learning has also been effective in machine translation tasks."
    },
    {
      "type": "USES",
      "source": "Transfer Learning",
      "target": "ImageNet",
      "description": "Transfer learning techniques in computer vision often utilize models pre-trained on ImageNet."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT's architecture is based on the Transformer model."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Pre-training",
      "description": "BERT employs pre-training on unlabeled data before fine-tuning."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT is fine-tuned using labeled data from downstream tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Downstream tasks",
      "description": "BERT has separate fine-tuned models for each downstream task."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture as described in Vaswani et al. (2017)."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERTBASE",
      "target": "Open AI GPT",
      "description": "BERTBASE was chosen to have the same model size as Open AI GPT for comparison purposes."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Bidirectional Self-Attention",
      "description": "BERT uses bidirectional self-attention to process input data."
    },
    {
      "type": "USES",
      "source": "Open AI GPT",
      "target": "Constrained Self-Attention",
      "description": "Open AI GPT uses constrained self-attention to process input data."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Bidirectional Self-Attention",
      "description": "BERT uses bidirectional self-attention to process language."
    },
    {
      "type": "USES",
      "source": "GPT",
      "target": "Constrained Self-Attention",
      "description": "GPT uses constrained self-attention for its language processing."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "GPT",
      "description": "The paper compares BERT's bidirectional self-attention with GPT's constrained self-attention."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "GPT",
      "target": "Transformer",
      "description": "GPT is also built on the Transformer architecture."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Deep Bidirectional Transformers",
      "description": "BERT introduces the use of deep bidirectional transformers for language understanding."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Word Piece embeddings",
      "description": "BERT uses Word Piece embeddings to represent input text."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "[CLS]",
      "description": "BERT uses the [CLS] token as the first token in input sequences for classification."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "[SEP]",
      "description": "BERT uses the [SEP] token to differentiate between sentences in a sequence."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Masked LM",
      "description": "BERT introduces the Masked LM task as part of its pre-training process."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "[SEP]",
      "description": "BERT uses the [SEP] token to differentiate between sentence pairs."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Input Embedding",
      "description": "BERT constructs input representations using input embeddings."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Hidden Vector",
      "description": "BERT utilizes hidden vectors for the final representation of tokens."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Masked LM",
      "description": "BERT introduces the Masked LM technique for pre-training."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer encoder",
      "description": "BERT builds on the Transformer encoder architecture for its bidirectional representation."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Conditional language models",
      "description": "BERT compares its bidirectional approach to standard conditional language models."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Bidirectional conditioning",
      "description": "BERT uses bidirectional conditioning to enhance its predictive capabilities."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Masked Language Model (MLM)",
      "description": "BERT introduces the masked language model technique for training."
    },
    {
      "type": "COMPARES_TO",
      "source": "Masked Language Model (MLM)",
      "target": "Cloze task",
      "description": "The MLM technique is often referred to as a Cloze task in the literature."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Word Piece tokens",
      "description": "BERT uses Word Piece tokens for input sequences."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Denoising auto-encoders",
      "description": "BERT improves upon denoising auto-encoders by predicting only masked words."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Masked Language Modeling",
      "description": "BERT uses masked language modeling as part of its training process."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Next Sentence Prediction (NSP)",
      "description": "BERT employs next sentence prediction to understand relationships between sentences."
    },
    {
      "type": "IMPROVES",
      "source": "Masked Language Modeling",
      "target": "Cross Entropy Loss",
      "description": "Masked language modeling aims to improve the prediction accuracy measured by cross entropy loss."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT builds on the transformer architecture to achieve its language understanding capabilities."
    },
    {
      "type": "IMPROVES",
      "source": "Binarized Next Sentence Prediction",
      "target": "Question Answering (QA)",
      "description": "Pre-training on NSP is beneficial for improving performance in QA tasks."
    },
    {
      "type": "IMPROVES",
      "source": "Binarized Next Sentence Prediction",
      "target": "Natural Language Inference (NLI)",
      "description": "Pre-training on NSP enhances the model's ability to perform NLI."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Next Sentence Prediction (NSP)",
      "description": "BERT utilizes NSP as part of its pre-training process."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Books Corpus",
      "description": "BERT uses the Books Corpus for its pre-training data."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "English Wikipedia",
      "description": "BERT uses English Wikipedia for its pre-training data."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Representation Learning",
      "description": "BERT improves upon traditional representation learning by transferring all parameters to downstream tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Jernite et al. (2017)",
      "description": "BERT's NSP task is compared to representation-learning objectives used in prior work."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Logeswaran and Lee (2018)",
      "description": "BERT's NSP task is compared to representation-learning objectives used in prior work."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Books Corpus",
      "description": "BERT uses the Books Corpus for pre-training."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "English Wikipedia",
      "description": "BERT uses English Wikipedia for pre-training."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Billion Word Benchmark",
      "description": "BERT is compared to the Billion Word Benchmark, which is less effective for certain tasks."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT improves the fine-tuning process by using a self-attention mechanism."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Bidirectional cross attention",
      "description": "BERT extends the concept of bidirectional cross attention by integrating it with self-attention."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Self-Attention Mechanism",
      "description": "BERT uses the self-attention mechanism to unify the encoding of sentence pairs."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Bidirectional Cross Attention",
      "description": "BERT's self-attention mechanism is compared to bidirectional cross attention methods."
    },
    {
      "type": "PLUGS_IN",
      "source": "BERT",
      "target": "Task-Specific Inputs and Outputs",
      "description": "BERT allows for task-specific inputs and outputs to be plugged in for fine-tuning."
    },
    {
      "type": "FEEDS_INTO",
      "source": "Token Representations",
      "target": "Output Layer",
      "description": "Token representations are fed into an output layer for token-level tasks."
    },
    {
      "type": "FEEDS_INTO",
      "source": "[CLS] Representation",
      "target": "Output Layer",
      "description": "[CLS] representation is fed into an output layer for classification tasks."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Deep Bidirectional Transformers",
      "description": "BERT introduces the use of deep bidirectional transformers for language understanding."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "CLS representation",
      "description": "BERT uses the [CLS] representation for classification tasks."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "GLUE",
      "description": "The paper evaluates BERT's performance on the GLUE benchmark."
    },
    {
      "type": "IMPROVES",
      "source": "Fine-tuning",
      "target": "BERT",
      "description": "Fine-tuning improves BERT's performance on specific NLP tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built upon the Transformer architecture."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "GLUE",
      "description": "BERT is fine-tuned and evaluated on the GLUE benchmark."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "F1 score",
      "description": "BERT achieves a high F1 score on the GLUE benchmark."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT introduces a fine-tuning approach for language tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Open AI GPT",
      "description": "BERT is compared to Open AI GPT in terms of performance on various tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Bi LSTM+ELMo+Attn",
      "description": "BERT is compared to Bi LSTM+ELMo+Attn regarding their performance metrics."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "GLUE",
      "description": "BERT is evaluated on the GLUE benchmark dataset."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT uses the Transformer architecture for its model design."
    },
    {
      "type": "IMPROVES",
      "source": "BERTBASE",
      "target": "accuracy",
      "description": "BERTBASE achieves a 4.5% average accuracy improvement over the prior state of the art."
    },
    {
      "type": "IMPROVES",
      "source": "BERTLARGE",
      "target": "accuracy",
      "description": "BERTLARGE achieves a 7.0% average accuracy improvement over the prior state of the art."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERTBASE",
      "target": "Open AI GPT",
      "description": "BERTBASE and Open AI GPT are nearly identical in terms of model architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "fine-tuning",
      "description": "BERT utilizes fine-tuning to adapt to specific tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "GLUE",
      "description": "BERT obtains a 4.6% absolute accuracy improvement on the MNLI task of the GLUE benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERTLARGE",
      "target": "Open AI GPT",
      "description": "BERTLARGE obtains a score of 80.5 on the GLUE leaderboard compared to Open AI GPT's score of 72.8."
    },
    {
      "type": "IMPROVES",
      "source": "BERTLARGE",
      "target": "BERTBASE",
      "description": "BERTLARGE significantly outperforms BERTBASE across all tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "GLUE",
      "description": "BERT is evaluated using the GLUE dataset to assess its performance on language understanding tasks."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Question Answering",
      "description": "BERT introduces a method for performing question answering by representing input as a packed sequence."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Deep Bidirectional Transformers",
      "description": "BERT improves the capabilities of deep bidirectional transformers for language understanding."
    },
    {
      "type": "USES",
      "source": "Question Answering",
      "target": "Softmax",
      "description": "The question answering task uses the softmax function to compute probabilities for answer span positions."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "SQuAD",
      "description": "BERT is fine-tuned on the SQuAD dataset for question answering tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Trivia QA",
      "description": "BERT is initially fine-tuned on the Trivia QA dataset before fine-tuning on SQuAD."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "SQuAD",
      "description": "BERT outperforms the top leaderboard system by +1.5 F1 in ensembling."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Trivia QA",
      "description": "BERT fine-tunes on the Trivia QA dataset."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "SQuAD",
      "description": "BERT fine-tunes on the SQuAD dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "top leaderboard system",
      "description": "BERT outperforms the top leaderboard system by +1.5 F1 in ensembling."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "top ensemble system",
      "description": "BERT outperforms the top ensemble system in terms of F1 score."
    },
    {
      "type": "IMPROVES",
      "source": "QANet",
      "target": "QANet (post-publication)",
      "description": "QANet has improved substantially after publication."
    },
    {
      "type": "IMPROVES",
      "source": "BERTLARGE",
      "target": "BERTBASE",
      "description": "BERTLARGE improves upon BERTBASE by having more parameters and better performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "nlnet",
      "description": "BERT is compared to nlnet in terms of performance on language understanding tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "QANet",
      "description": "BERT is compared to QANet in terms of performance on language understanding tasks."
    },
    {
      "type": "USES",
      "source": "BERTLARGE (Ensemble)",
      "target": "BERTLARGE",
      "description": "The BERTLARGE ensemble uses multiple BERTLARGE models with different pre-training checkpoints."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "SQuAD 1.1",
      "description": "BERT is evaluated on the SQuAD 1.1 dataset."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "SQuAD 2.0",
      "description": "BERT is evaluated on the SQuAD 2.0 dataset."
    },
    {
      "type": "EXTENDS",
      "source": "SQuAD 2.0",
      "target": "SQuAD 1.1",
      "description": "SQuAD 2.0 extends the problem definition of SQuAD 1.1 by allowing for no short answer."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "SQuAD 2.0",
      "description": "BERT is used to model the SQuAD 2.0 task."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERTLARGE",
      "target": "existing systems",
      "description": "BERTLARGE outperforms all existing systems by a wide margin."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "F1 Score",
      "description": "BERT improves the F1 score with minimal loss in tuning data."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "OpenAI GPT",
      "description": "BERT's performance is compared to OpenAI GPT in terms of accuracy."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "ESIM",
      "description": "BERT's performance is compared to ESIM in terms of accuracy."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "BERTBASE",
      "description": "BERT's performance is compared to BERTBASE in terms of accuracy."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "BERTLARGE",
      "description": "BERT's performance is compared to BERTLARGE in terms of accuracy."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Trivia QA",
      "description": "BERT utilizes the Trivia QA dataset for training and evaluation."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "F1 Score",
      "description": "BERT achieves a +5.1 F1 improvement over the previous best system."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "SWAG",
      "description": "BERT is fine-tuned on the SWAG dataset to evaluate grounded common-sense inference."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT introduces fine-tuning as a method to adapt the model for specific tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "ESIM+ELMo",
      "description": "BERT outperforms the ESIM+ELMo system by +27.1%."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Open AI GPT",
      "description": "BERT outperforms Open AI GPT by +8.3%."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT employs fine-tuning to adapt the model for specific tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Deep Bidirectional Transformers",
      "description": "BERT is built on the architecture of deep bidirectional transformers."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Masked Language Model (MLM)",
      "description": "BERT uses the masked language model as one of its pre-training objectives."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Next Sentence Prediction (NSP)",
      "description": "BERT extends the concept of language modeling by incorporating next sentence prediction."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERTBASE",
      "target": "No NSP",
      "description": "BERTBASE is compared to a model trained without the next sentence prediction task."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERTBASE",
      "target": "LTR & No NSP",
      "description": "BERTBASE is compared to a left-to-right language model trained without next sentence prediction."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERTBASE",
      "target": "+ Bi LSTM",
      "description": "BERTBASE is compared to a model that adds a Bi LSTM on top of the left-to-right model."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Open AI GPT",
      "description": "BERT is compared to Open AI GPT in terms of training dataset size, input representation, and fine-tuning scheme."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "QNLI",
      "description": "BERT shows improved performance on the QNLI dataset when using the NSP task."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "MNLI",
      "description": "BERT shows improved performance on the MNLI dataset when using the NSP task."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "SQuAD 1.1",
      "description": "BERT shows improved performance on the SQuAD 1.1 dataset when using the NSP task."
    },
    {
      "type": "COMPARES_TO",
      "source": "No NSP",
      "target": "LTR & No NSP",
      "description": "The performance of the No NSP model is compared to the LTR & No NSP model."
    },
    {
      "type": "IMPROVES",
      "source": "BiLSTM",
      "target": "SQuAD",
      "description": "Adding a BiLSTM on top of the LTR model significantly improves results on SQuAD."
    },
    {
      "type": "COMPARES_TO",
      "source": "LTR",
      "target": "MLM",
      "description": "The LTR model performs worse than the MLM model on all tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "NSP",
      "description": "BERT uses Next Sentence Prediction as part of its training."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "MLM",
      "description": "BERT uses Masked Language Model as part of its training."
    },
    {
      "type": "COMPARES_TO",
      "source": "LTR",
      "target": "BiLSTM",
      "description": "The LTR model's performance is compared to that of the BiLSTM."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Dev Set accuracy",
      "description": "Larger BERT models lead to strict accuracy improvements across tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Bidirectional Model",
      "description": "BERT utilizes a deep bidirectional model architecture to capture context."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "ELMo",
      "description": "BERT is compared to ELMo, highlighting its advantages in terms of model power and efficiency."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "GLUE",
      "description": "The performance of BERT is evaluated on the GLUE benchmark."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "MRPC",
      "description": "BERT's performance is also evaluated on the MRPC dataset."
    },
    {
      "type": "IMPROVES",
      "source": "BERT BASE",
      "target": "accuracy",
      "description": "BERT BASE leads to accuracy improvements across datasets."
    },
    {
      "type": "IMPROVES",
      "source": "BERT LARGE",
      "target": "accuracy",
      "description": "BERT LARGE leads to accuracy improvements across datasets."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT BASE",
      "target": "Transformer (Vaswani et al. 2017)",
      "description": "BERT BASE is compared to the largest Transformer from Vaswani et al. in terms of parameters."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT LARGE",
      "target": "Transformer (Al-Rfou et al. 2018)",
      "description": "BERT LARGE is compared to the largest Transformer from Al-Rfou et al. in terms of parameters."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "Model size",
      "target": "BERT",
      "description": "Increasing model size leads to improvements in performance on various tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Pre-training",
      "description": "BERT utilizes pre-training to enhance its performance on language tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Peters et al. (2018)",
      "description": "BERT's findings are compared to the results presented by Peters et al. regarding model scaling."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT uses the fine-tuning approach to adapt to downstream tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Fine-tuning",
      "target": "Feature-based Approach",
      "description": "Fine-tuning is compared to the feature-based approach in terms of model performance."
    },
    {
      "type": "IMPROVES",
      "source": "Hidden Dimension Size",
      "target": "bi-LM",
      "description": "Increasing the hidden dimension size from 200 to 600 improved the performance of the bi-LM."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "bi-LM",
      "description": "BERT builds on the concepts established by prior works on bi-LMs."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "CoNLL-2003",
      "description": "BERT is applied to the CoNLL-2003 dataset for the Named Entity Recognition task."
    },
    {
      "type": "COMPARES_TO",
      "source": "Feature-based approach",
      "target": "Transformer encoder",
      "description": "The feature-based approach is compared to the Transformer encoder architecture."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Accuracy",
      "description": "BERT aims to improve the accuracy of language understanding tasks."
    },
    {
      "type": "FORMULATES",
      "source": "NER task",
      "target": "Tagging task",
      "description": "The NER task is formulated as a tagging task."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is based on the transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT employs fine-tuning to adapt to specific tasks."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "MNLI",
      "description": "BERT is evaluated on the MNLI dataset."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "MRPC",
      "description": "BERT is evaluated on the MRPC dataset."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "SST-2",
      "description": "BERT is evaluated on the SST-2 dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "ELMo",
      "description": "BERT's performance is compared to ELMo."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "CVT",
      "description": "BERT's performance is compared to CVT."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "CSE",
      "description": "BERT's performance is compared to CSE."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "CoNLL-2003",
      "description": "BERT is evaluated on the CoNLL-2003 dataset for named entity recognition."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT improves performance through fine-tuning on specific tasks."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Feature-based approach",
      "description": "BERT extends the feature-based approach by providing contextual embeddings."
    },
    {
      "type": "USES",
      "source": "Feature-based approach",
      "target": "Bi LSTM",
      "description": "The feature-based approach uses a Bi LSTM for classification."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT improves the fine-tuning process for language understanding tasks."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Feature-based approaches",
      "description": "BERT enhances feature-based approaches by providing effective token representations."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Deep Bidirectional Architecture",
      "description": "BERT builds on the concept of deep bidirectional architectures for better context understanding."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Unsupervised Pre-training",
      "description": "BERT utilizes unsupervised pre-training to enhance its language understanding capabilities."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT LARGE",
      "target": "state-of-the-art methods",
      "description": "BERT LARGE is compared to state-of-the-art methods and performs competitively."
    },
    {
      "type": "EXTENDS",
      "source": "Transfer Learning",
      "target": "NLP tasks",
      "description": "Transfer learning extends its benefits to various NLP tasks through models like BERT."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Contextual string embeddings",
      "description": "BERT utilizes contextual string embeddings for improved language understanding."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Character-level language modeling",
      "description": "BERT extends the concept of character-level language modeling by incorporating deeper self-attention."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "PASCAL recognizing textual entailment challenge",
      "description": "BERT is evaluated on the PASCAL recognizing textual entailment challenge."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Domain adaptation",
      "description": "BERT improves domain adaptation techniques through its architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Annotated Corpus",
      "description": "BERT utilizes a large annotated corpus for training."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "Semeval-2017",
      "description": "BERT is evaluated using the Semeval-2017 dataset for semantic textual similarity."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Class-based n-gram models",
      "description": "BERT is compared to class-based n-gram models in terms of performance."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is based on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Pre-training",
      "description": "BERT employs pre-training techniques to enhance its language understanding capabilities."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "Quora question pairs",
      "description": "BERT is evaluated on the Quora question pairs dataset to assess its performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "One billion word benchmark",
      "description": "BERT's performance is compared against the one billion word benchmark for language modeling."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Deep Neural Networks",
      "description": "BERT builds on the principles of deep neural networks for language understanding."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Natural Language Inference Data",
      "description": "BERT uses natural language inference data for supervised learning."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Universal Sentence Representations",
      "description": "BERT extends the concept of universal sentence representations through its architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Deep Neural Networks",
      "description": "BERT is compared to traditional deep neural networks in terms of performance on language tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Distributed Representations",
      "description": "BERT uses distributed representations for understanding language."
    },
    {
      "type": "IMPROVES",
      "source": "MaskGAN",
      "target": "Text Generation",
      "description": "MaskGAN improves text generation by filling in missing text."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Universal Language Model",
      "description": "BERT is compared to the Universal Language Model in terms of performance."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Universal Language Model Fine-tuning",
      "description": "BERT improves upon the technique of universal language model fine-tuning for text classification."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Reinforced Mnemonic Reader",
      "description": "BERT can be utilized in conjunction with the reinforced mnemonic reader for enhanced comprehension."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Discourse-based Objectives",
      "description": "BERT is compared to discourse-based objectives for sentence representation learning."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "TriviaQA",
      "description": "BERT utilizes the TriviaQA dataset for training and evaluation."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Skip-thought vectors",
      "description": "BERT is compared to skip-thought vectors in terms of performance on language tasks."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Distributed representations",
      "description": "BERT introduces the concept of distributed representations for sentences."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Winograd Schema Challenge",
      "description": "BERT extends the evaluation of models on the Winograd Schema Challenge."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Sentence representations framework",
      "description": "BERT improves upon existing frameworks for learning sentence representations."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Contextualized word vectors",
      "description": "BERT utilizes contextualized word vectors for improved language understanding."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Bidirectional LSTM",
      "description": "BERT extends the concept of bidirectional LSTMs by using the Transformer architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Distributed representations",
      "description": "BERT compares its performance to traditional distributed representations of words."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Hierarchical distributed language model",
      "description": "BERT introduces new methods that build upon hierarchical distributed language models."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Decomposable attention",
      "description": "BERT improves upon decomposable attention mechanisms by integrating them into its architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Bidirectional Language Model",
      "description": "BERT improves upon traditional bidirectional language models by using a transformer-based architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "GloVe",
      "description": "BERT utilizes word embeddings similar to those generated by GloVe."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Decomposable Attention Model",
      "description": "BERT is compared to the decomposable attention model in terms of performance on natural language inference tasks."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Semi-supervised Sequence Tagging",
      "description": "BERT extends the concept of semi-supervised sequence tagging by leveraging pre-training."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Contextualized Word Representations",
      "description": "BERT introduces a new approach to contextualized word representations."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Unsupervised Learning",
      "description": "BERT improves language understanding using unsupervised learning techniques."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Bidirectional Attention Flow",
      "description": "BERT is compared to the Bidirectional Attention Flow model for machine comprehension."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "SQuAD",
      "description": "BERT extends the capabilities of models evaluated on the SQuAD dataset."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Pre-training",
      "description": "BERT uses pre-training to enhance its language understanding capabilities."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Named Entity Recognition",
      "description": "BERT introduces advancements in named entity recognition tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Attention Mechanism",
      "description": "BERT utilizes attention mechanisms for processing language."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "GLUE",
      "description": "BERT is evaluated on the GLUE benchmark."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Deep Bidirectional Transformers",
      "description": "BERT is built on the architecture of deep bidirectional transformers."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Multi-granularity hierarchical attention fusion networks",
      "description": "BERT utilizes techniques from multi-granularity hierarchical attention for its reading comprehension tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Google's Neural Machine Translation System",
      "description": "BERT is compared to Google's neural machine translation system in terms of performance on language tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Deep Bidirectional Transformers",
      "description": "BERT is built on the architecture of deep bidirectional transformers."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "QANet",
      "description": "BERT is compared to QANet in terms of performance on reading comprehension tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Neural Machine Translation",
      "description": "BERT utilizes techniques from neural machine translation for language understanding."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "SWAG",
      "description": "BERT introduces the SWAG dataset for evaluating grounded commonsense inference."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Pre-training",
      "description": "BERT utilizes pre-training to enhance its language understanding capabilities."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Ablation Study",
      "description": "BERT extends the concept of ablation studies to analyze its performance."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Training Steps",
      "description": "BERT's performance is influenced by the number of training steps."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Masking Procedures",
      "description": "BERT's effectiveness is enhanced by different masking procedures during training."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Masked LM",
      "description": "BERT introduces the Masked LM technique as part of its pre-training tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Masking Procedure",
      "description": "BERT uses the Masking Procedure to implement the Masked LM task."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Pre-training Tasks",
      "description": "BERT extends the concept of pre-training tasks by incorporating the Masked LM technique."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Masked Language Modeling",
      "description": "BERT introduces the technique of masked language modeling for training."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT uses the Transformer architecture to enhance language understanding."
    },
    {
      "type": "IMPROVES",
      "source": "Masked Language Modeling",
      "target": "Language Understanding Capability",
      "description": "Masked language modeling improves the model's language understanding capability."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Bidirectional Transformer",
      "description": "BERT uses a bidirectional Transformer architecture to process language."
    },
    {
      "type": "USES",
      "source": "Open AI GPT",
      "target": "Left-to-Right Transformer",
      "description": "Open AI GPT uses a left-to-right Transformer architecture for its predictions."
    },
    {
      "type": "USES",
      "source": "ELMo",
      "target": "LSTM",
      "description": "ELMo uses LSTMs to generate features for downstream tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Open AI GPT",
      "description": "BERT and Open AI GPT are both fine-tuning approaches, but differ in their architectures."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "ELMo",
      "description": "BERT is a fine-tuning approach while ELMo is a feature-based approach."
    },
    {
      "type": "IMPROVES",
      "source": "MLM",
      "target": "Left-to-Right Model",
      "description": "MLM converges marginally slower than a left-to-right model but provides empirical improvements."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Masked Language Model (MLM)",
      "description": "BERT uses the MLM technique for training."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Next Sentence Prediction",
      "description": "BERT employs the Next Sentence Prediction task as part of its training."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Bidirectional Transformers",
      "description": "BERT is built on the architecture of Bidirectional Transformers."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Next Sentence Prediction",
      "description": "BERT uses the next sentence prediction task during its training process."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Word Piece Tokenization",
      "description": "BERT applies word piece tokenization for processing input text."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Deep Bidirectional Transformers",
      "description": "BERT improves the capabilities of deep bidirectional transformers for language understanding."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Adam",
      "description": "BERT uses the Adam optimizer for training."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Dropout",
      "description": "BERT employs dropout as a regularization technique."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "GELU",
      "description": "BERT uses the GELU activation function in its architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Dropout",
      "description": "BERT uses dropout with a probability of 0.1 on all layers."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "GELU Activation",
      "description": "BERT uses GELU activation instead of the standard ReLU."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT is built on the Transformer architecture."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "Masked LM Likelihood",
      "description": "The training loss of BERT includes the mean masked LM likelihood."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "Next Sentence Prediction Likelihood",
      "description": "The training loss of BERT includes the mean next sentence prediction likelihood."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Learning Rate Warmup",
      "description": "BERT employs learning rate warmup over the first 10,000 steps."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Linear Decay",
      "description": "BERT uses linear decay of the learning rate after the warmup phase."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Cloud TPU",
      "description": "BERT training was performed on Cloud TPUs."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT employs fine-tuning to adapt the model to specific tasks."
    },
    {
      "type": "IMPROVES",
      "source": "Fine-tuning",
      "target": "BERT",
      "description": "Fine-tuning enhances the performance of BERT on specific tasks."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Positional Embeddings",
      "description": "BERT introduces positional embeddings to help the model understand the order of tokens."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Dropout Probability",
      "description": "BERT uses a dropout probability of 0.1 to mitigate overfitting."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Deep Bidirectional Transformers",
      "description": "BERT is built on the architecture of deep bidirectional transformers."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "ELMo",
      "description": "BERT is compared to ELMo in terms of representation learning approaches."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Open AI GPT",
      "description": "BERT is compared to Open AI GPT regarding their pre-training methods."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT utilizes the Transformer architecture for its model."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT improves the fine-tuning process for language understanding tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Open AI GPT",
      "description": "BERT is compared to Open AI GPT to highlight differences in training and architecture."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Books Corpus",
      "description": "BERT uses the Books Corpus for training."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Wikipedia",
      "description": "BERT uses Wikipedia for training."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "[SEP]",
      "description": "BERT introduces the [SEP] token during pre-training."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "[CLS]",
      "description": "BERT introduces the [CLS] token during pre-training."
    },
    {
      "type": "BUILDS_ON",
      "source": "BERT",
      "target": "Transformer",
      "description": "BERT builds on the Transformer architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "GPT",
      "description": "BERT is compared to GPT in terms of training steps and batch sizes."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "Language Understanding",
      "description": "BERT improves language understanding through its pre-training tasks and bidirectionality."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT employs fine-tuning to adapt to specific tasks."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "[CLS] and [SEP] tokens",
      "description": "BERT introduces special tokens for classification and separation during fine-tuning."
    },
    {
      "type": "EXTENDS",
      "source": "BERT",
      "target": "Bidirectional Transformers",
      "description": "BERT extends the capabilities of bidirectional transformers for language tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "[CLS]",
      "description": "BERT uses the [CLS] symbol for classification output."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "[SEP]",
      "description": "BERT uses the [SEP] symbol to separate non-consecutive token sequences."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "GLUE",
      "description": "BERT is evaluated on the GLUE benchmark to assess its performance on various language tasks."
    },
    {
      "type": "INCLUDES",
      "source": "GLUE",
      "target": "MNLI",
      "description": "The GLUE benchmark includes the MNLI dataset for evaluating entailment classification."
    },
    {
      "type": "USES",
      "source": "MNLI",
      "target": "entailment classification",
      "description": "MNLI is used for entailment classification tasks."
    },
    {
      "type": "USES",
      "source": "QQP",
      "target": "binary classification",
      "description": "QQP is used for binary classification tasks."
    },
    {
      "type": "USES",
      "source": "QNLI",
      "target": "binary classification",
      "description": "QNLI is used for binary classification tasks."
    },
    {
      "type": "EXTENDS",
      "source": "QNLI",
      "target": "Stanford Question Answering Dataset",
      "description": "QNLI extends the Stanford Question Answering Dataset by converting it to a binary classification task."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT introduces the technique of fine-tuning for various language understanding tasks."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "SST-2",
      "description": "BERT is used to evaluate performance on the SST-2 dataset."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "CoLA",
      "description": "BERT is used to evaluate performance on the CoLA dataset."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "STS-B",
      "description": "BERT uses the STS-B dataset for evaluating semantic similarity."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "MRPC",
      "description": "BERT uses the MRPC dataset for evaluating paraphrase detection."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "RTE",
      "description": "BERT uses the RTE dataset for evaluating textual entailment."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "WNLI",
      "description": "BERT uses the WNLI dataset for evaluating natural language inference."
    },
    {
      "type": "COMPARES_TO",
      "source": "WNLI",
      "target": "GLUE",
      "description": "WNLI is compared to GLUE due to issues with its construction."
    },
    {
      "type": "IMPROVES",
      "source": "Multi-task training",
      "target": "RTE",
      "description": "Multi-task training with MNLI observed substantial improvements on RTE."
    },
    {
      "type": "EXCLUDES",
      "source": "GLUE submission",
      "target": "WNLI",
      "description": "WNLI is excluded from the GLUE submission to be fair to Open AI GPT."
    },
    {
      "type": "IMPROVES",
      "source": "BERT",
      "target": "MNLI",
      "description": "BERT achieves higher accuracy on the MNLI dataset with increased training steps."
    },
    {
      "type": "COMPARES_TO",
      "source": "MLM",
      "target": "LTR",
      "description": "MLM converges slightly slower than LTR but begins to outperform it in terms of accuracy."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Masked Language Model (MLM)",
      "description": "BERT uses the MLM objective for pre-training."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "MNLI",
      "description": "BERT's performance is evaluated on the MNLI dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Masked Language Model (MLM)",
      "target": "Fine-tuning",
      "description": "The masking strategies aim to reduce the mismatch between pre-training and fine-tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "BERT",
      "target": "Feature-based approaches",
      "description": "BERT's performance is compared to feature-based approaches for NER."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Fine-tuning",
      "description": "BERT employs fine-tuning to adapt the model to specific tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Fine-tuning",
      "target": "Feature-based approach",
      "description": "The performance of fine-tuning is compared to that of feature-based approaches."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "MNLI",
      "description": "BERT's performance is evaluated on the MNLI dataset."
    },
    {
      "type": "EVALUATES",
      "source": "BERT",
      "target": "NER",
      "description": "BERT's performance is evaluated on the NER task."
    },
    {
      "type": "INTRODUCES",
      "source": "BERT",
      "target": "Masking Strategies",
      "description": "BERT introduces various masking strategies for masked language model pre-training."
    },
    {
      "type": "USES",
      "source": "BERT",
      "target": "Masked Language Modeling (MLM)",
      "description": "BERT uses MLM as a pre-training strategy."
    },
    {
      "type": "IMPROVES",
      "source": "Feature-based approach",
      "target": "BERT",
      "description": "The feature-based approach improves the performance of BERT for downstream tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "MASK strategy",
      "target": "RND strategy",
      "description": "The MASK strategy is compared to the RND strategy, which performs worse."
    },
    {
      "type": "COMPARES_TO",
      "source": "Feature-based approach",
      "target": "MASK strategy",
      "description": "The feature-based approach is compared to the MASK strategy in the context of NER."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model as a new architecture for sequence transduction."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Mechanism",
      "description": "The Transformer model uses attention mechanisms as its core component."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Sequence Transduction Models",
      "description": "The Transformer improves upon traditional sequence transduction models by eliminating the need for recurrence and convolutions."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "The paper introduces the Transformer model."
    },
    {
      "type": "EVALUATES",
      "source": "Attention Is All You Need",
      "target": "WMT 2014 English-to-German",
      "description": "The paper evaluates the Transformer model on the WMT 2014 English-to-German translation task."
    },
    {
      "type": "EVALUATES",
      "source": "Attention Is All You Need",
      "target": "WMT 2014 English-to-French",
      "description": "The paper evaluates the Transformer model on the WMT 2014 English-to-French translation task."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Mechanism",
      "description": "The Transformer model uses attention mechanisms as its core component."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "WMT 2014 English-to-German",
      "description": "The Transformer model improves over existing best results on the WMT 2014 English-to-German translation task."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "WMT 2014 English-to-French",
      "description": "The Transformer model establishes a new state-of-the-art BLEU score on the WMT 2014 English-to-French translation task."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model architecture."
    },
    {
      "type": "PROPOSES",
      "source": "Attention Is All You Need",
      "target": "Self-Attention",
      "description": "The paper proposes the use of self-attention as a replacement for RNNs."
    },
    {
      "type": "PROPOSES",
      "source": "Attention Is All You Need",
      "target": "Scaled Dot-Product Attention",
      "description": "The paper proposes the scaled dot-product attention mechanism."
    },
    {
      "type": "PROPOSES",
      "source": "Attention Is All You Need",
      "target": "Multi-Head Attention",
      "description": "The paper proposes the multi-head attention mechanism."
    },
    {
      "type": "PROPOSES",
      "source": "Attention Is All You Need",
      "target": "Position Representation",
      "description": "The paper proposes a parameter-free position representation."
    },
    {
      "type": "USES",
      "source": "Attention Is All You Need",
      "target": "Tensor2Tensor",
      "description": "The paper utilizes the Tensor2Tensor codebase for implementing and evaluating model variants."
    },
    {
      "type": "IMPROVES",
      "source": "Tensor2Tensor",
      "target": "Transformer",
      "description": "The redesign of Tensor2Tensor led to improved results and accelerated research for the Transformer model."
    },
    {
      "type": "IMPROVES",
      "source": "Recurrent Neural Networks",
      "target": "Long Short-Term Memory (LSTM)",
      "description": "LSTMs improve upon traditional recurrent neural networks by addressing the vanishing gradient problem."
    },
    {
      "type": "IMPROVES",
      "source": "Recurrent Neural Networks",
      "target": "Gated Recurrent Neural Networks",
      "description": "Gated RNNs enhance the capabilities of standard RNNs by introducing gating mechanisms."
    },
    {
      "type": "USES",
      "source": "Encoder-Decoder Architecture",
      "target": "Recurrent Neural Networks",
      "description": "Encoder-decoder architectures often utilize recurrent neural networks for processing sequences."
    },
    {
      "type": "COMPARES_TO",
      "source": "Recurrent Neural Networks",
      "target": "Sequence Modeling",
      "description": "Recurrent neural networks are a common approach used in sequence modeling tasks."
    },
    {
      "type": "EXTENDS",
      "source": "Long Short-Term Memory (LSTM)",
      "target": "Gated Recurrent Neural Networks",
      "description": "Gated RNNs extend the capabilities of LSTMs by providing additional control over information flow."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model architecture."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Mechanism",
      "description": "The Transformer model uses an attention mechanism to draw global dependencies."
    },
    {
      "type": "IMPROVES",
      "source": "Factorization Tricks",
      "target": "Computational Efficiency",
      "description": "Factorization tricks improve computational efficiency in model training."
    },
    {
      "type": "IMPROVES",
      "source": "Conditional Computation",
      "target": "Model Performance",
      "description": "Conditional computation improves model performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Attention Mechanism",
      "target": "Recurrent Network",
      "description": "Attention mechanisms are compared to recurrent networks, as they are often used in conjunction."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Translation Quality",
      "description": "The Transformer model achieves a new state of the art in translation quality."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Extended Neural GPU",
      "description": "The Transformer builds on the goal of reducing sequential computation established by the Extended Neural GPU."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Byte Net",
      "description": "The Transformer builds on the principles of parallel computation used in Byte Net."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Conv S2S",
      "description": "The Transformer builds on the concepts of convolutional neural networks used in Conv S2S."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Self-attention",
      "description": "The Transformer architecture improves the self-attention mechanism by reducing the number of operations needed to learn dependencies."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Multi-Head Attention",
      "description": "The Transformer uses Multi-Head Attention to counteract the effects of reduced effective resolution."
    },
    {
      "type": "BUILDS_ON",
      "source": "End-to-end memory networks",
      "target": "Recurrent attention mechanism",
      "description": "End-to-end memory networks build on the concept of a recurrent attention mechanism."
    },
    {
      "type": "USES",
      "source": "Self-attention",
      "target": "Reading comprehension",
      "description": "Self-attention is used in reading comprehension tasks."
    },
    {
      "type": "USES",
      "source": "Self-attention",
      "target": "Abstractive summarization",
      "description": "Self-attention is used in abstractive summarization tasks."
    },
    {
      "type": "USES",
      "source": "Self-attention",
      "target": "Textual entailment",
      "description": "Self-attention is used in textual entailment tasks."
    },
    {
      "type": "USES",
      "source": "Self-attention",
      "target": "Learning task-independent sentence representations",
      "description": "Self-attention is used in learning task-independent sentence representation tasks."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model uses self-attention to compute representations."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Encoder-Decoder",
      "description": "The Transformer model builds on the encoder-decoder architecture."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Language Modeling",
      "description": "The Transformer model improves performance on language modeling tasks."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Question Answering",
      "description": "The Transformer model improves performance on question answering tasks."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Auto-regressive generation",
      "description": "The Transformer model employs auto-regressive generation to produce output sequences."
    },
    {
      "type": "GENERATES",
      "source": "Decoder",
      "target": "Output sequence",
      "description": "The decoder generates an output sequence of symbols based on continuous representations."
    },
    {
      "type": "PRODUCES",
      "source": "Continuous representations",
      "target": "Output sequence",
      "description": "Continuous representations serve as input for generating the output sequence."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model employs self-attention mechanisms in its architecture."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Multi-Head Self-Attention",
      "description": "The Transformer model utilizes multi-head self-attention to enhance its encoding capabilities."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Encoder-Decoder Architecture",
      "description": "The Transformer is built on the encoder-decoder architecture, enhancing it with self-attention."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Feed-Forward Network",
      "description": "The Transformer incorporates feed-forward networks in each layer of its encoder and decoder."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Residual Connection",
      "description": "The Transformer model improves training efficiency by using residual connections."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Layer Normalization",
      "description": "The Transformer model enhances training stability through layer normalization."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Multi-Head Attention",
      "description": "The Transformer model employs multi-head attention to enhance its ability to focus on different parts of the input."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Residual Connections",
      "description": "The Transformer architecture incorporates residual connections to improve gradient flow."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Layer Normalization",
      "description": "Layer normalization is utilized in the Transformer to stabilize training."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Masking",
      "description": "Masking is improved in the Transformer to ensure that predictions depend only on known outputs."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Function",
      "description": "The Transformer architecture utilizes the attention function to compute outputs."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Scaled Dot-Product Attention",
      "description": "The paper introduces the Scaled Dot-Product Attention as a key component of the Transformer architecture."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Multi-Head Attention",
      "description": "The paper introduces Multi-Head Attention as a technique that allows the model to jointly attend to information from different representation subspaces."
    },
    {
      "type": "USES",
      "source": "Multi-Head Attention",
      "target": "Scaled Dot-Product Attention",
      "description": "Multi-Head Attention uses Scaled Dot-Product Attention as its underlying mechanism for computing attention."
    },
    {
      "type": "COMPARES_TO",
      "source": "Dot-product Attention",
      "target": "Additive Attention",
      "description": "Both attention mechanisms are compared in terms of performance and efficiency."
    },
    {
      "type": "IMPROVES",
      "source": "Additive Attention",
      "target": "Dot-product Attention",
      "description": "Additive attention outperforms dot-product attention without scaling for larger values of dk."
    },
    {
      "type": "USES",
      "source": "Attention Mechanism",
      "target": "Dot-product Attention",
      "description": "The attention mechanism employs dot-product attention for its computations."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer architecture, which is based on attention mechanisms."
    },
    {
      "type": "USES",
      "source": "Multi-Head Attention",
      "target": "Dot Product",
      "description": "Multi-Head Attention uses the dot product to compute attention scores between queries and keys."
    },
    {
      "type": "IMPROVES",
      "source": "Multi-Head Attention",
      "target": "Softmax Function",
      "description": "Multi-Head Attention improves the application of the softmax function by scaling dot products to prevent small gradients."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Multi-head attention",
      "description": "The Transformer model employs multi-head attention to enhance its ability to process information."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Encoder-decoder attention",
      "description": "The Transformer model utilizes encoder-decoder attention layers for processing queries from the decoder."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Multi-head Attention",
      "description": "The Transformer model employs multi-head attention in its architecture."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Encoder-Decoder",
      "description": "The Transformer architecture is based on the encoder-decoder structure."
    },
    {
      "type": "USES",
      "source": "Encoder-Decoder",
      "target": "Self-Attention",
      "description": "The encoder and decoder in the Transformer utilize self-attention layers."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Sequence-to-Sequence Models",
      "description": "The Transformer mimics typical encoder-decoder attention mechanisms found in sequence-to-sequence models."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model employs self-attention mechanisms to process input sequences."
    },
    {
      "type": "CONTAINS",
      "source": "Transformer",
      "target": "Encoder-Decoder",
      "description": "The Transformer architecture is built upon an encoder-decoder structure."
    },
    {
      "type": "IMPROVES",
      "source": "Scaled Dot-Product Attention",
      "target": "Self-Attention",
      "description": "Scaled dot-product attention enhances the self-attention mechanism by scaling the dot product of queries and keys."
    },
    {
      "type": "USES",
      "source": "Encoder",
      "target": "Position-wise Feed-Forward Networks",
      "description": "The encoder in the Transformer architecture utilizes position-wise feed-forward networks to process each position independently."
    },
    {
      "type": "USES",
      "source": "Decoder",
      "target": "Position-wise Feed-Forward Networks",
      "description": "The decoder in the Transformer architecture also employs position-wise feed-forward networks for processing."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Mechanism",
      "description": "The Transformer model uses attention mechanisms to establish dependencies between input and output."
    },
    {
      "type": "IMPROVES",
      "source": "Learned Embeddings",
      "target": "Next-token probabilities",
      "description": "Learned embeddings improve the prediction of next-token probabilities in the model."
    },
    {
      "type": "BUILDS_ON",
      "source": "Feed Forward Network (FFN)",
      "target": "Transformer",
      "description": "The FFN is a building block of the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Learned Embeddings",
      "description": "The Transformer model uses learned embeddings to convert input and output tokens to vectors."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Softmax Function",
      "description": "The Transformer model uses the softmax function to convert decoder output to predicted next-token probabilities."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Self-Attention",
      "description": "The paper introduces the self-attention mechanism as a core component of the Transformer architecture."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Positional Encoding",
      "description": "The paper introduces positional encoding to allow the model to understand the order of tokens in the absence of recurrence."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer architecture uses self-attention to process input sequences."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Positional Encoding",
      "description": "The Transformer architecture uses positional encoding to maintain the order of tokens."
    },
    {
      "type": "COMPARES_TO",
      "source": "Self-Attention",
      "target": "Recurrent",
      "description": "The paper compares the complexity of self-attention with recurrent mechanisms."
    },
    {
      "type": "COMPARES_TO",
      "source": "Self-Attention",
      "target": "Convolutional",
      "description": "The paper compares the complexity of self-attention with convolutional mechanisms."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Positional Encodings",
      "description": "The Transformer model uses positional encodings to incorporate token position information."
    },
    {
      "type": "INTRODUCES",
      "source": "Transformer",
      "target": "Sine and Cosine Functions",
      "description": "The Transformer introduces the use of sine and cosine functions for generating positional encodings."
    },
    {
      "type": "EXTENDS",
      "source": "Positional Encodings",
      "target": "Encoder-Decoder Stacks",
      "description": "Positional encodings extend the functionality of encoder-decoder stacks by providing positional information."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer architecture, which is based on self-attention mechanisms."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer architecture uses self-attention layers to process input sequences."
    },
    {
      "type": "COMPARES_TO",
      "source": "Self-Attention",
      "target": "Recurrent Layers",
      "description": "The paper compares self-attention layers to recurrent layers in terms of computational complexity and parallelization."
    },
    {
      "type": "COMPARES_TO",
      "source": "Self-Attention",
      "target": "Convolutional Layers",
      "description": "The paper also compares self-attention layers to convolutional layers regarding their effectiveness in sequence transduction."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Computational Complexity",
      "description": "The Transformer architecture improves computational complexity by allowing for greater parallelization."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Positional Embeddings",
      "description": "The Transformer uses positional embeddings to retain information about the order of tokens in the input sequence."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Parallelization",
      "description": "The Transformer architecture enhances the ability to parallelize computations compared to recurrent architectures."
    },
    {
      "type": "COMPARES_TO",
      "source": "Self-attention",
      "target": "Other layer types",
      "description": "Comparison of maximum path lengths between input and output positions in networks composed of different layer types."
    },
    {
      "type": "IMPROVES",
      "source": "Self-attention",
      "target": "Learning long-range dependencies",
      "description": "Self-attention layers help in learning long-range dependencies more effectively due to shorter path lengths."
    },
    {
      "type": "COMPARES_TO",
      "source": "Self-Attention Layer",
      "target": "Recurrent Layer",
      "description": "Self-attention layers are faster than recurrent layers in terms of computational complexity."
    },
    {
      "type": "IMPROVES",
      "source": "Self-Attention",
      "target": "Computational Performance",
      "description": "Self-attention can be restricted to a neighborhood of size r to enhance computational performance for long sequences."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model utilizes self-attention mechanisms to process input sequences."
    },
    {
      "type": "COMPARES_TO",
      "source": "Convolutional Layer",
      "target": "Self-Attention",
      "description": "Convolutional layers are compared to self-attention in terms of their ability to connect input and output positions."
    },
    {
      "type": "EXTENDS",
      "source": "Dilated Convolutions",
      "target": "Convolutional Layer",
      "description": "Dilated convolutions extend the capabilities of standard convolutional layers by increasing the receptive field."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model uses self-attention mechanisms to process input sequences."
    },
    {
      "type": "IMPROVES",
      "source": "Separable Convolutions",
      "target": "Complexity",
      "description": "Separable convolutions improve the computational complexity compared to standard convolutions."
    },
    {
      "type": "COMPARES_TO",
      "source": "Separable Convolutions",
      "target": "Self-Attention",
      "description": "The complexity of separable convolutions is compared to that of self-attention layers."
    },
    {
      "type": "BUILDS_ON",
      "source": "Self-Attention",
      "target": "Point-wise Feed-Forward Layer",
      "description": "The self-attention mechanism is combined with point-wise feed-forward layers in the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "WMT 2014 English-German",
      "description": "The Transformer model is trained using the WMT 2014 English-German dataset."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "WMT 2014 English-French",
      "description": "The Transformer model is trained using the WMT 2014 English-French dataset."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Byte-Pair Encoding",
      "description": "The Transformer model uses byte-pair encoding for sentence encoding."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "NVIDIA P 100 GPUs",
      "description": "The training of the Transformer model is conducted on NVIDIA P 100 GPUs."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Adam optimizer",
      "description": "The Transformer model employs the Adam optimizer for training."
    },
    {
      "type": "IMPROVES",
      "source": "Learning Rate Scheduling",
      "target": "Training Steps",
      "description": "Learning rate scheduling improves the efficiency of training steps."
    },
    {
      "type": "EMPLOYS",
      "source": "Transformer",
      "target": "Regularization",
      "description": "The Transformer model employs regularization techniques during training."
    },
    {
      "type": "USES",
      "source": "Training Steps",
      "target": "NVIDIA P100 GPUs",
      "description": "Training steps are executed on NVIDIA P100 GPUs."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "BLEU",
      "description": "The Transformer achieves better BLEU scores than previous state-of-the-art models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Byte Net",
      "description": "The Transformer is compared to Byte Net in terms of BLEU scores."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Deep-Att + Pos Unk",
      "description": "The Transformer is compared to Deep-Att + Pos Unk in terms of BLEU scores."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "GNMT + RL",
      "description": "The Transformer is compared to GNMT + RL in terms of BLEU scores."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Conv S2S",
      "description": "The Transformer is compared to Conv S2S in terms of BLEU scores."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "MoE",
      "description": "The Transformer is compared to MoE in terms of BLEU scores."
    },
    {
      "type": "IMPROVES",
      "source": "Label Smoothing",
      "target": "Accuracy",
      "description": "Label smoothing improves the accuracy of the model despite hurting perplexity."
    },
    {
      "type": "IMPROVES",
      "source": "Label Smoothing",
      "target": "BLEU score",
      "description": "Label smoothing improves the BLEU score of the model."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer (big)",
      "target": "Previously reported models",
      "description": "The Transformer (big) model outperforms the best previously reported models by more than 2.0 BLEU."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Self-Attention Mechanism",
      "description": "The Transformer architecture builds on the self-attention mechanism for processing sequences."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "WMT 2014 English-to-French",
      "description": "The Transformer model achieves a BLEU score of 41.0 on the WMT 2014 English-to-French translation task."
    },
    {
      "type": "USES",
      "source": "big model",
      "target": "dropout rate",
      "description": "The big model uses a dropout rate of Pdrop = 0.1."
    },
    {
      "type": "USES",
      "source": "big model",
      "target": "beam search",
      "description": "The big model employs beam search with a beam size of 4."
    },
    {
      "type": "USES",
      "source": "big model",
      "target": "length penalty",
      "description": "The big model applies a length penalty \u03b1 = 0.6 during beam search."
    },
    {
      "type": "COMPARES_TO",
      "source": "big model",
      "target": "previous state-of-the-art model",
      "description": "The big model outperforms the previous state-of-the-art model at less than 1/4 the training cost."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Other Model Architectures",
      "description": "The performance and training costs of the Transformer are compared to other model architectures from the literature."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention Mechanisms",
      "description": "The Transformer model relies entirely on self-attention mechanisms for its operations."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Translation Quality",
      "description": "The Transformer model aims to improve translation quality in tasks such as English-to-German translation."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Floating Point Operations",
      "description": "The training of the Transformer model is estimated based on floating point operations."
    },
    {
      "type": "EVALUATES",
      "source": "Attention Is All You Need",
      "target": "newstest 2013",
      "description": "The paper evaluates the performance of the Transformer model on the newstest 2013 dataset."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "The paper introduces the Transformer model as a novel architecture for sequence-to-sequence tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer architecture variations",
      "target": "base model",
      "description": "The variations on the Transformer architecture are compared to the base model in terms of performance metrics."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Mechanism",
      "description": "The Transformer architecture uses attention mechanisms to process input sequences."
    },
    {
      "type": "EVALUATES",
      "source": "Attention Is All You Need",
      "target": "newstest 2013",
      "description": "The paper evaluates the model's performance on the newstest 2013 dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Single-Head Attention",
      "target": "Multi-Head Attention",
      "description": "The paper compares the performance of single-head attention to multi-head attention."
    },
    {
      "type": "IMPROVES",
      "source": "Multi-Head Attention",
      "target": "Model Quality",
      "description": "Using multiple attention heads improves the quality of the model."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Dropout",
      "description": "The Transformer architecture uses dropout to avoid overfitting."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Dropout",
      "description": "The Transformer model utilizes dropout to avoid overfitting."
    },
    {
      "type": "EVALUATES",
      "source": "Transformer",
      "target": "English Constituency Parsing",
      "description": "The Transformer model is evaluated on the task of English constituency parsing."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Penn Treebank",
      "description": "The Transformer model is built using the Penn Treebank dataset for training."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Wall Street Journal (WSJ)",
      "description": "The Transformer model is trained on the WSJ portion of the Penn Treebank."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Berkley Parser corpora",
      "description": "The Transformer model is trained in a semi-supervised setting using the Berkley Parser corpora."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Penn Treebank",
      "description": "The Transformer model uses the Penn Treebank dataset for training."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Berkley Parser corpora",
      "description": "The Transformer model uses the Berkley Parser corpora in a semi-supervised setting."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Dropout",
      "description": "The Transformer model improves upon the dropout technique by optimizing its parameters."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Attention",
      "description": "The Transformer model introduces an improved attention mechanism."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Residual Connection",
      "description": "The Transformer model enhances the use of residual connections in its architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "English-to-German base translation model",
      "description": "The performance of the Transformer model is compared to the English-to-German base translation model."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Vinyals & Kaiser et al. (2014)",
      "description": "The performance of the Transformer model is compared to that of Vinyals & Kaiser et al. (2014) in terms of F1 scores."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Petrov et al. (2006)",
      "description": "The performance of the Transformer model is compared to that of Petrov et al. (2006) in terms of F1 scores."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Dyer et al. (2016)",
      "description": "The performance of the Transformer model is compared to that of Dyer et al. (2016) in terms of F1 scores."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Mc Closky et al. (2006)",
      "description": "The performance of the Transformer model is compared to that of Mc Closky et al. (2006) in terms of F1 scores."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Luong et al. (2015)",
      "description": "The performance of the Transformer model is compared to that of Luong et al. (2015) in terms of F1 scores."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Recurrent Neural Network Grammar",
      "description": "The Transformer model performs better than the Recurrent Neural Network Grammar despite the lack of task-specific tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Berkeley-Parser",
      "description": "The Transformer outperforms the Berkeley-Parser even when trained only on the WSJ training set."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "multi-headed self-attention",
      "description": "The Transformer model uses multi-headed self-attention instead of recurrent layers."
    },
    {
      "type": "EVALUATES",
      "source": "Transformer",
      "target": "WMT 2014 English-to-German",
      "description": "The Transformer model is evaluated on the WMT 2014 English-to-German translation task."
    },
    {
      "type": "EVALUATES",
      "source": "Transformer",
      "target": "WMT 2014 English-to-French",
      "description": "The Transformer model is evaluated on the WMT 2014 English-to-French translation task."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model for sequence-to-sequence tasks."
    },
    {
      "type": "EVALUATES",
      "source": "Attention Is All You Need",
      "target": "WMT 2014 English-to-German",
      "description": "The paper evaluates the Transformer model on the WMT 2014 English-to-German translation task."
    },
    {
      "type": "EVALUATES",
      "source": "Attention Is All You Need",
      "target": "WMT 2014 English-to-French",
      "description": "The paper evaluates the Transformer model on the WMT 2014 English-to-French translation task."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "Efficient Handling of Large Inputs",
      "description": "The paper plans to extend the Transformer model to handle large inputs like images and audio."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "Less Sequential Generation",
      "description": "The paper aims to extend the Transformer model to make generation less sequential."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model as a novel architecture for sequence transduction tasks."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Mechanism",
      "description": "The Transformer model uses attention mechanisms to process input sequences."
    },
    {
      "type": "EXTENDS",
      "source": "Self-Attention",
      "target": "Attention Mechanism",
      "description": "Self-attention is an extension of the attention mechanism that allows the model to consider the entire input sequence."
    },
    {
      "type": "EVALUATES",
      "source": "Attention Is All You Need",
      "target": "BLEU Score",
      "description": "The paper evaluates the performance of the Transformer model using the BLEU score metric."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model architecture."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model uses self-attention mechanisms to process input sequences."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "Encoder-Decoder",
      "description": "The Transformer architecture extends the traditional encoder-decoder framework by incorporating self-attention."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "BERT",
      "description": "BERT improves upon the Transformer architecture for specific natural language understanding tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "BLEU",
      "description": "The performance of the Transformer model is often evaluated using the BLEU metric."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model as a novel approach to sequence to sequence learning."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Sequence to Sequence Learning",
      "description": "The Transformer model improves upon traditional sequence to sequence learning methods by using self-attention."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer architecture uses self-attention mechanisms to process input sequences."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "The paper introduces the Transformer model as a novel architecture for sequence transduction tasks."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model uses self-attention mechanisms to process input sequences."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Neural Machine Translation",
      "description": "The Transformer model improves neural machine translation by enabling parallelization and reducing training time."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "Language Modeling",
      "description": "The performance of the Transformer model is compared to traditional language modeling techniques."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model architecture."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model uses self-attention mechanisms to process input data."
    },
    {
      "type": "EXTENDS",
      "source": "Self-Attention",
      "target": "Multi-Head Attention",
      "description": "Multi-head attention extends the self-attention mechanism by allowing multiple attention heads."
    },
    {
      "type": "EVALUATES",
      "source": "Attention Is All You Need",
      "target": "BLEU",
      "description": "The paper evaluates the performance of the Transformer model using the BLEU metric."
    },
    {
      "type": "BUILDS_ON",
      "source": "Transformer",
      "target": "Encoder-Decoder",
      "description": "The Transformer model builds on the encoder-decoder architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "Decomposable Attention Model",
      "target": "Transformer",
      "description": "The decomposable attention model builds on the principles of attention mechanisms introduced by the Transformer."
    },
    {
      "type": "IMPROVES",
      "source": "Self-training",
      "target": "Parsing",
      "description": "Self-training improves parsing accuracy by utilizing additional unannotated data."
    },
    {
      "type": "USES",
      "source": "Deep Reinforced Model",
      "target": "Reinforcement Learning",
      "description": "The deep reinforced model uses reinforcement learning techniques for summarization tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Tree Annotation",
      "target": "Traditional Parsing Techniques",
      "description": "Tree annotation methods are compared to traditional parsing techniques in terms of accuracy and interpretability."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model as a novel approach to sequence transduction tasks."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model uses self-attention mechanisms to process input sequences."
    },
    {
      "type": "EXTENDS",
      "source": "Attention Mechanism",
      "target": "Self-Attention",
      "description": "The self-attention mechanism is an extension of the general attention mechanism."
    },
    {
      "type": "EVALUATES",
      "source": "Attention Is All You Need",
      "target": "Machine Translation Datasets",
      "description": "The paper evaluates the Transformer model on various machine translation datasets."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model as a novel architecture for sequence transduction tasks."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model uses self-attention mechanisms to process input sequences."
    },
    {
      "type": "EVALUATES",
      "source": "Attention Is All You Need",
      "target": "WMT",
      "description": "The paper evaluates the performance of the Transformer model on the WMT dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "Transformer",
      "target": "RNN",
      "description": "The paper compares the performance of the Transformer model to traditional recurrent neural networks."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model as a novel approach to sequence transduction tasks."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Mechanism",
      "description": "The Transformer model uses attention mechanisms to process input sequences."
    },
    {
      "type": "EXTENDS",
      "source": "Attention Mechanism",
      "target": "Encoder-Decoder Architecture",
      "description": "The attention mechanism extends the traditional encoder-decoder architecture by allowing for direct connections between input and output sequences."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model as a novel architecture for sequence transduction tasks."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Attention Mechanism",
      "description": "The Transformer model uses attention mechanisms to process input sequences."
    },
    {
      "type": "EXTENDS",
      "source": "Attention Mechanism",
      "target": "Anaphora Resolution",
      "description": "The attention mechanism is extended to improve tasks such as anaphora resolution in the model."
    },
    {
      "type": "INTRODUCES",
      "source": "Attention Is All You Need",
      "target": "Transformer",
      "description": "This paper introduces the Transformer model as a novel architecture for sequence transduction tasks."
    },
    {
      "type": "USES",
      "source": "Transformer",
      "target": "Self-Attention",
      "description": "The Transformer model uses self-attention mechanisms to process input sequences."
    },
    {
      "type": "EXTENDS",
      "source": "Self-Attention",
      "target": "Attention Heads",
      "description": "Self-attention is extended through the use of multiple attention heads that learn different aspects of the input."
    },
    {
      "type": "BUILDS_ON",
      "source": "Encoder-Decoder",
      "target": "Transformer",
      "description": "The Transformer architecture builds on the encoder-decoder framework but replaces recurrent layers with self-attention."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA-13B",
      "target": "GPT-3",
      "description": "LLaMA-13B outperforms GPT-3 on most benchmarks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "Chinchilla",
      "description": "LLaMA-65B is competitive with Chinchilla-70B."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "PaLM",
      "description": "LLaMA-65B is competitive with PaLM-540B."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Publicly Available Datasets",
      "description": "LLaMA is trained using publicly available datasets exclusively."
    },
    {
      "type": "IMPROVES",
      "source": "Hoffmann et al. (2022)",
      "target": "Scaling laws",
      "description": "Hoffmann et al. (2022) provides insights on how to best scale dataset and model sizes."
    },
    {
      "type": "BUILDS_ON",
      "source": "Brown et al. (2020)",
      "target": "Few-shot learning",
      "description": "Brown et al. (2020) discusses the few-shot capabilities of LLMs."
    },
    {
      "type": "BUILDS_ON",
      "source": "Kaplan et al. (2020)",
      "target": "Few-shot learning",
      "description": "Kaplan et al. (2020) shows that scaling models leads to few-shot properties."
    },
    {
      "type": "COMPARES_TO",
      "source": "Hoffmann et al. (2022)",
      "target": "Large Language Models (LLMs)",
      "description": "Hoffmann et al. (2022) compares performance of smaller models trained on more data to larger models."
    },
    {
      "type": "EXTENDS",
      "source": "Chowdhery et al. (2022)",
      "target": "Large Language Models (LLMs)",
      "description": "Chowdhery et al. (2022) extends the work on scaling language models."
    },
    {
      "type": "EXTENDS",
      "source": "Rae et al. (2021)",
      "target": "Large Language Models (LLMs)",
      "description": "Rae et al. (2021) extends the understanding of scaling language models."
    },
    {
      "type": "COMPARES_TO",
      "source": "10 B model",
      "target": "7 B model",
      "description": "The 10 B model is compared to the 7 B model in terms of performance and training efficiency."
    },
    {
      "type": "IMPROVES",
      "source": "7 B model",
      "target": "1 T tokens",
      "description": "The performance of the 7 B model improves with training on 1 T tokens."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Inference Budget",
      "description": "LLaMA considers inference budget as a critical factor for model performance."
    },
    {
      "type": "DISREGARDS",
      "source": "Training Compute Budget",
      "target": "Inference Budget",
      "description": "The objective of scaling dataset and model sizes disregards the inference budget."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA-7B",
      "target": "Performance",
      "description": "The performance of the 7 B model continues to improve even after 1 T tokens."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-13B",
      "target": "GPT-3",
      "description": "LLaMA-13 B outperforms GPT-3 on most benchmarks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "Chinchilla",
      "description": "LLaMA's 65 B-parameter model is competitive with Chinchilla."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "PaLM-540B",
      "description": "LLaMA's 65 B-parameter model is competitive with PaLM-540 B."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Publicly Available Data",
      "description": "LLaMA uses publicly available data for training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Publicly Available Data",
      "description": "LLaMA is developed using only publicly available data."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Chinchilla",
      "description": "LLaMA is compared to Chinchilla in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Pa LM",
      "description": "LLaMA is compared to Pa LM in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is compared to GPT-3, which relies on non-public data."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA introduces modifications to the transformer architecture."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Standard Benchmarks",
      "description": "LLaMA's performance is evaluated against standard benchmarks."
    },
    {
      "type": "EXPOSES",
      "source": "LLaMA",
      "target": "Biases and Toxicity",
      "description": "LLaMA exposes biases and toxicity encoded in the models."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Common Crawl",
      "description": "LLaMA uses the Common Crawl dataset as part of its training data."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Chinchilla scaling laws",
      "description": "LLaMA's training approach is inspired by Chinchilla scaling laws."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "CCNet pipeline",
      "description": "LLaMA employs the CCNet pipeline to preprocess Common Crawl data."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Pre-training Data",
      "description": "LLaMA is trained on a mixture of pre-training data sources."
    },
    {
      "type": "USES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "Common Crawl",
      "description": "The LLaMA model uses data from Common Crawl for training."
    },
    {
      "type": "USES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "C4",
      "description": "The LLaMA model includes the C4 dataset to improve performance."
    },
    {
      "type": "IMPROVES",
      "source": "C4",
      "target": "LLaMA: Open and Efficient Foundation Language Models",
      "description": "The inclusion of the C4 dataset improves the performance of the LLaMA model."
    },
    {
      "type": "BUILDS_ON",
      "source": "C4",
      "target": "Common Crawl",
      "description": "The C4 dataset is built on data collected from Common Crawl."
    },
    {
      "type": "USES",
      "source": "C4",
      "target": "Deduplication",
      "description": "The C4 dataset employs deduplication as part of its preprocessing steps."
    },
    {
      "type": "USES",
      "source": "C4",
      "target": "Language Identification",
      "description": "The C4 dataset uses language identification to filter data."
    },
    {
      "type": "USES",
      "source": "C4",
      "target": "Quality Filtering",
      "description": "The C4 dataset applies quality filtering based on heuristics."
    },
    {
      "type": "USES",
      "source": "Wikipedia",
      "target": "Linear Classifier",
      "description": "A linear classifier is trained to classify pages used as references in Wikipedia."
    },
    {
      "type": "USES",
      "source": "Wikipedia",
      "target": "n-gram Language Model",
      "description": "An n-gram language model is used for quality filtering of content."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Common Crawl",
      "description": "LLaMA uses the Common Crawl dataset for training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Github",
      "description": "LLaMA uses the Github dataset for training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Wikipedia",
      "description": "LLaMA uses the Wikipedia dataset for training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Books",
      "description": "LLaMA uses the Books dataset for training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Ar Xiv",
      "description": "LLaMA uses the Ar Xiv dataset for training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Stack Exchange",
      "description": "LLaMA uses the Stack Exchange dataset for training."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Quality Filtering",
      "description": "LLaMA improves dataset quality filtering techniques."
    },
    {
      "type": "EXTENDS",
      "source": "Quality Filtering",
      "target": "Heuristics",
      "description": "Quality filtering extends the use of heuristics for dataset evaluation."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Common Crawl",
      "description": "LLaMA uses Common Crawl as a significant portion of its pre-training data."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "C",
      "description": "LLaMA uses the C dataset as part of its pre-training data."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Github",
      "description": "LLaMA uses the Github dataset in its pre-training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Wikipedia",
      "description": "LLaMA incorporates Wikipedia data for pre-training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Books",
      "description": "LLaMA utilizes the Books dataset in its training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Ar Xiv",
      "description": "LLaMA includes Ar Xiv data in its pre-training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Stack Exchange",
      "description": "LLaMA employs Stack Exchange data for training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Gutenberg Project",
      "description": "LLaMA uses the Gutenberg Project dataset for training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Books 3",
      "description": "LLaMA incorporates the Books 3 dataset in its training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "The Pile",
      "description": "LLaMA uses The Pile dataset for training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "arXiv",
      "description": "LLaMA uses arXiv data to enhance its dataset."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Stack Exchange",
      "description": "LLaMA includes data from Stack Exchange to improve its training."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Deduplication",
      "description": "LLaMA improves dataset quality through deduplication techniques."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Byte-Pair Encoding (BPE)",
      "description": "LLaMA uses the byte-pair encoding algorithm for tokenizing data."
    },
    {
      "type": "USES",
      "source": "Byte-Pair Encoding (BPE)",
      "target": "SentencePiece",
      "description": "The byte-pair encoding algorithm is implemented using the SentencePiece library."
    },
    {
      "type": "REMOVES",
      "source": "LLaMA",
      "target": "HTML tags",
      "description": "LLaMA removes HTML tags from the text data before processing."
    },
    {
      "type": "DECOMPOSES",
      "source": "LLaMA",
      "target": "UTF-8 characters",
      "description": "LLaMA falls back to bytes to decompose unknown UTF-8 characters."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA is based on the transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer",
      "target": "Pre-normalization",
      "description": "Pre-normalization is an improvement to the original transformer architecture."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Wikipedia",
      "description": "LLaMA uses Wikipedia as part of its training dataset."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Books",
      "description": "LLaMA uses Books as part of its training dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Pre-normalization",
      "target": "Training Stability",
      "description": "Pre-normalization improves training stability by normalizing the input of each transformer sub-layer."
    },
    {
      "type": "IMPROVES",
      "source": "Swi GLU activation function",
      "target": "Performance",
      "description": "The Swi GLU activation function improves performance compared to the ReLU non-linearity."
    },
    {
      "type": "REPLACES",
      "source": "ReLU",
      "target": "Swi GLU activation function",
      "description": "The Swi GLU activation function replaces the ReLU non-linearity."
    },
    {
      "type": "REPLACES",
      "source": "Absolute Positional Embeddings",
      "target": "Rotary Embeddings",
      "description": "Rotary embeddings replace absolute positional embeddings in the model."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "AdamW",
      "description": "LLaMA uses the AdamW optimizer for training."
    },
    {
      "type": "USES",
      "source": "LLaMA-7B",
      "target": "AdamW optimizer",
      "description": "LLaMA-7B uses the AdamW optimizer for training."
    },
    {
      "type": "USES",
      "source": "LLaMA-13B",
      "target": "AdamW optimizer",
      "description": "LLaMA-13B uses the AdamW optimizer for training."
    },
    {
      "type": "USES",
      "source": "LLaMA-33B",
      "target": "AdamW optimizer",
      "description": "LLaMA-33B uses the AdamW optimizer for training."
    },
    {
      "type": "USES",
      "source": "LLaMA-65B",
      "target": "AdamW optimizer",
      "description": "LLaMA-65B uses the AdamW optimizer for training."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA-33B",
      "target": "Training loss",
      "description": "LLaMA-33B improves training loss compared to smaller models."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA-65B",
      "target": "Training loss",
      "description": "LLaMA-65B improves training loss compared to smaller models."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "LLaMA-7B",
      "description": "This paper introduces the LLaMA-7B model."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "LLaMA-13B",
      "description": "This paper introduces the LLaMA-13B model."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "LLaMA-33B",
      "description": "This paper introduces the LLaMA-33B model."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "LLaMA-65B",
      "description": "This paper introduces the LLaMA-65B model."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Causal Multi-Head Attention",
      "description": "LLaMA uses an efficient implementation of causal multi-head attention to enhance training speed."
    },
    {
      "type": "USES",
      "source": "Causal Multi-Head Attention",
      "target": "xformers",
      "description": "The efficient implementation of causal multi-head attention is available in the xformers library."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Checkpointing",
      "description": "LLaMA employs checkpointing to reduce the amount of activations recomputed during the backward pass."
    },
    {
      "type": "EXTENDS",
      "source": "Causal Multi-Head Attention",
      "target": "Backward Pass Optimization",
      "description": "The implementation extends backward pass optimization techniques by not storing attention weights."
    },
    {
      "type": "INSPIRES",
      "source": "Causal Multi-Head Attention",
      "target": "Rabe and Staats (2021)",
      "description": "The efficient implementation is inspired by the work of Rabe and Staats."
    },
    {
      "type": "REFERENCES",
      "source": "Backward Pass Optimization",
      "target": "Dao et al. (2022)",
      "description": "The backward pass optimization technique references the work of Dao et al."
    },
    {
      "type": "USES",
      "source": "Checkpointing",
      "target": "Transformer",
      "description": "Checkpointing is used to optimize the computation of the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "Checkpointing",
      "target": "Backward Pass",
      "description": "Checkpointing improves the efficiency of the backward pass by saving expensive activations."
    },
    {
      "type": "EXTENDS",
      "source": "Transformer",
      "target": "PyTorch",
      "description": "The Transformer architecture is implemented in PyTorch, extending its capabilities."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA's performance is compared to GPT-3."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Gopher",
      "description": "LLaMA's performance is compared to Gopher."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Chinchilla",
      "description": "LLaMA's performance is compared to Chinchilla."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Pa LM",
      "description": "LLaMA's performance is compared to Pa LM."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Model and sequence parallelism",
      "description": "LLaMA uses model and sequence parallelism to reduce memory usage."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is compared to GPT-3 in terms of performance on various benchmarks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Gopher",
      "description": "LLaMA is compared to Gopher in terms of performance on various benchmarks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Chinchilla",
      "description": "LLaMA is compared to Chinchilla in terms of performance on various benchmarks."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "1.4 T tokens dataset",
      "description": "LLaMA uses the 1.4 trillion tokens dataset for training."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "tokens/sec/GPU",
      "description": "LLaMA improves the processing speed of tokens per second per GPU."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is compared with GPT-3."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Gopher",
      "description": "LLaMA is compared with Gopher."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Chinchilla",
      "description": "LLaMA is compared with Chinchilla."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "PaLM",
      "description": "LLaMA is compared with PaLM."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "OPT",
      "description": "LLaMA is compared with OPT models."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-J",
      "description": "LLaMA is compared with GPT-J."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-Neo",
      "description": "LLaMA is compared with GPT-Neo."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "OPT-IML",
      "description": "LLaMA is compared with OPT-IML."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Flan-PaLM",
      "description": "LLaMA is compared with Flan-PaLM."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Likelihood",
      "description": "LLaMA uses likelihood to select completions."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA's performance is compared to GPT-3 across various shot settings."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Gopher",
      "description": "LLaMA's performance is compared to Gopher across various shot settings."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Chinchilla",
      "description": "LLaMA's performance is compared to Chinchilla across various shot settings."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "PaLM",
      "description": "LLaMA's performance is compared to PaLM across various shot settings."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Likelihood Normalization",
      "description": "LLaMA uses likelihood normalization for evaluating completions."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Open Book QA",
      "description": "LLaMA follows specific evaluation methods for the Open Book QA dataset."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Bool Q",
      "description": "LLaMA follows specific evaluation methods for the Bool Q dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "Chinchilla-70B",
      "description": "LLaMA-65B outperforms Chinchilla-70B on all reported benchmarks except Bool Q."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "PaLM-540B",
      "description": "LLaMA-65B surpasses PaLM-540B on all benchmarks except Bool Q and Wino Grande."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-13B",
      "target": "GPT-3",
      "description": "LLaMA-13B outperforms GPT-3 on most benchmarks despite being 10\u00d7 smaller."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Natural Questions",
      "description": "LLaMA is evaluated on the Natural Questions benchmark for closed-book question answering."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Trivia QA",
      "description": "LLaMA is evaluated on the Trivia QA benchmark for closed-book question answering."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-13B",
      "target": "GPT-3",
      "description": "LLaMA-13B is competitive with GPT-3 despite being smaller."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "state-of-the-art",
      "description": "LLaMA-65B achieves state-of-the-art performance on benchmarks."
    },
    {
      "type": "USES",
      "source": "LLaMA-65B",
      "target": "V100 GPU",
      "description": "LLaMA-65B runs on a single V100 GPU during inference."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA-65B",
      "target": "Natural Questions",
      "description": "LLaMA-65B's performance is evaluated on the Natural Questions benchmark."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA-65B",
      "target": "Trivia QA",
      "description": "LLaMA-65B's performance is evaluated on the Trivia QA benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Chinchilla",
      "description": "LLaMA's performance is compared to Chinchilla's across various benchmarks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA-13B outperforms GPT-3 by a few percent on reading comprehension tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "PaLM",
      "description": "LLaMA-65B is competitive with PaLM-540B on reading comprehension benchmarks."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "RACE",
      "description": "LLaMA is evaluated on the RACE reading comprehension benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-13B",
      "target": "GPT-3",
      "description": "LLaMA-13B outperforms GPT-3 by a few percent."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "Minerva-62B",
      "description": "LLaMA-65B outperforms Minerva-62B on the GSM 8k benchmark."
    },
    {
      "type": "USES",
      "source": "Minerva",
      "target": "PaLM",
      "description": "Minerva is a series of PaLM models fine-tuned on mathematical data."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "MATH",
      "description": "LLaMA models are evaluated on the MATH benchmark."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "GSM 8k",
      "description": "LLaMA models are evaluated on the GSM 8k benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "Minerva-62B",
      "description": "LLaMA-65B is compared to Minerva-62B on the GSM 8k dataset."
    },
    {
      "type": "USES",
      "source": "LLaMA-65B",
      "target": "Human Eval",
      "description": "LLaMA-65B is evaluated on the Human Eval benchmark for code generation."
    },
    {
      "type": "USES",
      "source": "LLaMA-65B",
      "target": "MBPP",
      "description": "LLaMA-65B is evaluated on the MBPP benchmark for code generation."
    },
    {
      "type": "USES",
      "source": "Majority Voting",
      "target": "GSM 8k",
      "description": "Majority voting is used to evaluate model performance on the GSM 8k dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Minerva",
      "description": "LLaMA-65B outperforms Minerva 62B on GSM 8k."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "MATH",
      "description": "LLaMA is evaluated on the MATH dataset."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "GSM 8k",
      "description": "LLaMA is evaluated on the GSM 8k dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "PaLM",
      "description": "LLaMA is compared with PaLM in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "LaMDA",
      "description": "LLaMA is compared with LaMDA in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "LaMDA",
      "description": "LLaMA outperforms LaMDA on both Human Eval and MBPP."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "PaLM",
      "description": "LLaMA outperforms PaLM even when it is trained longer."
    },
    {
      "type": "IMPROVES",
      "source": "PaLM-Coder",
      "target": "PaLM",
      "description": "PaLM-Coder increases the pass@1 score of PaLM on Human Eval."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Human Eval",
      "description": "LLaMA's performance is evaluated using the Human Eval dataset."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "MBPP",
      "description": "LLaMA's performance is evaluated using the MBPP dataset."
    },
    {
      "type": "IMPROVES",
      "source": "Pa LM-Coder",
      "target": "Pa LM",
      "description": "Pa LM-Coder increases the pass@1 score of Pa LM on Human Eval from 26.2% to 36%."
    },
    {
      "type": "COMPARES_TO",
      "source": "Pa LM",
      "target": "LLaMA",
      "description": "Pa LM is compared to LLaMA in terms of performance on code generation tasks."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Human Eval",
      "description": "LLaMA's performance is evaluated using the Human Eval dataset."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "MBPP",
      "description": "LLaMA's performance is evaluated using the MBPP dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "Chinchilla-70B",
      "description": "LLaMA-65B is compared to Chinchilla-70B in terms of performance on the MMLU benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "PaLM-540B",
      "description": "LLaMA-65B is compared to PaLM-540B in terms of performance on the MMLU benchmark."
    },
    {
      "type": "USES",
      "source": "LLaMA-65B",
      "target": "Human Eval",
      "description": "LLaMA-65B is evaluated using the Human Eval benchmark."
    },
    {
      "type": "USES",
      "source": "LLaMA-65B",
      "target": "MBPP",
      "description": "LLaMA-65B is evaluated using the MBPP benchmark."
    },
    {
      "type": "INTRODUCES",
      "source": "MMLU",
      "target": "Hendrycks et al. (2020)",
      "description": "MMLU is introduced by Hendrycks et al. in 2020."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "ArXiv",
      "description": "LLaMA uses ArXiv as part of its pre-training data."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Gutenberg",
      "description": "LLaMA uses Gutenberg as part of its pre-training data."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Books 3",
      "description": "LLaMA uses Books 3 as part of its pre-training data."
    },
    {
      "type": "COMPARES_TO",
      "source": "Gopher",
      "target": "GPT-3",
      "description": "Gopher outperforms GPT-3 on certain benchmarks."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "training perplexity",
      "description": "The performance of LLaMA improves steadily and correlates with training perplexity."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Gopher",
      "description": "LLaMA is compared with Gopher, Chinchilla, and Pa LM in terms of performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-Neo X",
      "description": "LLaMA's performance is compared to GPT-Neo X."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA's performance is compared to GPT-3."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Gopher",
      "description": "LLaMA's performance is compared to Gopher."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Chinchilla",
      "description": "LLaMA's performance is compared to Chinchilla."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "PaLM",
      "description": "LLaMA's performance is compared to PaLM."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "MMLU",
      "description": "Finetuning LLaMA leads to improvements on MMLU."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Wino Grande",
      "description": "LLaMA's performance is evaluated on the Wino Grande dataset."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA-65B",
      "target": "MMLU",
      "description": "Fine-tuning LLaMA-65B leads to improved performance on MMLU."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "OPT-30B",
      "description": "LLaMA-I is compared to OPT-30B in instruction fine-tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "GLM-120B",
      "description": "LLaMA-I is compared to GLM-120B in instruction fine-tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "PaLM-62B",
      "description": "LLaMA-I is compared to PaLM-62B in instruction fine-tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "Chinchilla-70B",
      "description": "LLaMA-I is compared to Chinchilla-70B in instruction fine-tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "OPT-IML-Max-30B",
      "description": "LLaMA-I is compared to OPT-IML-Max-30B in instruction fine-tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "Flan-T5-XXL-11B",
      "description": "LLaMA-I is compared to Flan-T5-XXL-11B in instruction fine-tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "Flan-PaLM-62B",
      "description": "LLaMA-I is compared to Flan-PaLM-62B in instruction fine-tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "Flan-PaLM-cont-62B",
      "description": "LLaMA-I is compared to Flan-PaLM-cont-62B in instruction fine-tuning."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "OPT-IML",
      "description": "LLaMA-I is compared to OPT-IML in terms of performance on MMLU."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "Flan-PaLM",
      "description": "LLaMA-I is compared to the Flan-PaLM series in terms of performance on MMLU."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-I",
      "target": "GPT code-davinci-002",
      "description": "LLaMA-I's performance is compared to the state-of-the-art GPT code-davinci-002 on MMLU."
    },
    {
      "type": "USES",
      "source": "LLaMA-I",
      "target": "Instruction Fine-tuning",
      "description": "LLaMA-I uses instruction fine-tuning to improve its performance."
    },
    {
      "type": "REPRODUCES",
      "source": "Large language models",
      "target": "Bias, Toxicity and Misinformation",
      "description": "Large language models reproduce and amplify biases present in training data."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA-65 B",
      "target": "Toxic Content Production Benchmark",
      "description": "LLaMA-65 B is evaluated on benchmarks measuring its potential for generating toxic content."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA-65 B",
      "target": "Stereotypes Detection Benchmark",
      "description": "LLaMA-65 B is evaluated on benchmarks measuring its ability to detect stereotypes."
    },
    {
      "type": "USES",
      "source": "LLaMA-65 B",
      "target": "Web Data",
      "description": "LLaMA-65 B is trained on a large proportion of data sourced from the web."
    },
    {
      "type": "COMPARES_TO",
      "source": "Toxic Content Production Benchmark",
      "target": "Stereotypes Detection Benchmark",
      "description": "Both benchmarks are used to indicate issues with language models."
    },
    {
      "type": "EXTENDS",
      "source": "Bias in Language Models",
      "target": "Toxic Content Generation",
      "description": "Bias in language models can lead to the generation of toxic content."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Real Toxicity Prompts",
      "description": "LLaMA is evaluated using the Real Toxicity Prompts benchmark to assess its ability to generate toxic language."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Trivia QA",
      "description": "LLaMA's performance is evaluated on the Trivia QA dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Hella Swag",
      "description": "LLaMA's performance is evaluated on the Hella Swag dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Natural Questions",
      "description": "LLaMA's performance is evaluated on the Natural Questions dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "SIQA",
      "description": "LLaMA's performance is evaluated on the SIQA dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Wino Grande",
      "description": "LLaMA's performance is evaluated on the Wino Grande dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "PIQA",
      "description": "LLaMA's performance is evaluated on the PIQA dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Chinchilla",
      "description": "LLaMA's performance is compared to that of the Chinchilla model."
    },
    {
      "type": "USES",
      "source": "Real Toxicity Prompts",
      "target": "Perspective API",
      "description": "Real Toxicity Prompts uses the Perspective API to evaluate toxicity scores."
    },
    {
      "type": "COMPARES_TO",
      "source": "toxicity score",
      "target": "Chinchilla",
      "description": "The toxicity score of the model is compared to that of Chinchilla."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Real Toxicity Prompts",
      "description": "LLaMA uses the Real Toxicity Prompts benchmark to evaluate its performance."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Perplexity",
      "description": "LLaMA's outputs are evaluated using the Perplexity metric."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Chinchilla",
      "description": "LLaMA's toxicity scores are compared to those of Chinchilla."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Toxicity",
      "description": "LLaMA shows an increase in toxicity with the size of the model."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Greedy Decoder",
      "description": "LLaMA employs a greedy decoder to generate text from prompts."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA",
      "target": "Sampling Strategy",
      "description": "LLaMA extends the sampling strategy used in previous works."
    },
    {
      "type": "COMPARES_TO",
      "source": "Chinchilla",
      "target": "Gopher",
      "description": "Chinchilla is compared to Gopher in terms of performance despite Gopher being larger."
    },
    {
      "type": "BUILDS_ON",
      "source": "Zhang et al. (2022)",
      "target": "LLaMA",
      "description": "LLaMA builds on the findings of Zhang et al. regarding model size and performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "Hoffmann et al. (2022)",
      "target": "Chinchilla",
      "description": "Hoffmann et al. did not find a performance difference between Chinchilla and Gopher."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is compared to GPT-3 in terms of bias levels."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "OPT",
      "description": "LLaMA is compared to OPT in terms of bias levels."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Crow S-Pairs",
      "description": "LLaMA uses the Crow S-Pairs dataset to evaluate biases."
    },
    {
      "type": "MEASURES",
      "source": "Crow S-Pairs",
      "target": "Bias",
      "description": "Crow S-Pairs measures biases in various categories."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is compared to GPT-3 in terms of bias evaluation."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "OPT-175B",
      "description": "LLaMA is compared to OPT-175B in terms of bias evaluation."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Common Crawl",
      "description": "LLaMA's biases are expected to originate from the Common Crawl dataset."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Wino Gender",
      "description": "LLaMA's biases are further investigated using the Wino Gender benchmark."
    },
    {
      "type": "MEASURES",
      "source": "LLaMA",
      "target": "Perplexity",
      "description": "LLaMA's preference for stereotypical sentences is measured using perplexity."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Wino Gender dataset",
      "description": "LLaMA uses the Wino Gender dataset to evaluate co-reference resolution performance."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Co-reference resolution",
      "description": "LLaMA evaluates its performance in co-reference resolution based on societal biases."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Perplexity",
      "description": "LLaMA compares the perplexity of different pronoun continuations to assess co-reference resolution."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Co-reference resolution",
      "description": "LLaMA aims to improve co-reference resolution performance by addressing societal biases."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "co-reference scores",
      "description": "The paper evaluates the performance of the LLaMA model using co-reference scores for different pronouns."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Wino Gender",
      "description": "The model's performance is compared to previous work using the Wino Gender dataset."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "her/her/she",
      "description": "The model uses the 'her/her/she' pronouns to evaluate co-reference resolution."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "his/him/he",
      "description": "The model uses the 'his/him/he' pronouns to evaluate co-reference resolution."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "their/them/someone",
      "description": "The model uses the 'their/them/someone' pronouns to evaluate co-reference resolution."
    },
    {
      "type": "INDICATES",
      "source": "co-reference scores",
      "target": "gender bias",
      "description": "The co-reference scores indicate a potential gender bias in the model's performance."
    },
    {
      "type": "USES",
      "source": "LLaMA-65B",
      "target": "Wino Gender dataset",
      "description": "LLaMA-65B uses the Wino Gender dataset to investigate gender biases."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA-65B",
      "target": "Truthful QA",
      "description": "LLaMA-65B is evaluated using the Truthful QA metric to measure its truthfulness."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "Societal Biases",
      "description": "LLaMA-65B's performance is compared to societal biases related to gender and occupation."
    },
    {
      "type": "EXTENDS",
      "source": "Truthful QA",
      "target": "Literal Truth",
      "description": "Truthful QA extends the concept of literal truth to measure model claims."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "Adversarial Questions Benchmark",
      "description": "The paper introduces a benchmark to evaluate the risks of models generating misinformation."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "The performance of LLaMA is compared to that of GPT-3 in terms of truthful QA."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Wino Gender",
      "description": "LLaMA uses the Wino Gender dataset to evaluate co-reference resolution accuracy."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Co-reference resolution accuracy",
      "description": "The paper evaluates LLaMA's performance using co-reference resolution accuracy."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Truthful QA",
      "description": "The paper evaluates LLaMA's performance using the Truthful QA metric."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is compared to GPT-3 in terms of performance on truthful and informative metrics."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "energy consumption estimation",
      "description": "LLaMA uses a formula for estimating energy consumption based on GPU usage."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Power Usage Effectiveness (PUE)",
      "description": "LLaMA incorporates PUE in its energy consumption calculations."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "truthful models",
      "description": "LLaMA aims to improve the performance of models in providing truthful answers."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "informative",
      "description": "LLaMA aims to enhance the informativeness of model responses."
    },
    {
      "type": "COMPARES_TO",
      "source": "BLOOM",
      "target": "OPT",
      "description": "The paper compares the carbon emissions of training BLOOM and OPT models."
    },
    {
      "type": "USES",
      "source": "Wh",
      "target": "Power Usage Effectiveness (PUE)",
      "description": "The formula for calculating Wh uses the Power Usage Effectiveness (PUE)."
    },
    {
      "type": "USES",
      "source": "Carbon Emission",
      "target": "Carbon Intensity Factor",
      "description": "The carbon emission calculation uses the carbon intensity factor."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "OPT",
      "description": "LLaMA is compared to OPT in terms of training duration and carbon emissions."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "BLOOM",
      "description": "LLaMA is compared to BLOOM in terms of training duration and carbon emissions."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "A100",
      "description": "LLaMA utilizes A100 GPUs for model development."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "t CO2 eq",
      "description": "LLaMA uses the t CO2 eq metric to estimate carbon emissions."
    },
    {
      "type": "IMPROVES",
      "source": "smoothing techniques",
      "target": "n-gram models",
      "description": "Smoothing techniques enhance the performance of n-gram models by better estimating rare events."
    },
    {
      "type": "BUILDS_ON",
      "source": "neural networks",
      "target": "language modeling",
      "description": "Neural networks have been developed to advance the field of language modeling."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-7",
      "target": "OPT-175B",
      "description": "LLaMA-7 is compared to OPT-175B in terms of carbon emissions and power consumption."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-7",
      "target": "BLOOM-175B",
      "description": "LLaMA-7 is compared to BLOOM-175B in terms of carbon emissions and power consumption."
    },
    {
      "type": "USES",
      "source": "LLaMA-7",
      "target": "NVLink",
      "description": "LLaMA-7 uses NVLink technology for its GPU architecture."
    },
    {
      "type": "USES",
      "source": "LLaMA-13",
      "target": "NVLink",
      "description": "LLaMA-13 uses NVLink technology for its GPU architecture."
    },
    {
      "type": "USES",
      "source": "LLaMA-33",
      "target": "NVLink",
      "description": "LLaMA-33 uses NVLink technology for its GPU architecture."
    },
    {
      "type": "USES",
      "source": "LLaMA-65",
      "target": "NVLink",
      "description": "LLaMA-65 uses NVLink technology for its GPU architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Transformer Networks",
      "description": "LLaMA builds on the transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "Transformer Networks",
      "target": "Long Range Dependencies",
      "description": "Transformer networks improve the capture of long-range dependencies."
    },
    {
      "type": "USES",
      "source": "Language Models",
      "target": "Tokens",
      "description": "Language models are trained on tokens."
    },
    {
      "type": "USES",
      "source": "Language Models",
      "target": "n-grams",
      "description": "Language models utilize n-grams for quality improvements."
    },
    {
      "type": "COMPARES_TO",
      "source": "Stupid Backoff",
      "target": "Kneser-Ney Smoothing",
      "description": "Stupid Backoff is compared to Kneser-Ney smoothing in terms of scaling."
    },
    {
      "type": "INTRODUCES",
      "source": "One Billion Word benchmark",
      "target": "Dataset",
      "description": "Introduced as a benchmark for measuring language model progress."
    },
    {
      "type": "BUILDS_ON",
      "source": "LSTM",
      "target": "1 billion parameters",
      "description": "LSTMs were scaled to 1 billion parameters to achieve state-of-the-art results."
    },
    {
      "type": "IMPROVES",
      "source": "Transformers",
      "target": "NLP tasks",
      "description": "Scaling transformers led to improvements on many NLP tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "Jurassic-1",
      "description": "Jurassic-1 is a large language model developed after GPT-3."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "Megatron-Turing NLG",
      "description": "Megatron-Turing NLG is a large language model that follows advancements made by GPT-3."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "Jurassic-1",
      "description": "Both are large language models with significant parameter counts."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "Megatron-Turing NLG",
      "description": "Both are large language models developed by leading AI organizations."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "Gopher",
      "description": "Both are prominent models in the large language model landscape."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "Chinchilla",
      "description": "Both are large language models that focus on performance and efficiency."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "PaLM",
      "description": "Both are state-of-the-art language models developed by major tech companies."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "OPT",
      "description": "Both are large language models with significant capabilities."
    },
    {
      "type": "COMPARES_TO",
      "source": "GPT-3",
      "target": "GLM",
      "description": "Both are influential models in the field of natural language processing."
    },
    {
      "type": "IMPROVES",
      "source": "Power Laws",
      "target": "Learning Rate Schedule",
      "description": "The learning rate schedule was refined based on power law findings."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-13B",
      "target": "GPT-3",
      "description": "LLaMA-13B outperforms GPT-3."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "Chinchilla-70B",
      "description": "LLaMA-65B is competitive with Chinchilla-70B."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA-65B",
      "target": "PaLM-540B",
      "description": "LLaMA-65B is competitive with PaLM-540B."
    },
    {
      "type": "USES",
      "source": "LLaMA models",
      "target": "Publicly available data",
      "description": "LLaMA models are trained exclusively on publicly available data."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA models",
      "target": "Toxicity",
      "description": "The paper aims to mitigate toxicity in large language models."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA models",
      "target": "Bias",
      "description": "The paper aims to mitigate bias in large language models."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA models",
      "target": "Finetuning on instructions",
      "description": "Finetuning on instructions leads to promising results for LLaMA models."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Finetuning",
      "description": "Finetuning LLaMA on instructions leads to promising results."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA",
      "target": "Pretraining Corpora",
      "description": "The authors plan to release larger models trained on larger pretraining corpora."
    },
    {
      "type": "IMPROVES",
      "source": "Scaling",
      "target": "Performance",
      "description": "Constant improvement in performance is observed as models are scaled."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "data deduplication",
      "description": "LLaMA utilizes data deduplication to enhance training efficiency."
    },
    {
      "type": "SUPPORTS",
      "source": "xformers team",
      "target": "LLaMA",
      "description": "The xformers team provides support for the development of LLaMA."
    },
    {
      "type": "SUPPORTS",
      "source": "AI infra team",
      "target": "LLaMA",
      "description": "The AI infra team supports LLaMA with necessary infrastructure."
    },
    {
      "type": "SUPPORTS",
      "source": "evaluation team",
      "target": "LLaMA",
      "description": "The evaluation team assists in assessing the performance of LLaMA."
    },
    {
      "type": "SUPPORTS",
      "source": "training stability",
      "target": "LLaMA",
      "description": "Training stability is crucial for the effective training of LLaMA."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "GPT-NeoX-20B",
      "description": "LLaMA introduces a new model that is compared to existing models like GPT-NeoX-20B."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Neural Probabilistic Language Model",
      "description": "LLaMA builds on concepts from neural probabilistic language models."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "PIQA",
      "description": "LLaMA uses the PIQA dataset for evaluating commonsense reasoning."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "BERT",
      "description": "LLaMA builds on the foundational concepts established by BERT."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Machine Translation",
      "description": "LLaMA utilizes techniques from machine translation to enhance its language processing capabilities."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Statistical Approach",
      "description": "The performance of LLaMA is compared to traditional statistical approaches in machine translation."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is introduced in the context of comparing its capabilities to those of GPT-3."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Common Crawl",
      "description": "LLaMA utilizes the Common Crawl dataset for training."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "One Billion Word Benchmark",
      "description": "LLaMA builds on the methodologies established by the One Billion Word Benchmark."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Self-supervised learning",
      "description": "LLaMA utilizes self-supervised learning techniques for training."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "C4",
      "description": "LLaMA is trained on the C4 dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is compared to GPT-3 in terms of performance and efficiency."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Perplexity",
      "description": "LLaMA aims to improve perplexity scores over previous models."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Language Model Evaluation",
      "description": "LLaMA introduces new methodologies for evaluating language models."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "BERT",
      "description": "LLaMA builds on the architecture and principles established by BERT."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is compared to GPT-3 in terms of performance and efficiency."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "LLaMA",
      "description": "This paper introduces the LLaMA model."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "Palm",
      "description": "The paper compares LLaMA with the Palm model."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "BERT",
      "description": "LLaMA builds on the principles established by BERT."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Instruction-Finetuning",
      "description": "LLaMA utilizes instruction-finetuning to enhance its performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "BoolQ",
      "description": "LLaMA's performance is compared against benchmarks like BoolQ."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "ARC",
      "description": "LLaMA's capabilities are evaluated against the ARC dataset."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "BERT",
      "description": "LLaMA builds on the concepts introduced by BERT."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA",
      "target": "Transformer-XL",
      "description": "LLaMA extends the capabilities of Transformer-XL."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "FlashAttention",
      "description": "LLaMA uses FlashAttention for efficient processing."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "ARC",
      "description": "LLaMA introduces the ARC dataset for evaluation."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Math Word Problems",
      "description": "LLaMA utilizes the Math Word Problems dataset for training."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "BERT",
      "description": "LLaMA builds on the concepts introduced by BERT."
    },
    {
      "type": "USES",
      "source": "Incoder",
      "target": "BERT",
      "description": "Incoder uses the architecture of BERT for its generative capabilities."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "The Pile",
      "description": "LLaMA utilizes The Pile dataset for training."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Few-shot learning",
      "description": "LLaMA builds on the framework for few-shot language model evaluation."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Realtoxicityprompts",
      "description": "LLaMA uses the Realtoxicityprompts dataset to evaluate toxic degeneration."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Recurrent Neural Networks",
      "description": "LLaMA introduces techniques related to recurrent neural networks for sequence generation."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA",
      "target": "Kneser-Ney smoothing",
      "description": "LLaMA extends the Kneser-Ney smoothing technique for improved language model estimation."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "BERT",
      "description": "LLaMA builds on the concepts introduced by BERT."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Math Dataset",
      "description": "LLaMA uses the Math Dataset to evaluate its mathematical problem-solving capabilities."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Massive Multitask Language Understanding",
      "description": "LLaMA is compared to the metric of Massive Multitask Language Understanding."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA",
      "target": "Long Short-Term Memory (LSTM)",
      "description": "LLaMA extends the principles of LSTM architectures."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA is based on the Transformer architecture."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Compute-optimal training",
      "description": "LLaMA builds on the principles of compute-optimal training for efficiency."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Instruction meta learning",
      "description": "LLaMA utilizes instruction meta learning to enhance its capabilities."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "TriviaQA",
      "description": "LLaMA's performance is compared to existing datasets like TriviaQA."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA is built on the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Activation Re-computation",
      "description": "LLaMA improves upon existing techniques for activation re-computation."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Scaling Laws",
      "description": "LLaMA builds on the principles of scaling laws for neural language models."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA is built on the principles of the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "SentencePiece",
      "description": "LLaMA uses SentencePiece for tokenization."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Natural Questions",
      "description": "LLaMA's performance is compared to benchmarks like Natural Questions."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA",
      "target": "Quantifying Social Biases",
      "description": "LLaMA extends the work on quantifying social biases in language models."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "RACE",
      "description": "LLaMA utilizes the RACE dataset for training or evaluation."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "TruthfulQA",
      "description": "LLaMA employs the TruthfulQA dataset to assess model performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Jurassic-1",
      "description": "LLaMA is compared to Jurassic-1 in terms of performance and capabilities."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA",
      "target": "Quantitative Reasoning",
      "description": "LLaMA extends the capabilities of models in solving quantitative reasoning problems."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "TruthfulQA",
      "description": "LLaMA model is evaluated using the TruthfulQA dataset."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Decoupled Weight Decay Regularization",
      "description": "LLaMA incorporates decoupled weight decay regularization as part of its training methodology."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Crow S-pairs",
      "description": "LLaMA uses the Crow S-pairs dataset to measure social biases."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Recurrent Neural Network",
      "description": "LLaMA is compared to recurrent neural network models in terms of performance."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Open Book Question Answering Dataset",
      "description": "LLaMA introduces a new approach to evaluating models on the Open Book Question Answering Dataset."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA builds on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Human Feedback",
      "description": "LLaMA utilizes human feedback for training."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Crow S-pairs",
      "description": "LLaMA is compared to the Crow S-pairs dataset for bias measurement."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Codegen",
      "description": "LLaMA introduces new methodologies that may improve upon Codegen."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Self-attention",
      "description": "LLaMA utilizes self-attention mechanisms for processing language."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Generative Pre-training",
      "description": "LLaMA improves upon generative pre-training techniques for better language understanding."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "GPT",
      "description": "LLaMA builds on the principles established by GPT models."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Common Crawl",
      "description": "LLaMA is trained on the Common Crawl dataset to enhance its language capabilities."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Perplexity",
      "description": "The performance of LLaMA is evaluated using perplexity as a metric."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA is built on the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "BERT",
      "description": "LLaMA improves upon BERT by providing a more efficient model."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Fine-tuning",
      "description": "LLaMA utilizes fine-tuning to adapt to specific tasks."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Text-to-Text Transformer",
      "description": "LLaMA builds on the architecture of the Text-to-Text Transformer for its design."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Gopher",
      "description": "LLaMA is compared to Gopher in terms of performance and scaling."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Transfer Learning",
      "description": "LLaMA utilizes transfer learning techniques to enhance its capabilities."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA is built on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Transfer Learning",
      "description": "LLaMA utilizes transfer learning techniques."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Winogrande",
      "description": "LLaMA's performance is compared to the Winogrande dataset."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "SocialIQA",
      "description": "LLaMA's performance is compared to the SocialIQA dataset."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Generalization Error",
      "description": "LLaMA introduces methods to evaluate generalization error."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Bloom",
      "description": "LLaMA introduces the concept of an open-access multilingual language model."
    },
    {
      "type": "IMPROVES",
      "source": "Glu variants",
      "target": "Transformer",
      "description": "Glu variants are techniques that enhance the performance of transformer models."
    },
    {
      "type": "USES",
      "source": "Neural Machine Translation",
      "target": "Subword Units",
      "description": "Neural machine translation uses subword units to handle rare words."
    },
    {
      "type": "BUILDS_ON",
      "source": "Entropy",
      "target": "Communication Theory",
      "description": "The concept of entropy builds on the foundations of communication theory."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA builds on the Transformer architecture."
    },
    {
      "type": "USES",
      "source": "Megatron-LM",
      "target": "Model Parallelism",
      "description": "Megatron-LM uses model parallelism to train large models."
    },
    {
      "type": "USES",
      "source": "Megatron-Turing NLG 530B",
      "target": "DeepSpeed",
      "description": "Megatron-Turing NLG 530B uses DeepSpeed for training."
    },
    {
      "type": "EXTENDS",
      "source": "Roformer",
      "target": "Transformer",
      "description": "Roformer extends the Transformer architecture."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Megatron-Turing NLG 530B",
      "description": "LLaMA is compared to Megatron-Turing NLG 530B in terms of performance and efficiency."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA",
      "target": "Roformer",
      "description": "LLaMA extends the concepts introduced in Roformer by incorporating enhanced techniques."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Attention Mechanism",
      "description": "LLaMA builds on the attention mechanism introduced in previous works."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Lamda",
      "description": "LLaMA is compared to Lamda in terms of performance and application."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "GPT-J-6B",
      "description": "LLaMA builds on the architecture and principles established by GPT-J-6B."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA is built on the Transformer architecture."
    },
    {
      "type": "IMPROVES",
      "source": "Self-consistency",
      "target": "Chain of thought reasoning",
      "description": "Self-consistency enhances the reasoning capabilities of language models."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "CCNet",
      "description": "LLaMA utilizes the CCNet dataset for training."
    },
    {
      "type": "EXTENDS",
      "source": "Large Language Models",
      "target": "Emergent abilities",
      "description": "Large language models extend their capabilities with emergent abilities."
    },
    {
      "type": "INTRODUCES",
      "source": "Sustainable AI",
      "target": "Environmental implications",
      "description": "Sustainable AI introduces discussions on the environmental impacts of AI."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "GLM-130B",
      "description": "LLaMA is introduced in relation to GLM-130B as a new model."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "OPT",
      "description": "LLaMA is compared to OPT in terms of performance and efficiency."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Hellaswag",
      "description": "LLaMA uses the Hellaswag dataset for evaluation."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA",
      "target": "Root Mean Square Layer Normalization",
      "description": "LLaMA extends the technique of Root Mean Square Layer Normalization for better performance."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Natural Questions",
      "description": "LLaMA is evaluated on the Natural Questions dataset for question answering."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Trivia QA",
      "description": "LLaMA is evaluated on the Trivia QA dataset for question answering."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is compared to GPT-3 in the context of evaluation on question answering datasets."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "PaLM",
      "description": "LLaMA is compared to PaLM in the context of evaluation on question answering datasets."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Greedy Decoding",
      "description": "LLaMA uses greedy decoding to generate answers."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Exact Match",
      "description": "LLaMA uses the exact match metric to evaluate generated answers."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Natural Questions",
      "description": "LLaMA utilizes the Natural Questions dataset for training and evaluation."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Trivia QA",
      "description": "LLaMA utilizes the Trivia QA dataset for training and evaluation."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "1-shot setting",
      "description": "LLaMA improves performance in the 1-shot setting for question answering tasks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "GPT-3",
      "description": "LLaMA is compared to GPT-3 in terms of performance on various benchmarks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Gopher",
      "description": "LLaMA is compared to Gopher in terms of performance on various benchmarks."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Chinchilla",
      "description": "LLaMA is compared to Chinchilla in terms of performance on various benchmarks."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "MMLU",
      "description": "LLaMA is evaluated using the MMLU benchmark."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "High School Subjects",
      "description": "LLaMA evaluates performance across various high school subjects."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "STEM Scores",
      "description": "LLaMA's performance is compared to STEM scores in the evaluation."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Social Science Scores",
      "description": "LLaMA's performance is compared to social science scores in the evaluation."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Humanities Scores",
      "description": "LLaMA's performance is compared to humanities scores in the evaluation."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "High School Subjects",
      "description": "LLaMA utilizes the dataset of high school subjects to evaluate its performance."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Performance Scores",
      "description": "LLaMA's performance is compared against the performance scores of various subjects."
    },
    {
      "type": "INCLUDES",
      "source": "High School Subjects",
      "target": "Social Science",
      "description": "High School Subjects dataset includes various subjects categorized under Social Science."
    },
    {
      "type": "INCLUDES",
      "source": "High School Subjects",
      "target": "STEM",
      "description": "High School Subjects dataset includes various subjects categorized under STEM."
    },
    {
      "type": "INCLUDES",
      "source": "High School Subjects",
      "target": "Humanities",
      "description": "High School Subjects dataset includes various subjects categorized under Humanities."
    },
    {
      "type": "INCLUDES",
      "source": "High School Subjects",
      "target": "Other",
      "description": "High School Subjects dataset includes subjects categorized as Other."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Performance Scores",
      "description": "LLaMA introduces performance scores to evaluate its effectiveness across various domains."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Machine Learning",
      "description": "LLaMA utilizes machine learning techniques to enhance its language processing capabilities."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Humanities",
      "description": "LLaMA's performance is compared to various humanities disciplines to assess its applicability."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "STEM",
      "description": "LLaMA's performance is also compared to STEM fields to evaluate its versatility."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Other",
      "description": "LLaMA's performance is compared against other non-standard categories."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "Performance Scores",
      "description": "LLaMA's performance is evaluated using various metrics."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Humanities",
      "description": "LLaMA's performance is compared across different domains including Humanities."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Social Science",
      "description": "LLaMA's performance is compared across different domains including Social Science."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "STEM",
      "description": "LLaMA's performance is compared across different domains including STEM."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Professional Fields",
      "description": "LLaMA's performance is compared across different domains including Professional Fields."
    },
    {
      "type": "COMPARES_TO",
      "source": "LLaMA",
      "target": "Others",
      "description": "LLaMA's performance is compared across different domains including Others."
    },
    {
      "type": "EVALUATES",
      "source": "LLaMA",
      "target": "MMLU",
      "description": "LLaMA is evaluated using the MMLU benchmark."
    },
    {
      "type": "COMPARES_TO",
      "source": "MMLU",
      "target": "STEM",
      "description": "MMLU provides detailed results for the STEM domain."
    },
    {
      "type": "COMPARES_TO",
      "source": "MMLU",
      "target": "Social Science",
      "description": "MMLU provides detailed results for the Social Science domain."
    },
    {
      "type": "COMPARES_TO",
      "source": "MMLU",
      "target": "Others",
      "description": "MMLU provides detailed results for other domains."
    },
    {
      "type": "COMPARES_TO",
      "source": "MMLU",
      "target": "All",
      "description": "MMLU provides aggregated results across all domains."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA-65B",
      "target": "Fibonacci sequence",
      "description": "LLaMA-65B generates text that includes information about the Fibonacci sequence."
    },
    {
      "type": "USES",
      "source": "LLaMA-65B",
      "target": "Instruction finetuning",
      "description": "LLaMA-65B is evaluated without instruction finetuning in the examples provided."
    },
    {
      "type": "DISCOVERS",
      "source": "Leonardo of Pisa",
      "target": "Fibonacci sequence",
      "description": "Leonardo of Pisa discovered the Fibonacci sequence around 1200."
    },
    {
      "type": "PUBLISHES",
      "source": "Leonardo of Pisa",
      "target": "Liber abaci",
      "description": "Leonardo of Pisa published the Liber abaci in 1202, discussing the Fibonacci sequence."
    },
    {
      "type": "INTRODUCES",
      "source": "Yann Le Cun",
      "target": "Deep Learning Album",
      "description": "Yann Le Cun introduces his music album that combines various genres and reflects on deep learning."
    },
    {
      "type": "CREATES",
      "source": "Yann Le Cun",
      "target": "Deep Learning",
      "description": "Yann Le Cun creates the album 'Deep Learning' with personal lyrics about AI."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Deep Learning",
      "description": "LLaMA introduces concepts related to deep learning in its framework."
    },
    {
      "type": "USES",
      "source": "Le Cun",
      "target": "Deep Learning",
      "description": "Le Cun uses deep learning as a theme in his music."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "AI",
      "description": "LLaMA introduces new methodologies in the field of AI."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Training",
      "description": "LLaMA uses training techniques to improve its performance."
    },
    {
      "type": "EXTENDS",
      "source": "LLaMA",
      "target": "Network",
      "description": "LLaMA extends the concept of networks in AI to enhance learning capabilities."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "Perception",
      "description": "LLaMA improves the perception abilities of AI models."
    },
    {
      "type": "COMPARES_TO",
      "source": "Telegraph",
      "target": "Social Media",
      "description": "Gauss compares the telegraph to modern social media platforms like Facebook and Twitter."
    },
    {
      "type": "INTRODUCES",
      "source": "Telegraph",
      "target": "Geek",
      "description": "Gauss claims to be an early 'geek' for using the telegraph."
    },
    {
      "type": "IMPROVES",
      "source": "Gauss",
      "target": "Character Limit",
      "description": "Gauss argues that he communicated more effectively with fewer characters."
    },
    {
      "type": "USES",
      "source": "LLaMA-65B",
      "target": "Instruction Dataset",
      "description": "LLaMA-65B is fine-tuned using the Instruction Dataset."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "LLaMA-65B",
      "description": "The paper introduces the LLaMA-65B model."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "HTTP Request",
      "description": "The paper introduces the concept of making HTTP requests in JavaScript."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA: Open and Efficient Foundation Language Models",
      "target": "GET Request",
      "description": "The paper introduces the GET request technique in the context of JavaScript."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Fetch API",
      "description": "LLaMA introduces the use of the Fetch API for making GET requests."
    },
    {
      "type": "USES",
      "source": "LLaMA",
      "target": "Regular Expressions",
      "description": "LLaMA uses regular expressions to manipulate strings and extract information."
    },
    {
      "type": "INTRODUCES",
      "source": "LLaMA",
      "target": "Regular Expressions",
      "description": "LLaMA introduces the use of regular expressions for extracting function definitions."
    },
    {
      "type": "USES",
      "source": "extract_functions",
      "target": "Regular Expressions",
      "description": "The function 'extract_functions' uses regular expressions to find function definitions in Python code."
    },
    {
      "type": "COMPARES_TO",
      "source": "Sicilian Defense",
      "target": "French Defense",
      "description": "Both are popular chess openings that counter white's early attacks."
    },
    {
      "type": "COMPARES_TO",
      "source": "Caro-Kann Defense",
      "target": "Ruy Lopez",
      "description": "Both openings are widely used and focus on establishing a strong position."
    },
    {
      "type": "COMPARES_TO",
      "source": "Italian Game",
      "target": "Scotch Game",
      "description": "Both openings begin with 1.e4 e5 and are known for their aggressive play."
    },
    {
      "type": "COMPARES_TO",
      "source": "Italian Game",
      "target": "Scotch Game",
      "description": "Both openings are similar as they are classified as open games, but differ in their strategic approaches."
    },
    {
      "type": "DIFFERS_FROM",
      "source": "Italian Game",
      "target": "Scotch Game",
      "description": "They differ after white's third move, leading to different strategic goals."
    },
    {
      "type": "COMPARES_TO",
      "source": "Italian Game",
      "target": "Scotch Game",
      "description": "The Italian Game and Scotch Game lead to different strategic goals for white."
    },
    {
      "type": "USES",
      "source": "Language Models",
      "target": "Ethical Implications",
      "description": "The use of language models should consider ethical implications."
    },
    {
      "type": "USES",
      "source": "Language Models",
      "target": "Legal and Policy Constraints",
      "description": "The use of language models may be subject to legal and policy constraints."
    },
    {
      "type": "BUILDS_ON",
      "source": "LLaMA",
      "target": "Transformer",
      "description": "LLaMA builds on the Transformer architecture for language modeling."
    },
    {
      "type": "IMPROVES",
      "source": "LLaMA",
      "target": "BERT",
      "description": "LLaMA improves upon BERT by offering more efficient training and performance."
    },
    {
      "type": "INTRODUCES",
      "source": "Theory of Relativity",
      "target": "E = mc^2",
      "description": "The equation E = mc^2 is introduced as part of Einstein's Theory of Relativity."
    },
    {
      "type": "BUILDS_ON",
      "source": "Law of Photons",
      "target": "Quantum Mechanics",
      "description": "The Law of Photons contributes to the understanding of Quantum Mechanics."
    }
  ],
  "chunk_entity_map": {
    "vision_transformer.pdf_chunk_0": [
      "Transformer",
      "Attention Mechanism",
      "Convolutional Neural Networks (CNNs)",
      "Image Patches"
    ],
    "vision_transformer.pdf_chunk_1": [
      "Vision Transformer (ViT)",
      "Transformer",
      "ImageNet",
      "CIFAR-100",
      "VTAB",
      "Self-attention",
      "Computational Efficiency"
    ],
    "vision_transformer.pdf_chunk_2": [
      "Transformer",
      "CNN",
      "ResNet",
      "Self-Attention",
      "Large-scale image datasets",
      "Performance"
    ],
    "vision_transformer.pdf_chunk_3": [
      "Transformer",
      "Image Net",
      "Res Nets",
      "Image Patching",
      "Accuracy"
    ],
    "vision_transformer.pdf_chunk_4": [
      "Vision Transformer",
      "Fine-tuning",
      "ImageNet",
      "Transformer"
    ],
    "vision_transformer.pdf_chunk_5": [
      "Vision Transformer (ViT)",
      "ImageNet-21k",
      "JFT-300M",
      "Accuracy",
      "ImageNet",
      "ImageNet-ReaL",
      "CIFAR-100",
      "VTAB",
      "Large Scale Training",
      "Inductive Bias"
    ],
    "vision_transformer.pdf_chunk_6": [
      "Transformer",
      "BERT",
      "GPT",
      "self-attention",
      "local neighborhoods self-attention",
      "VTAB",
      "77.63%"
    ],
    "vision_transformer.pdf_chunk_7": [
      "Self-Attention",
      "Sparse Transformers",
      "Cordonnier et al. (2020)",
      "Multi-Head Dot-Product Self Attention",
      "Local Multi-Head Attention",
      "Attention in Blocks"
    ],
    "vision_transformer.pdf_chunk_8": [
      "ViT",
      "CNN",
      "Self-Attention",
      "Cordonnier et al. (2020)",
      "Image Classification",
      "Object Detection",
      "Bello et al. (2019)",
      "Hu et al. (2018)",
      "Carion et al. (2020)",
      "Wang et al. (2018)",
      "Sun et al. (2019)",
      "Large Scale Pre-training"
    ],
    "vision_transformer.pdf_chunk_9": [
      "image GPT (i GPT)",
      "Image Net",
      "accuracy",
      "standard benchmarks",
      "image recognition"
    ],
    "vision_transformer.pdf_chunk_10": [
      "Transformer",
      "ResNet",
      "ImageNet-21k",
      "JFT-300M",
      "CNN transfer learning",
      "performance"
    ],
    "vision_transformer.pdf_chunk_11": [
      "Vision Transformer (ViT)",
      "Multi-Head Attention",
      "Transformer",
      "MLP",
      "Position Embedding",
      "Patch",
      "Class Embedding"
    ],
    "vision_transformer.pdf_chunk_12": [
      "Transformer",
      "Vision Transformer (ViT)",
      "Position Embeddings",
      "Classification Token",
      "NLP Transformer",
      "Attention is All You Need"
    ],
    "vision_transformer.pdf_chunk_13": [
      "Transformer",
      "Token Embeddings",
      "Flattening",
      "BERT",
      "Patch Embeddings",
      "Latent Vector Size",
      "Image Patches"
    ],
    "vision_transformer.pdf_chunk_14": [
      "Transformer",
      "BERT",
      "Multiheaded Self-Attention (MSA)",
      "MLP",
      "Layernorm (LN)",
      "Position Embeddings",
      "Image Representation"
    ],
    "vision_transformer.pdf_chunk_15": [
      "Transformer encoder",
      "Multiheaded Self-Attention (MSA)",
      "MLP blocks",
      "Layernorm (LN)",
      "Residual connections"
    ],
    "vision_transformer.pdf_chunk_16": [
      "Vision Transformer (ViT)",
      "MLP",
      "GELU",
      "Inductive Bias",
      "Self-Attention",
      "Position Embeddings",
      "Locality",
      "Translation Equivariance"
    ],
    "vision_transformer.pdf_chunk_17": [
      "ViT",
      "Hybrid Architecture",
      "Fine-tuning",
      "Large datasets",
      "Position Embeddings",
      "Classification Input Embedding"
    ],
    "vision_transformer.pdf_chunk_18": [
      "Vision Transformer (ViT)",
      "Fine-tuning",
      "Higher Resolution Training",
      "Patch Extraction",
      "Position Embeddings",
      "Large Datasets"
    ],
    "vision_transformer.pdf_chunk_19": [
      "Vision Transformer (ViT)",
      "ResNet",
      "ILSVRC-2012 ImageNet",
      "Self-supervision",
      "State of the art"
    ],
    "vision_transformer.pdf_chunk_20": [
      "ViT",
      "ILSVRC-2012 ImageNet",
      "ImageNet-21k",
      "JFT",
      "CIFAR-10",
      "CIFAR-100",
      "Oxford-IIIT Pets",
      "Oxford Flowers-102",
      "Self-supervised learning"
    ],
    "vision_transformer.pdf_chunk_21": [
      "ViT-Base",
      "ViT-Large",
      "ViT-Huge",
      "VTAB",
      "Low-data transfer",
      "Geometric understanding",
      "BERT"
    ],
    "vision_transformer.pdf_chunk_22": [
      "ViT-L/16",
      "BERT",
      "ResNet",
      "Group Normalization",
      "Standardized Convolutions",
      "ResNet (BiT)"
    ],
    "vision_transformer.pdf_chunk_23": [
      "Res Net (Bi T)",
      "Vi T",
      "Adam",
      "SGD",
      "Weight Decay",
      "Res Net 50",
      "Linear Learning Rate Warmup and Decay"
    ],
    "vision_transformer.pdf_chunk_24": [
      "ViT-L/16",
      "ViT-H/14",
      "ImageNet",
      "SGD",
      "Adam",
      "Fine-tuning accuracy",
      "Few-shot accuracy",
      "Polyak & Juditsky averaging"
    ],
    "vision_transformer.pdf_chunk_25": [
      "ViT-H/14",
      "ViT-L/16",
      "Big Transfer (BiT)",
      "Noisy Student",
      "ImageNet",
      "JFT-300M",
      "CNN",
      "few-shot accuracy"
    ],
    "vision_transformer.pdf_chunk_26": [
      "Noisy Student",
      "Bi T-L",
      "Vi T-L/16",
      "Vi T-H/14",
      "ImageNet",
      "CIFAR-100",
      "VTAB suite",
      "JFT-300 M",
      "TPUv3"
    ],
    "vision_transformer.pdf_chunk_27": [
      "Vision Transformer (ViT)",
      "JFT-300M",
      "ImageNet",
      "CIFAR-10",
      "CIFAR-100",
      "Oxford-IIIT Pets",
      "Oxford Flowers-102",
      "Accuracy",
      "ResNet",
      "EfficientNet"
    ],
    "vision_transformer.pdf_chunk_28": [
      "Vision Transformer",
      "JFT-300 M",
      "Image Net-21 k",
      "Accuracy",
      "VTAB",
      "ResNet"
    ],
    "vision_transformer.pdf_chunk_29": [
      "Vi T-L/16",
      "Vi T-H/14",
      "Bi T",
      "VIVI",
      "S 4 L",
      "Image Net-21k",
      "VTAB",
      "Transformer",
      "performance vs. compute",
      "training schedule",
      "optimizer",
      "weight decay"
    ],
    "vision_transformer.pdf_chunk_30": [
      "ViT-H/14",
      "BiT-R152x4",
      "JFT-300M",
      "ImageNet",
      "ImageNet-21k",
      "Regularization",
      "ViT-Large",
      "ViT-Base"
    ],
    "vision_transformer.pdf_chunk_31": [
      "Image Net",
      "Image Net-21k",
      "JFT-300M",
      "ViT-Large",
      "ViT-Base",
      "performance"
    ],
    "vision_transformer.pdf_chunk_32": [
      "ViT",
      "ResNet",
      "ImageNet",
      "ImageNet-21k",
      "JFT-300M",
      "Top 1 Accuracy",
      "Linear 5-shot Evaluation"
    ],
    "vision_transformer.pdf_chunk_33": [
      "Vi T",
      "Vi T-b",
      "Res Net",
      "Bi T",
      "Image Net",
      "JFT-300 M",
      "Transfer accuracy",
      "Hybrid"
    ],
    "vision_transformer.pdf_chunk_34": [
      "JFT-300M",
      "Vision Transformer (ViT)",
      "ResNet",
      "validation accuracy",
      "few-shot linear accuracy",
      "ViT-B/32",
      "ResNet 50",
      "ResNet 152 x 2",
      "ViT-L/16"
    ],
    "vision_transformer.pdf_chunk_35": [
      "ViT",
      "ImageNet",
      "VTAB",
      "few-shot results",
      "low-data transfer"
    ],
    "vision_transformer.pdf_chunk_36": [
      "ResNet",
      "Vision Transformer (ViT)",
      "JFT-300M",
      "transfer performance",
      "Hybrid Model"
    ],
    "vision_transformer.pdf_chunk_37": [
      "Vision Transformer (ViT)",
      "ResNet",
      "performance/compute trade-off",
      "5 datasets",
      "hybrids"
    ],
    "vision_transformer.pdf_chunk_38": [
      "Vision Transformer",
      "Attention Mechanism",
      "Position Embedding",
      "Principal Components"
    ],
    "vision_transformer.pdf_chunk_39": [
      "ViT",
      "Self-attention",
      "Attention distance",
      "Position embeddings",
      "CNN"
    ],
    "vision_transformer.pdf_chunk_40": [
      "Attention Distance",
      "Self-Supervised Pre-Training",
      "Transformer",
      "ResNet",
      "Attention Heads"
    ],
    "vision_transformer.pdf_chunk_41": [
      "ViT-L/16",
      "ViT-L/32",
      "Cosine Similarity",
      "Mean Attention Distance",
      "Transformer",
      "RGB Embedding Filters",
      "Position Embedding"
    ],
    "vision_transformer.pdf_chunk_42": [
      "ViT-B/16",
      "ImageNet",
      "Masked Patch Prediction",
      "BERT",
      "Self-Supervised Pre-Training",
      "Contrastive Pre-Training",
      "Transformer"
    ],
    "vision_transformer.pdf_chunk_43": [
      "Vision Transformer",
      "self-attention",
      "Transformer",
      "large datasets",
      "self-supervised pre-training",
      "image classification",
      "detection",
      "segmentation"
    ],
    "vision_transformer.pdf_chunk_44": [
      "ViT",
      "Self-supervised pre-training",
      "Large-scale supervised pre-training",
      "Berlin",
      "Z\u00fcrich",
      "Amsterdam",
      "Andreas Steiner",
      "Joan Puigcerver",
      "Maxim Neumann",
      "Dmitry Lepikhin",
      "Aravindh Mahendran",
      "Daniel Keysers",
      "Mario Lu\u010di\u0107",
      "Noam Shazeer",
      "Ashish Vaswani",
      "Colin Raffel"
    ],
    "vision_transformer.pdf_chunk_45": [
      "Transformer",
      "Attention Mechanism",
      "Mutual Information",
      "NeurIPS",
      "ACL"
    ],
    "vision_transformer.pdf_chunk_46": [
      "Transformer",
      "ImageNet",
      "Attention Augmented Convolutional Networks",
      "End-to-End Object Detection with Transformers",
      "Generative Pretraining from Pixels",
      "Adaptive Input Representations",
      "Few-Shot Learning"
    ],
    "vision_transformer.pdf_chunk_47": [
      "Transformer",
      "ImageNet",
      "BERT",
      "Contrastive Learning",
      "UNITER",
      "Self-Attention"
    ],
    "vision_transformer.pdf_chunk_48": [
      "Transformer",
      "BERT",
      "Deep Residual Learning",
      "Momentum Contrast",
      "Axial Attention",
      "Image Database",
      "Convolutional Neural Networks",
      "Robustness",
      "Transferability"
    ],
    "vision_transformer.pdf_chunk_49": [
      "Transformer",
      "Contrastive Predictive Coding",
      "Relation Networks",
      "Local Relation Networks",
      "Ccnet"
    ],
    "vision_transformer.pdf_chunk_50": [
      "Transformer",
      "Batch Normalization",
      "Adam",
      "Big Transfer (BiT)",
      "ImageNet",
      "Accuracy",
      "Deep Convolutional Neural Networks"
    ],
    "vision_transformer.pdf_chunk_51": [
      "Transformer",
      "Conditional Computation",
      "Gshard",
      "Visual BERT",
      "ViLBERT",
      "Object-centric Learning",
      "Slot Attention"
    ],
    "vision_transformer.pdf_chunk_52": [
      "Transformer",
      "Weakly Supervised Pretraining",
      "Cats and Dogs",
      "Weight Standardization",
      "Stochastic Approximation",
      "Image Transformer"
    ],
    "vision_transformer.pdf_chunk_53": [
      "Transformer",
      "Weight Standardization",
      "Language Models",
      "Unsupervised Learning",
      "VideoBERT",
      "Data Effectiveness",
      "Self-Attention"
    ],
    "vision_transformer.pdf_chunk_54": [
      "Transformer",
      "Self-supervised learning",
      "EfficientNet",
      "Axial-DeepLab",
      "ICLR",
      "CVPR",
      "NIPS",
      "Attention is All You Need"
    ],
    "vision_transformer.pdf_chunk_55": [
      "Transformer",
      "Axial Attention",
      "Axial-DeepLab",
      "Non-local Neural Networks",
      "Visual Transformers",
      "Panoptic Segmentation Dataset",
      "Panoptic Quality"
    ],
    "vision_transformer.pdf_chunk_56": [
      "Transformer",
      "Self-Supervised Learning",
      "ImageNet",
      "Group Normalization",
      "Noisy Student",
      "Visual Task Adaptation"
    ],
    "vision_transformer.pdf_chunk_57": [
      "ViT-B/{16,32}",
      "ViT-L/32",
      "ViT-L/16",
      "ViT-H/14",
      "R50",
      "R101",
      "R152",
      "JFT-300M",
      "ImageNet-21k",
      "ImageNet",
      "Learning Rate Decay",
      "Gradient Clipping"
    ],
    "vision_transformer.pdf_chunk_58": [
      "Multihead Self-Attention",
      "Self-Attention",
      "Vision Transformer (ViT)",
      "ImageNet",
      "Gradient Clipping",
      "Neural Architecture"
    ],
    "vision_transformer.pdf_chunk_59": [
      "ViT (Vision Transformer)",
      "ImageNet",
      "Dropout",
      "SGD (Stochastic Gradient Descent)",
      "Fine-tuning",
      "MSA (Multi-Head Self-Attention)"
    ],
    "vision_transformer.pdf_chunk_60": [
      "Transformer",
      "Pets",
      "Flowers",
      "CIFAR",
      "Image Net",
      "Learning Rate Sweep",
      "Res Nets",
      "Hybrid Models"
    ],
    "vision_transformer.pdf_chunk_61": [
      "ViT (Vision Transformer)",
      "ImageNet",
      "CIFAR-100",
      "CIFAR-10",
      "Oxford-IIIT Pets",
      "Oxford Flowers-102",
      "VTAB",
      "Cosine Learning Rate Decay",
      "Batch Size",
      "Gradient Clipping",
      "ResNet"
    ],
    "vision_transformer.pdf_chunk_62": [
      "Vision Transformer (ViT)",
      "VTAB",
      "Self-Supervision",
      "VTAB score",
      "Linear Layer"
    ],
    "vision_transformer.pdf_chunk_63": [
      "Vision Transformer",
      "Masked Patch Prediction",
      "JFT",
      "Mean Color",
      "Adam"
    ],
    "vision_transformer.pdf_chunk_64": [
      "Adam",
      "few-shot performance",
      "JFT",
      "masked patch prediction",
      "cosine learning rate decay",
      "L2 regression",
      "corruption rate"
    ],
    "vision_transformer.pdf_chunk_65": [
      "ViT",
      "Image Net",
      "Image Net-21k",
      "JFT-300M",
      "transfer performance",
      "masked patch prediction"
    ],
    "vision_transformer.pdf_chunk_66": [
      "Vision Transformer (ViT)",
      "ImageNet",
      "CIFAR-10",
      "CIFAR-100",
      "Oxford Flowers-102",
      "Oxford-IIIT Pets",
      "Top-1 Accuracy",
      "Transformer"
    ],
    "vision_transformer.pdf_chunk_67": [
      "Vision Transformer",
      "Oxford Flowers-102",
      "Oxford-IIIT-Pets",
      "Image Net",
      "Image Net-21k",
      "JFT 300 M",
      "Top 1 accuracy",
      "ResNet"
    ],
    "vision_transformer.pdf_chunk_68": [
      "ViT-H",
      "ResNet-50",
      "ResNet-101",
      "ResNet-152",
      "R50",
      "Transfer Accuracy",
      "FLOPs",
      "Vision Transformer (ViT)"
    ],
    "vision_transformer.pdf_chunk_69": [
      "ViT",
      "ResNet",
      "SGD",
      "Adam",
      "transfer accuracy",
      "FLOPs"
    ],
    "vision_transformer.pdf_chunk_70": [
      "Vision Transformer",
      "ResNet-50",
      "ResNet-152",
      "ImageNet",
      "CIFAR-10",
      "CIFAR-100",
      "Oxford-IIIT Pets",
      "Oxford Flowers-102",
      "Adam",
      "SGD",
      "Accuracy"
    ],
    "vision_transformer.pdf_chunk_71": [
      "SGD",
      "Adam",
      "Res Nets",
      "JFT",
      "ViT",
      "5-shot performance",
      "ImageNet",
      "Transformer",
      "absolute numbers"
    ],
    "vision_transformer.pdf_chunk_72": [
      "Transformer",
      "Scaling",
      "Compute",
      "DMLP",
      "Class Token",
      "Patch Size"
    ],
    "vision_transformer.pdf_chunk_73": [
      "Transformer",
      "Multi-layer Perceptron (MLP)",
      "ResNet",
      "Class Token",
      "Global Average Pooling (GAP)"
    ],
    "vision_transformer.pdf_chunk_74": [
      "ViT-B/16",
      "ImageNet",
      "5-shot linear accuracy",
      "Positional Embedding",
      "Global Average Pooling (GAP)",
      "Class-Token"
    ],
    "vision_transformer.pdf_chunk_75": [
      "Positional Embedding",
      "1-dimensional Positional Embedding",
      "2-dimensional Positional Embedding",
      "Relative Positional Embeddings",
      "Transformer"
    ],
    "vision_transformer.pdf_chunk_76": [
      "Relative Attention",
      "Positional Embeddings",
      "Attention Mechanism",
      "Vision Transformer"
    ],
    "vision_transformer.pdf_chunk_77": [
      "ViT (Vision Transformer)",
      "Positional Embeddings",
      "Cosine Similarity",
      "Transformer Encoder",
      "Learning Rate (LR)",
      "Weight Decay (WD)"
    ],
    "vision_transformer.pdf_chunk_78": [
      "ViT-B/16",
      "Positional Embedding",
      "Patch-Level Inputs",
      "Performance"
    ],
    "vision_transformer.pdf_chunk_79": [
      "ViT-L/16",
      "Mean attention distance",
      "Vision Transformer",
      "Positional encoding",
      "Attention mechanism"
    ],
    "vision_transformer.pdf_chunk_80": [
      "Transformer",
      "Vision Transformer (ViT)",
      "FLOPs",
      "Empirical Computational Costs"
    ],
    "vision_transformer.pdf_chunk_81": [
      "ViT",
      "ResNet",
      "inference speed",
      "batch size",
      "TPUv3"
    ],
    "vision_transformer.pdf_chunk_82": [
      "ViT-B/32",
      "ViT-L/32",
      "ViT-B/16",
      "ViT-L/16",
      "ViT-H/14",
      "ResNet",
      "Axial Attention"
    ],
    "vision_transformer.pdf_chunk_83": [
      "Axial Res Net",
      "ViT",
      "Axial Attention",
      "Axial Transformer blocks",
      "ImageNet"
    ],
    "vision_transformer.pdf_chunk_84": [
      "Axial Res Net",
      "Axial-Vi T-B/32",
      "Axial-Vi T-B/16",
      "ImageNet",
      "JFT",
      "FLOPs",
      "inference time",
      "column-self-attention",
      "MLP"
    ],
    "vision_transformer.pdf_chunk_85": [
      "Axial ViT",
      "ViT",
      "ResNet",
      "ImageNet",
      "top-1 accuracy",
      "Axial Attention"
    ],
    "vision_transformer.pdf_chunk_86": [
      "Axial Transformer",
      "Axial Res Net",
      "Attention Distance",
      "ViT (Vision Transformer)",
      "Self-Attention"
    ],
    "vision_transformer.pdf_chunk_87": [
      "ViT-L/16",
      "ViT-H/14",
      "Object Net",
      "top-5 accuracy",
      "top-1 accuracy",
      "Attention Rollout",
      "VTAB-1k"
    ],
    "vision_transformer.pdf_chunk_88": [
      "Transformer",
      "Image Recognition",
      "ImageNet",
      "Accuracy",
      "Vision Transformer (ViT)"
    ],
    "vision_transformer.pdf_chunk_89": [
      "ViT-H/14",
      "ViT-L/16",
      "ViT-L/16 (I21k)",
      "JFT",
      "VTAB-1k",
      "Caltech 101",
      "CIFAR-100",
      "Flowers 102",
      "Mean"
    ],
    "diffusion_ddpm.pdf_chunk_0": [
      "Denoising Diffusion Probabilistic Models",
      "Denoising Score Matching",
      "Langevin Dynamics",
      "CIFAR 10",
      "Inception Score",
      "FID Score",
      "LSUN",
      "Progressive GAN"
    ],
    "diffusion_ddpm.pdf_chunk_1": [
      "Denoising Diffusion Probabilistic Models",
      "Generative Adversarial Networks (GANs)",
      "Autoregressive Models",
      "Flows",
      "Variational Autoencoders (VAEs)",
      "Energy-based Modeling",
      "Score Matching",
      "Celeb A-HQ",
      "CIFAR 10",
      "Sample Quality"
    ],
    "diffusion_ddpm.pdf_chunk_2": [
      "Denoising Diffusion Probabilistic Models"
    ],
    "diffusion_ddpm.pdf_chunk_3": [
      "Denoising Diffusion Probabilistic Models"
    ],
    "diffusion_ddpm.pdf_chunk_4": [],
    "diffusion_ddpm.pdf_chunk_5": [
      "Denoising Diffusion Probabilistic Models"
    ],
    "diffusion_ddpm.pdf_chunk_6": [
      "Denoising Diffusion Probabilistic Models",
      "Diffusion Probabilistic Models"
    ],
    "diffusion_ddpm.pdf_chunk_7": [
      "Diffusion Probabilistic Models",
      "Variational Inference",
      "Diffusion Model",
      "Markov Chain",
      "Gaussian Noise",
      "Neural Network"
    ],
    "diffusion_ddpm.pdf_chunk_8": [
      "Diffusion Models",
      "Denoising Score Matching",
      "Annealed Langevin Dynamics",
      "Log Likelihood"
    ],
    "diffusion_ddpm.pdf_chunk_9": [
      "Denoising Diffusion Probabilistic Models",
      "Annealed Importance Sampling",
      "Log Likelihood",
      "Lossless Codelengths",
      "Lossy Compression",
      "Progressive Decoding",
      "Autoregressive Models",
      "Markov Chain",
      "Latent Variable Models"
    ],
    "diffusion_ddpm.pdf_chunk_10": [
      "Denoising Diffusion Probabilistic Models",
      "Markov Chain",
      "Variational Inference",
      "Negative Log Likelihood",
      "Gaussian Distribution",
      "Forward Process",
      "Reverse Process"
    ],
    "diffusion_ddpm.pdf_chunk_11": [
      "Denoising Diffusion Probabilistic Models",
      "Reparameterization",
      "Gaussian Conditionals",
      "L",
      "q(xt|x0)"
    ],
    "diffusion_ddpm.pdf_chunk_12": [
      "Denoising Diffusion Probabilistic Models",
      "Stochastic Gradient Descent",
      "KL Divergence",
      "Gaussian Distribution",
      "Denoising Autoencoders"
    ],
    "diffusion_ddpm.pdf_chunk_13": [
      "Diffusion Models",
      "Denoising Score Matching",
      "Denoising Diffusion Probabilistic Models",
      "Model Architecture",
      "Weighted Variational Bound Objective",
      "Forward Process",
      "Reverse Process"
    ],
    "diffusion_ddpm.pdf_chunk_14": [
      "Forward process",
      "Reverse process",
      "Reparameterization",
      "Denoising Diffusion Probabilistic Models",
      "Entropy",
      "N(xt\u22121; \u00b5\u03b8(xt, t),\u03a3\u03b8(xt, t))"
    ],
    "diffusion_ddpm.pdf_chunk_15": [
      "Denoising Diffusion Probabilistic Models",
      "\u00b5\u03b8(xt, t)",
      "Lt",
      "Forward Process Posterior",
      "\u03c32t",
      "\u03b1t",
      "\u03b2t"
    ],
    "diffusion_ddpm.pdf_chunk_16": [
      "Denoising Diffusion Probabilistic Models",
      "\u03f5\u03b8",
      "Training Algorithm",
      "Sampling Algorithm",
      "N(0, I)",
      "xt",
      "xt\u22121",
      "\u03c3t",
      "\u03b2t",
      "\u03b1t",
      "\u00af\u03b1t"
    ],
    "diffusion_ddpm.pdf_chunk_17": [
      "Denoising Score Matching",
      "Variational Inference",
      "Langevin Dynamics",
      "Reverse Process Mean Function Approximator",
      "\u03f5-Prediction Parameterization",
      "Diffusion Process"
    ],
    "diffusion_ddpm.pdf_chunk_18": [
      "Denoising Diffusion Probabilistic Models",
      "\u03f5-prediction parameterization",
      "Langevin dynamics",
      "Denoising score matching",
      "Neural network reverse process",
      "Discrete log likelihoods",
      "Image data",
      "Gaussian N(x 0; \u00b5\u03b8(x 1,1),\u03c32 1 I)"
    ],
    "diffusion_ddpm.pdf_chunk_19": [
      "Denoising Diffusion Probabilistic Models",
      "Gaussian Distribution",
      "Variational Bound",
      "Conditional Autoregressive Model",
      "Codelength",
      "Discrete Data"
    ],
    "diffusion_ddpm.pdf_chunk_20": [
      "Denoising Diffusion Probabilistic Models",
      "Variational Bound",
      "Reverse Process",
      "Decoder",
      "Log Likelihood"
    ],
    "diffusion_ddpm.pdf_chunk_21": [
      "Denoising Diffusion Probabilistic Models",
      "CIFAR 10",
      "IS",
      "FID",
      "EBM",
      "JEM",
      "Big GAN",
      "Style GAN 2 + ADA",
      "Gated Pixel CNN",
      "Sparse Transformer",
      "Pixel IQN",
      "NCSNv 2",
      "NCSN",
      "SNGAN",
      "SNGAN-DDLS"
    ],
    "diffusion_ddpm.pdf_chunk_22": [
      "FID",
      "Denoising Diffusion Probabilistic Models",
      "NCSN",
      "Lsimple",
      "\u03b1t",
      "\u03b2t"
    ],
    "diffusion_ddpm.pdf_chunk_23": [
      "Denoising Diffusion Probabilistic Models",
      "NCSN",
      "Weighted Variational Bound",
      "Denoising",
      "Sample Quality",
      "T"
    ],
    "diffusion_ddpm.pdf_chunk_24": [
      "Denoising Diffusion Probabilistic Models",
      "U-Net",
      "Pixel CNN++",
      "Transformer",
      "signal-to-noise ratio",
      "DKL"
    ],
    "diffusion_ddpm.pdf_chunk_25": [
      "Denoising Diffusion Probabilistic Models",
      "CIFAR 10",
      "FID score",
      "Inception score",
      "Transformer",
      "Self-attention"
    ],
    "diffusion_ddpm.pdf_chunk_26": [
      "Denoising Diffusion Probabilistic Models",
      "CIFAR 10",
      "Celeb A-HQ",
      "LSUN",
      "FID",
      "Sending",
      "Receiving"
    ],
    "diffusion_ddpm.pdf_chunk_27": [
      "Denoising Diffusion Probabilistic Models",
      "Variational Bound",
      "Mean Squared Error",
      "CIFAR 10",
      "CIFAR 10 models",
      "Reverse Process Parameterizations",
      "Fixed Variances",
      "Learning Reverse Process Variances",
      "Predicting \u03f5",
      "Codelengths"
    ],
    "diffusion_ddpm.pdf_chunk_28": [
      "Denoising Diffusion Probabilistic Model",
      "CIFAR 10",
      "Codelength",
      "Rate",
      "Distortion",
      "Root Mean Squared Error (RMSE)",
      "Annealed Importance Sampling",
      "Energy Based Models",
      "Likelihood-based Generative Models"
    ],
    "diffusion_ddpm.pdf_chunk_29": [
      "root mean squared error",
      "progressive lossy compression",
      "Algorithm 3",
      "Algorithm 4",
      "rate-distortion behavior",
      "minimal random coding"
    ],
    "diffusion_ddpm.pdf_chunk_30": [
      "Denoising Diffusion Probabilistic Models",
      "CIFAR 10",
      "Root Mean Squared Error (RMSE)",
      "Rate-Distortion"
    ],
    "diffusion_ddpm.pdf_chunk_31": [
      "CIFAR 10",
      "RMSE",
      "Progressive Generation",
      "Denoising Diffusion Probabilistic Models"
    ],
    "diffusion_ddpm.pdf_chunk_32": [
      "Denoising Diffusion Probabilistic Models",
      "CIFAR 10",
      "Celeb A-HQ",
      "Sample Quality Metrics",
      "Variational Bound"
    ],
    "diffusion_ddpm.pdf_chunk_33": [
      "Denoising Diffusion Probabilistic Models",
      "Diffusion Process",
      "p\u03b8(xt\u22121|xt)",
      "Forward Process",
      "Blank Image"
    ],
    "diffusion_ddpm.pdf_chunk_34": [
      "Denoising Diffusion Probabilistic Models",
      "Gaussian diffusion model",
      "Autoregressive model",
      "Celeb A-HQ",
      "DKL"
    ],
    "diffusion_ddpm.pdf_chunk_35": [
      "Gaussian diffusion",
      "Interpolation",
      "Stochastic encoder",
      "Reverse process",
      "32x32x3 images",
      "256x256x3 images"
    ],
    "diffusion_ddpm.pdf_chunk_36": [
      "Denoising Diffusion Probabilistic Models",
      "Celeb A-HQ",
      "\u03f5-prediction reverse process",
      "Mutual Information",
      "Interpolation"
    ],
    "diffusion_ddpm.pdf_chunk_37": [
      "Denoising Diffusion Probabilistic Models",
      "Denoising Score Matching",
      "Variational Inference",
      "Langevin Dynamics",
      "Infusion Training",
      "Variational Walkback",
      "Generative Stochastic Networks"
    ],
    "diffusion_ddpm.pdf_chunk_38": [
      "Score Matching",
      "Energy-Based Modeling",
      "Rate-Distortion Curves",
      "Annealed Importance Sampling",
      "Convolutional DRAW",
      "Progressive Decoding",
      "Autoregressive Models"
    ],
    "diffusion_ddpm.pdf_chunk_39": [
      "Diffusion Models",
      "Variational Inference",
      "Denoising Score Matching",
      "Annealed Langevin Dynamics",
      "Autoregressive Models",
      "Progressive Lossy Compression",
      "Inductive Biases",
      "Generative Models",
      "GANs",
      "Flows"
    ],
    "diffusion_ddpm.pdf_chunk_40": [
      "Denoising Diffusion Probabilistic Models",
      "Generative Models",
      "Large Datasets",
      "CNN"
    ],
    "diffusion_ddpm.pdf_chunk_41": [
      "Generative Models",
      "Biases in Datasets",
      "Data Compression",
      "Diffusion Models",
      "Representation Learning",
      "Image Classification",
      "Reinforcement Learning",
      "Creative Uses"
    ],
    "diffusion_ddpm.pdf_chunk_42": [
      "Denoising Diffusion Probabilistic Models",
      "Diffusion Models",
      "ONR PECASE",
      "NSF Graduate Research Fellowship",
      "Google\u2019s Tensor Flow Research Cloud (TFRC)"
    ],
    "diffusion_ddpm.pdf_chunk_43": [
      "Denoising Diffusion Probabilistic Models",
      "GAN",
      "Energy-based model",
      "Neural Ordinary Differential Equations",
      "Pixel SNAIL",
      "Sparse Transformers"
    ],
    "diffusion_ddpm.pdf_chunk_44": [
      "Denoising Diffusion Probabilistic Models",
      "Residual Energy-Based Models",
      "Non-linear Independent Components Estimation (NICE)",
      "Real NVP",
      "Energy-Based Models",
      "Generative Conv Nets"
    ],
    "diffusion_ddpm.pdf_chunk_45": [
      "Denoising Diffusion Probabilistic Models",
      "Generative Adversarial Nets",
      "Flow Contrastive Estimation",
      "Variational Walkback",
      "FFJORD"
    ],
    "diffusion_ddpm.pdf_chunk_46": [
      "Denoising Diffusion Probabilistic Models",
      "International Conference on Learning Representations",
      "Advances In Neural Information Processing Systems",
      "Twenty-Second Annual IEEE Conference on Computational Complexity"
    ],
    "diffusion_ddpm.pdf_chunk_47": [
      "Denoising Diffusion Probabilistic Models",
      "Diffusion Process",
      "Nash Equilibrium",
      "GAN",
      "beta-VAE",
      "International Conference on Learning Representations",
      "Advances in Neural Information Processing Systems"
    ],
    "diffusion_ddpm.pdf_chunk_48": [
      "Denoising Diffusion Probabilistic Models",
      "International Conference on Learning Representations",
      "International Conference on Machine Learning",
      "Flow++",
      "Video Pixel Networks",
      "Neural Audio Synthesis"
    ],
    "diffusion_ddpm.pdf_chunk_49": [
      "Denoising Diffusion Probabilistic Models",
      "Generative Adversarial Networks (GANs)",
      "Progressive Growing of GANs",
      "Style-based Generator Architecture"
    ],
    "diffusion_ddpm.pdf_chunk_50": [
      "Denoising Diffusion Probabilistic Models",
      "Generative Adversarial Networks",
      "Stochastic Optimization",
      "Glow",
      "Auto-encoding Variational Bayes",
      "Image Quality"
    ],
    "diffusion_ddpm.pdf_chunk_51": [
      "Denoising Diffusion Probabilistic Models",
      "Variational Inference",
      "Hamiltonian Monte Carlo",
      "Auto-encoding Variational Bayes",
      "Inverse Autoregressive Flow",
      "Energy-inspired Models"
    ],
    "diffusion_ddpm.pdf_chunk_52": [
      "Denoising Diffusion Probabilistic Models",
      "Generative Modeling",
      "Spectral Normalization",
      "VQ-DRAW",
      "Energy-Based Models"
    ],
    "diffusion_ddpm.pdf_chunk_53": [
      "Denoising Diffusion Probabilistic Models",
      "Maximum Likelihood Learning",
      "Energy-Based Models",
      "Autoregressive Quantile Networks",
      "Wave Glow",
      "VQ-VAE"
    ],
    "diffusion_ddpm.pdf_chunk_54": [
      "Denoising Diffusion Probabilistic Models",
      "Variational Inference",
      "VQ-VAE-2",
      "U-Net",
      "Stochastic Backpropagation",
      "Weight Normalization"
    ],
    "diffusion_ddpm.pdf_chunk_55": [
      "Denoising Diffusion Probabilistic Models",
      "Weight Normalization",
      "Markov Chain Monte Carlo",
      "Variational Inference"
    ],
    "diffusion_ddpm.pdf_chunk_56": [
      "Denoising Diffusion Probabilistic Models",
      "Generative modeling",
      "Pixel CNN++",
      "A-NICE-MC",
      "Deep unsupervised learning"
    ],
    "diffusion_ddpm.pdf_chunk_57": [
      "Denoising Diffusion Probabilistic Models",
      "Wave Net",
      "Pixel Recurrent Neural Networks",
      "Pixel CNN",
      "Score-based generative models"
    ],
    "diffusion_ddpm.pdf_chunk_58": [
      "Denoising Diffusion Probabilistic Models",
      "Score Matching",
      "Pixel CNN",
      "Attention Mechanism",
      "Denoising Autoencoders",
      "CNN",
      "Advances in Neural Information Processing Systems",
      "IEEE Conference on Computer Vision and Pattern Recognition"
    ],
    "diffusion_ddpm.pdf_chunk_59": [
      "Denoising Diffusion Probabilistic Models",
      "Predictive sampling",
      "Stochastic normalizing flows",
      "Group normalization",
      "Generative ConvNet"
    ],
    "diffusion_ddpm.pdf_chunk_60": [
      "Denoising Diffusion Probabilistic Models",
      "LSUN",
      "Learning descriptor networks",
      "Energy-based spatial-temporal generative convnets",
      "Wide residual networks"
    ],
    "diffusion_ddpm.pdf_chunk_61": [
      "Denoising Diffusion Probabilistic Models",
      "Progressive GAN",
      "Style GAN",
      "Style GAN 2",
      "LSUN",
      "FID",
      "Variational Bound",
      "Lossy Compression",
      "Minimal Random Coding",
      "CIFAR 10"
    ],
    "diffusion_ddpm.pdf_chunk_62": [
      "Denoising Diffusion Probabilistic Models",
      "CIFAR 10",
      "Rate-Distortion",
      "RMSE"
    ],
    "diffusion_ddpm.pdf_chunk_63": [
      "Denoising Diffusion Probabilistic Models",
      "Divergence KL",
      "Pixel CNN++",
      "U-Net",
      "Wide Res Net",
      "Group Normalization",
      "H(x 0)"
    ],
    "diffusion_ddpm.pdf_chunk_64": [
      "CIFAR 10",
      "LSUN",
      "Celeb A-HQ",
      "Convolutional Residual Blocks",
      "Self-Attention Blocks",
      "Diffusion Probabilistic Models",
      "Training Steps",
      "Parameters",
      "TPU v3-8"
    ],
    "diffusion_ddpm.pdf_chunk_65": [
      "Denoising Diffusion Probabilistic Models",
      "Celeb A-HQ",
      "LSUN Bedroom",
      "LSUN Cat",
      "LSUN Church",
      "CIFAR 10 sample quality",
      "\u03b2t schedule",
      "Neural Network"
    ],
    "diffusion_ddpm.pdf_chunk_66": [
      "Denoising Diffusion Probabilistic Models",
      "CIFAR 10",
      "Dropout",
      "Random Horizontal Flips",
      "Adam",
      "RMSProp",
      "Learning Rate",
      "Pixel CNN++",
      "LSUN Bedroom"
    ],
    "diffusion_ddpm.pdf_chunk_67": [
      "CIFAR 10",
      "LSUN",
      "Celeb A-HQ",
      "FID",
      "Inception Score",
      "EMA",
      "Denoising Diffusion Probabilistic Models"
    ],
    "diffusion_ddpm.pdf_chunk_68": [
      "Denoising Diffusion Probabilistic Models",
      "NCSN",
      "U-Net",
      "Re\ufb01ne Net",
      "Latent Variable Model",
      "Sinusoidal Position Embedding",
      "Diffusion Process",
      "LSUN"
    ],
    "diffusion_ddpm.pdf_chunk_69": [
      "Denoising Diffusion Probabilistic Models",
      "NCSN",
      "Langevin-like sampler",
      "Markov chain",
      "Variational inference",
      "DKL(q(x T|x 0) \u2225N(0, I))"
    ],
    "diffusion_ddpm.pdf_chunk_70": [
      "Denoising Diffusion Probabilistic Models",
      "Variational Inference",
      "NCSN",
      "Celeb A-HQ",
      "CIFAR 10",
      "LSUN",
      "Latent Structure",
      "Reverse Process Stochasticity",
      "Quality Metric"
    ],
    "diffusion_ddpm.pdf_chunk_71": [
      "Celeb A",
      "Denoising Diffusion Probabilistic Models",
      "Latent Variables",
      "Interpolation",
      "High-level Attributes"
    ],
    "diffusion_ddpm.pdf_chunk_72": [
      "Denoising Diffusion Probabilistic Models",
      "CIFAR 10",
      "Celeb A-HQ",
      "Inception Score",
      "FID"
    ],
    "diffusion_ddpm.pdf_chunk_73": [
      "Denoising Diffusion Probabilistic Models",
      "Celeb A-HQ",
      "CIFAR 10",
      "LSUN",
      "FID"
    ],
    "lora.pdf_chunk_0": [
      "Low-Rank Adaptation",
      "GPT-3",
      "Transformer"
    ],
    "lora.pdf_chunk_1": [
      "LoRA",
      "GPT-3",
      "RoBERTa",
      "DeBERTa",
      "GPT-2",
      "Training Throughput",
      "Transformer"
    ],
    "lora.pdf_chunk_2": [
      "RoBERTa",
      "DeBERTa",
      "GPT-2",
      "GPT-3",
      "Fine-tuning",
      "Low-Rank Adaptation (LoRA)"
    ],
    "lora.pdf_chunk_3": [
      "GPT-3",
      "Low-Rank Adaptation (LoRA)",
      "GLUE",
      "Adapter Latency"
    ],
    "lora.pdf_chunk_4": [
      "Low-Rank Adaptation (LoRA)",
      "GPT-3",
      "Intrinsic Rank",
      "Over-Parametrized Models",
      "Inference Latency"
    ],
    "lora.pdf_chunk_5": [
      "LoRA",
      "GPT-3",
      "Rank",
      "Dense Layers",
      "Task Switching",
      "Adaptive Optimizers"
    ],
    "lora.pdf_chunk_6": [
      "LoRA",
      "Transformer",
      "Adaptive Optimizers",
      "Inference Latency",
      "Prefix-Tuning"
    ],
    "lora.pdf_chunk_7": [
      "Transformer",
      "LoRA",
      "autoregressive language model",
      "conditional probabilities",
      "Adam"
    ],
    "lora.pdf_chunk_8": [
      "P\u03a6(y|x)",
      "GPT",
      "Transformer",
      "Z",
      "summarization",
      "machine reading comprehension (MRC)",
      "natural language to SQL (NL 2 SQL)"
    ],
    "lora.pdf_chunk_9": [
      "GPT-3",
      "Full Fine-Tuning",
      "Low-Rank Adaptation",
      "\u03a60",
      "\u2206\u03a6",
      "\u0398",
      "Conditional Language Modeling Objective"
    ],
    "lora.pdf_chunk_10": [
      "GPT-3",
      "Low-Rank Adaptation",
      "Transfer Learning",
      "Adapter Layers",
      "Input Layer Activations Optimization",
      "LoRA: Low-Rank Adaptation of Large Language Models"
    ],
    "lora.pdf_chunk_11": [
      "Low-Rank Adaptation",
      "Transformer",
      "Adapter Layers",
      "BERT",
      "GPT",
      "GLUE",
      "Inference Latency"
    ],
    "lora.pdf_chunk_12": [
      "Low-Rank Adaptation",
      "GPT-2",
      "Adapter Layers",
      "FLOPs",
      "Shoeybi et al. (2020)",
      "Lepikhin et al. (2020)"
    ],
    "lora.pdf_chunk_13": [
      "LoRA",
      "Prefix Tuning",
      "Large Language Models",
      "All Reduce",
      "Broadcast",
      "Shoeybi et al. (2020)",
      "Lepikhin et al. (2020)",
      "Li & Liang (2021)"
    ],
    "lora.pdf_chunk_14": [
      "LoRA",
      "GPT-2",
      "Transformer",
      "Inference Latency",
      "Adapter Tuning",
      "NVIDIA Quadro RTX 8000",
      "Trainable Parameters"
    ],
    "lora.pdf_chunk_15": [
      "Transformer",
      "Low-Rank Adaptation",
      "Intrinsic Dimension",
      "Intrinsic Rank",
      "Neural Network",
      "Rank"
    ],
    "lora.pdf_chunk_16": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "W0",
      "\u0394W",
      "B",
      "A",
      "Adam",
      "\u03b1"
    ],
    "lora.pdf_chunk_17": [
      "LoRA",
      "Full Fine-tuning",
      "Adapter-based methods",
      "Prefix-based methods",
      "Inference Latency"
    ],
    "lora.pdf_chunk_18": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "W",
      "W0",
      "BA",
      "B\u2032A\u2032"
    ],
    "lora.pdf_chunk_19": [
      "LoRA",
      "Transformer",
      "Self-Attention Module",
      "MLP Module",
      "Parameter Efficiency"
    ],
    "lora.pdf_chunk_20": [
      "Transformer",
      "LoRA",
      "GPT-3",
      "VRAM usage",
      "Checkpoint size",
      "MLP layers",
      "Layer Norm layers"
    ],
    "lora.pdf_chunk_21": [
      "LoRA",
      "GPT-3",
      "Speedup",
      "VRAM"
    ],
    "lora.pdf_chunk_22": [
      "LoRA",
      "RoBERTa",
      "DeBERTa",
      "GPT-2",
      "GPT-3",
      "GLUE",
      "WikiSQL",
      "SAMSum",
      "NVIDIA Tesla V100"
    ],
    "lora.pdf_chunk_23": [
      "GPT-2",
      "GPT-3",
      "Fine-Tuning (FT)",
      "Low-Rank Adaptation (LoRA)",
      "NVIDIA Tesla V100",
      "Training Throughput",
      "350 GB Model"
    ],
    "lora.pdf_chunk_24": [
      "GPT-3",
      "LoRA",
      "Training Throughput",
      "V100 GPU"
    ],
    "lora.pdf_chunk_25": [
      "RoBERTa",
      "LoRA",
      "MNLI",
      "SST-2",
      "MRPC",
      "CoLA",
      "QNLI",
      "QQP",
      "RTE",
      "STS-B",
      "Accuracy"
    ],
    "lora.pdf_chunk_26": [
      "Ro BERTabase",
      "Ro BERTalarge",
      "De BERTa XXL",
      "Low-Rank Adaptation (LoRA)",
      "GLUE",
      "Accuracy",
      "Matthew\u2019s correlation",
      "Pearson correlation"
    ],
    "lora.pdf_chunk_27": [
      "LoRA",
      "Bias-only or Bit Fit",
      "Pre\ufb01x-embedding tuning",
      "Pre\ufb01x-layer tuning",
      "lp",
      "li",
      "Large Language Models"
    ],
    "lora.pdf_chunk_28": [
      "Prefix-layer tuning",
      "Adapter tuning",
      "Adapter H",
      "Adapter L",
      "Trainable parameters",
      "Transformer"
    ],
    "lora.pdf_chunk_29": [
      "LoRA",
      "Adapter H",
      "Adapter L",
      "Adapter P",
      "Adapter D",
      "\u0398",
      "Wq",
      "Wv"
    ],
    "lora.pdf_chunk_30": [
      "LoRA",
      "Large Language Models",
      "Trainable Parameters",
      "Rank",
      "Weight Matrices"
    ],
    "lora.pdf_chunk_31": [
      "GPT-2 M",
      "GPT-2 L",
      "LoRA",
      "BLEU",
      "NIST",
      "MET",
      "ROUGE-L",
      "CIDEr",
      "E2E NLG Challenge"
    ],
    "lora.pdf_chunk_32": [
      "LoRA",
      "GPT-2",
      "RoBERTa",
      "BERT",
      "E2E NLG Challenge",
      "GLUE",
      "Hugging Face Transformers"
    ],
    "lora.pdf_chunk_33": [
      "RoBERTa base",
      "RoBERTa large",
      "GLUE",
      "LoRA",
      "Adapters",
      "MRPC",
      "RTE",
      "STS-B",
      "MNLI"
    ],
    "lora.pdf_chunk_34": [
      "De BERTa",
      "De BERTa XXL",
      "LoRA",
      "GPT-2",
      "GLUE",
      "Super GLUE",
      "E2E NLG Challenge"
    ],
    "lora.pdf_chunk_35": [
      "E2E NLG Challenge",
      "Web NLG",
      "DART",
      "Low-Rank Adaptation",
      "Li & Liang (2021)"
    ],
    "lora.pdf_chunk_36": [
      "GPT-3",
      "LoRA",
      "Wiki SQL",
      "MNLI-m",
      "SAMSum",
      "Accuracy",
      "Rouge-1",
      "Rouge-2",
      "Rouge-L"
    ],
    "lora.pdf_chunk_37": [
      "GPT-3",
      "LoRA",
      "Wiki SQL",
      "MNLI-m",
      "SAMSum",
      "Standard Deviation",
      "Performance Drop"
    ],
    "lora.pdf_chunk_38": [
      "LoRA",
      "GPT-3",
      "Wiki SQL",
      "Multi NLI",
      "Validation Accuracy",
      "Fine-Tune",
      "Prefix Embed",
      "Prefix Layer",
      "Adapter(H)"
    ],
    "lora.pdf_chunk_39": [
      "LoRA",
      "Transformer",
      "BERT",
      "GPT-2",
      "Wiki SQL",
      "MNLI-matched"
    ],
    "lora.pdf_chunk_40": [
      "GPT-3",
      "Prompt Engineering",
      "Fine-Tuning",
      "Transformer",
      "Low-Rank Adaptation"
    ],
    "lora.pdf_chunk_41": [
      "GPT-3",
      "Fine-tuning",
      "Prompt Engineering",
      "Parameter-Efficient Adaptation",
      "Adapter Layers",
      "Low-Rank Adaptation"
    ],
    "lora.pdf_chunk_42": [
      "LoRA",
      "COMPACTER",
      "Prompt Engineering",
      "Adapter Layers"
    ],
    "lora.pdf_chunk_43": [
      "Low-Rank Adaptation",
      "Low-Rank Structures",
      "Large Language Models",
      "Prompt Engineering",
      "Li & Liang (2021)",
      "Lester et al. (2021)",
      "Hambardzumyan et al. (2020)",
      "Liu et al. (2021)",
      "Li et al. (2016)",
      "Cai et al. (2010)",
      "Li et al. (2018)",
      "Grasedyck et al. (2013)",
      "Oymak et al. (2019)"
    ],
    "lora.pdf_chunk_44": [
      "Low-Rank Adaptation",
      "Low-Rank Constraint",
      "Neural Network",
      "Neural Tangent Kernels",
      "Over-Parametrized Neural Network"
    ],
    "lora.pdf_chunk_45": [
      "Low-Rank Adaptation",
      "GPT-3",
      "Adversarial Training",
      "Low-Rank Structure",
      "Trainable Parameters Reduction"
    ],
    "lora.pdf_chunk_46": [
      "GPT-3",
      "LoRA",
      "trainable parameters",
      "Transformer"
    ],
    "lora.pdf_chunk_47": [
      "LoRA",
      "GPT-3",
      "Transformer",
      "Parameter Budget",
      "Self-Attention Module"
    ],
    "lora.pdf_chunk_48": [
      "LoRA",
      "GPT-3",
      "Wiki SQL",
      "Multi NLI",
      "Validation accuracy",
      "Attention weights"
    ],
    "lora.pdf_chunk_49": [
      "LoRA",
      "Wiki SQL",
      "Multi NLI",
      "Validation accuracy",
      "Wq",
      "Wv",
      "Wk",
      "Wo",
      "r"
    ],
    "lora.pdf_chunk_50": [
      "Low-Rank Adaptation (LoRA)",
      "GPT-2",
      "Wiki SQL",
      "Multi NLI",
      "Validation Accuracy",
      "Update Matrix (\u2206W)",
      "Intrinsic Rank"
    ],
    "lora.pdf_chunk_51": [
      "LoRA",
      "Large Language Models",
      "r",
      "Pre-training",
      "Downstream Task"
    ],
    "lora.pdf_chunk_52": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Subspace Similarity",
      "Grassmann Distance",
      "Adaptation Matrices",
      "Right-Singular Unitary Matrices",
      "48th Layer"
    ],
    "lora.pdf_chunk_53": [
      "Low-Rank Adaptation (LoRA)",
      "Large Language Models",
      "Subspace Similarity",
      "Transformer",
      "Ar",
      "\u2206Wq",
      "\u2206Wv"
    ],
    "lora.pdf_chunk_54": [
      "GPT-3",
      "Low-Rank Adaptation (LoRA)",
      "Normalized Similarity",
      "Ar=8",
      "Ar=64",
      "Intrinsic Rank"
    ],
    "lora.pdf_chunk_55": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Intrinsic Rank",
      "\u2206W",
      "W",
      "\u2206Wq",
      "\u2206Wv",
      "Gaussian Matrices"
    ],
    "lora.pdf_chunk_56": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Frobenius norm",
      "W",
      "\u2206W",
      "U",
      "V",
      "Ar",
      "A'r"
    ],
    "lora.pdf_chunk_57": [
      "GPT-3",
      "Frobenius norm",
      "Low-Rank Adaptation (LoRA)",
      "Singular vectors",
      "Weight matrices",
      "Random matrix"
    ],
    "lora.pdf_chunk_58": [
      "LoRA",
      "Large Language Models",
      "Ampli\ufb01cation Factor",
      "Task-Switching",
      "Section H.4",
      "Section H.3"
    ],
    "lora.pdf_chunk_59": [
      "LoRA",
      "Transformer",
      "Neural Networks"
    ],
    "lora.pdf_chunk_60": [
      "LoRA",
      "Rank-de\ufb01ciency"
    ],
    "lora.pdf_chunk_61": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Adversarial Training",
      "Over-parameterization",
      "Layer Normalization"
    ],
    "lora.pdf_chunk_62": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Semeval-2017",
      "Semantic Textual Similarity",
      "Deep Neural Networks"
    ],
    "lora.pdf_chunk_63": [
      "BERT",
      "Low-Rank Adaptation",
      "Deep Neural Networks",
      "ICML"
    ],
    "lora.pdf_chunk_64": [
      "Low-Rank Adaptation (LoRA)",
      "Large Language Models",
      "Bidirectional Transformers",
      "WebNLG",
      "Paraphrase Quality"
    ],
    "lora.pdf_chunk_65": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Samsum corpus",
      "Grassmann Discriminant Analysis",
      "Word-level Adversarial Re Programming (WARP)"
    ],
    "lora.pdf_chunk_66": [
      "LoRA",
      "DeBERTa",
      "Parameter-Efficient Transfer Learning",
      "Low-Rank Expansions",
      "Stochastic Optimization"
    ],
    "lora.pdf_chunk_67": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Stochastic Optimization",
      "Gshard",
      "Prompt Tuning",
      "Prefix-Tuning",
      "Overparameterized Neural Networks"
    ],
    "lora.pdf_chunk_68": [
      "LoRA",
      "Low-Rank Adaptation",
      "Stochastic Gradient Descent",
      "Advances in Neural Information Processing Systems",
      "International Conference on Machine Learning",
      "Conference On Learning Theory",
      "Findings of the Association for Computational Linguistics: EMNLP"
    ],
    "lora.pdf_chunk_69": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "EMNLP 2020",
      "Association for Computational Linguistics"
    ],
    "lora.pdf_chunk_70": [
      "LoRA",
      "GPT",
      "Roberta",
      "Decoupled weight decay regularization",
      "Compacter",
      "DART"
    ],
    "lora.pdf_chunk_71": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "e2e dataset",
      "Dart",
      "Adapter-fusion",
      "Semi-orthogonal low-rank matrix factorization",
      "Generalization guarantees"
    ],
    "lora.pdf_chunk_72": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "SQuAD",
      "Unanswerable Questions",
      "Transformer"
    ],
    "lora.pdf_chunk_73": [
      "Low-Rank Adaptation",
      "Megatron-LM",
      "Adapters",
      "Sentiment Treebank",
      "Low-Rank Matrix Factorization"
    ],
    "lora.pdf_chunk_74": [
      "Low-Rank Adaptation",
      "Large Language Models"
    ],
    "lora.pdf_chunk_75": [
      "LoRA",
      "Transformer",
      "GLUE",
      "SuperGLUE",
      "Neural Network Acceptability Judgments",
      "Broad-Coverage Challenge Corpus"
    ],
    "lora.pdf_chunk_76": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Broad-Coverage Challenge Corpus",
      "North American Chapter of the Association for Computational Linguistics",
      "Transformers"
    ],
    "lora.pdf_chunk_77": [
      "Transformer",
      "Low-Rank Adaptation (LoRA)",
      "BitFit",
      "Low-Rank Matrix Factorization",
      "Infinite-Width Neural Networks"
    ],
    "lora.pdf_chunk_78": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Few-shot Learning",
      "Prompt Engineering",
      "ICASSP"
    ],
    "lora.pdf_chunk_79": [
      "GPT-3",
      "RTE",
      "MNLI",
      "Fine-tuning",
      "Few-shot learning"
    ],
    "lora.pdf_chunk_80": [
      "GPT-3",
      "GPT-2",
      "Few-Shot Learning",
      "Fine-Tuning",
      "Adapter Layers",
      "LoRA",
      "Validation Accuracy",
      "MNLI",
      "RTE"
    ],
    "lora.pdf_chunk_81": [
      "LoRA",
      "Adapter H",
      "Adapter L",
      "Latency",
      "NVIDIA Quadro RTX 8000",
      "Batch Size",
      "Sequence Length",
      "Adapter Bottleneck Dimension"
    ],
    "lora.pdf_chunk_82": [
      "LoRA",
      "GLUE Benchmark",
      "MNLI",
      "SST-2",
      "MRPC",
      "CoLA",
      "QNLI",
      "QQP",
      "RTE"
    ],
    "lora.pdf_chunk_83": [
      "GLUE",
      "RoBERTa",
      "DeBERTa",
      "WikiSQL",
      "SAMSum",
      "STS-B"
    ],
    "lora.pdf_chunk_84": [
      "E2E NLG Challenge",
      "E2E",
      "DART",
      "Low-Rank Adaptation"
    ],
    "lora.pdf_chunk_85": [
      "DART",
      "Web NLG",
      "Adam W",
      "learning rate decay schedule",
      "RoBERTa"
    ],
    "lora.pdf_chunk_86": [
      "LoRA",
      "RoBERTa",
      "DeBERTa",
      "MNLI",
      "MRPC",
      "RTE",
      "STS-B",
      "Median",
      "AdamW"
    ],
    "lora.pdf_chunk_87": [
      "LoRA",
      "DEBERTA",
      "MNLI",
      "MRPC",
      "RTE",
      "STS-B",
      "median",
      "AdamW",
      "linear learning rate decay"
    ],
    "lora.pdf_chunk_88": [
      "LoRA",
      "RoBERTa base",
      "RoBERTa large",
      "MNLI",
      "SST-2",
      "MRPC",
      "CoLA",
      "QNLI",
      "QQP",
      "RTE",
      "STS-B",
      "AdamW",
      "Learning Rate",
      "Bottleneck"
    ],
    "lora.pdf_chunk_89": [
      "RoBERTa",
      "GPT-2",
      "Low-Rank Adaptation (LoRA)",
      "GLUE",
      "AdamW",
      "Transformer"
    ],
    "lora.pdf_chunk_90": [
      "GPT-2",
      "GPT-3",
      "LoRA",
      "Adam W",
      "Mean over random seeds",
      "Batch size",
      "Learning rate",
      "Weight decay factor",
      "Sequence length"
    ],
    "lora.pdf_chunk_91": [
      "LoRA",
      "De BERTa XXL",
      "GPT-2",
      "MNLI",
      "SST-2",
      "MRPC",
      "CoLA",
      "QNLI",
      "QQP",
      "RTE",
      "STS-B",
      "E2E",
      "Web NLG",
      "DART",
      "Learning Rate",
      "Batch Size",
      "Weight Decay",
      "Dropout Probability"
    ],
    "lora.pdf_chunk_92": [
      "LoRA",
      "GPT-2",
      "GPT-3",
      "E2E",
      "WebNLG",
      "DART",
      "WikiSQL",
      "MNLI",
      "SAMSum",
      "Validation Performance"
    ],
    "lora.pdf_chunk_93": [
      "LoRA",
      "Prefix Tuning",
      "Prefix Embedding Tuning",
      "Prefix Layer Tuning",
      "Wiki SQL",
      "MNLI"
    ],
    "lora.pdf_chunk_94": [
      "LoRA",
      "GPT-3",
      "Wiki SQL",
      "Multi NLI",
      "pre\ufb01x-embedding tuning",
      "Learning Rate",
      "Batch Size",
      "Epoch"
    ],
    "lora.pdf_chunk_95": [
      "LoRA",
      "Wiki SQL",
      "Multi NLI",
      "Prefix-Embedding Tuning (PE)",
      "Prefix-Layer Tuning (PL)",
      "GPT-2",
      "DART",
      "Web NLG",
      "Human Baseline",
      "E2E NLG Challenge"
    ],
    "lora.pdf_chunk_96": [
      "GPT-2 Medium",
      "GPT-2 Large",
      "LoRA",
      "BLEU",
      "MET",
      "TER",
      "E2E NLG Challenge"
    ],
    "lora.pdf_chunk_97": [
      "GPT-2 Medium",
      "GPT-2 Large",
      "Fine-Tune",
      "Adapter L",
      "FTTop 2",
      "Pre\ufb01x",
      "LoRA",
      "Web NLG",
      "BLEU",
      "MET",
      "TER"
    ],
    "lora.pdf_chunk_98": [
      "LoRA",
      "GPT-3",
      "MNLI",
      "MNLI-100",
      "Fine-Tune",
      "Pre\ufb01x Embed",
      "Pre\ufb01x Layer"
    ],
    "lora.pdf_chunk_99": [
      "LoRA",
      "Fine-Tuning",
      "Pre\ufb01x Embed",
      "MNLI-100",
      "MNLI-Full",
      "MNLI-1k",
      "MNLI-10k",
      "Subspace Similarity"
    ],
    "lora.pdf_chunk_100": [
      "Projection Metric",
      "Low-Rank Adaptation",
      "UiA",
      "UjB",
      "Subspace Similarity"
    ],
    "lora.pdf_chunk_101": [
      "LoRA",
      "LoRA+PE",
      "Large Language Models",
      "Wiki SQL",
      "MNLI-m",
      "Accuracy",
      "Prefix Embed",
      "Prefix Layer",
      "Adapter H"
    ],
    "lora.pdf_chunk_102": [
      "LoRA",
      "GPT-3",
      "MNLI",
      "Validation Accuracy",
      "Prefix Embedding Tuning",
      "Prefix Layer Tuning"
    ],
    "lora.pdf_chunk_103": [
      "GPT-3",
      "MNLI",
      "LoRA",
      "Projection Metric"
    ],
    "lora.pdf_chunk_104": [
      "LoRA",
      "GPT-3",
      "MNLI",
      "Similarity",
      "Learning Rate",
      "Batch Size",
      "Epoch"
    ],
    "lora.pdf_chunk_105": [
      "Low-Rank Adaptation",
      "GPT-2",
      "GPT-3",
      "E2E NLG Challenge",
      "Validation Loss",
      "Test Metrics",
      "Optimal Rank",
      "Subspace Similarity",
      "Low-Rank Matrices"
    ],
    "lora.pdf_chunk_106": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Subspace Similarity",
      "Task-Specific Directions"
    ],
    "lora.pdf_chunk_107": [
      "Transformer",
      "Low-Rank Adaptation (LoRA)",
      "Amplification Factor",
      "96-layer Transformer",
      "SVD Decomposition"
    ],
    "lora.pdf_chunk_108": [
      "Low-Rank Adaptation",
      "Large Language Models",
      "Accuracy",
      "\u2206W",
      "r",
      "Amplification Factor"
    ],
    "lora.pdf_chunk_109": [
      "LoRA",
      "Transformer",
      "BLEU",
      "NIST",
      "METEOR",
      "ROUGE L",
      "CIDEr"
    ],
    "lora.pdf_chunk_110": [
      "LoRA",
      "GPT-2 Medium",
      "GPT-3",
      "Validation Loss",
      "BLEU",
      "E2E NLG Challenge",
      "Hyperparameters"
    ],
    "lora.pdf_chunk_111": [
      "Low-Rank Adaptation (LoRA)",
      "Large Language Models",
      "Normalized Subspace Similarity",
      "Wq",
      "\u2206Wq"
    ],
    "flash_attention.pdf_chunk_0": [
      "Self-Attention",
      "Approximate Attention",
      "IO-Aware Attention",
      "Flash Attention"
    ],
    "flash_attention.pdf_chunk_1": [
      "Flash Attention",
      "block-sparse attention",
      "BERT-large",
      "GPT-2",
      "long-range arena",
      "end-to-end wall-clock speedup"
    ],
    "flash_attention.pdf_chunk_2": [
      "Transformer",
      "Flash Attention",
      "Perplexity",
      "Path-X",
      "Path-256",
      "Accuracy"
    ],
    "flash_attention.pdf_chunk_3": [
      "Attention",
      "Approximate Attention Methods",
      "Sparse-Approximation",
      "Low-Rank Approximation",
      "IO-Awareness",
      "FlashAttention"
    ],
    "flash_attention.pdf_chunk_4": [
      "FlashAttention",
      "IO-Awareness",
      "GPU",
      "SRAM",
      "HBM"
    ],
    "flash_attention.pdf_chunk_5": [
      "Flash Attention",
      "GPT-2",
      "PyTorch",
      "Speedup",
      "SRAM",
      "HBM",
      "DRAM"
    ],
    "flash_attention.pdf_chunk_6": [
      "Flash Attention",
      "GPT-2",
      "Speedup",
      "Transformer",
      "IO-aware algorithms",
      "HBM",
      "PyTorch",
      "TensorFlow"
    ],
    "flash_attention.pdf_chunk_7": [
      "Flash Attention",
      "Softmax Reduction",
      "CUDA",
      "Intermediate Attention Matrix",
      "HBM"
    ],
    "flash_attention.pdf_chunk_8": [
      "Flash Attention",
      "FLOPs",
      "GPT-2",
      "IO complexity",
      "HBM",
      "SRAM",
      "Standard Attention"
    ],
    "flash_attention.pdf_chunk_9": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "IO Complexity",
      "Approximate Attention Algorithms",
      "Multi-GPU",
      "Kernel Regression",
      "Sparsity Ratio"
    ],
    "flash_attention.pdf_chunk_10": [
      "Flash Attention",
      "BERT-large",
      "GPT-2",
      "Megatron-LM",
      "Long-Range Arena",
      "Perplexity"
    ],
    "flash_attention.pdf_chunk_11": [
      "Flash Attention",
      "GPT-2",
      "Path-X",
      "Path-256",
      "perplexity",
      "Block-sparse Flash Attention"
    ],
    "flash_attention.pdf_chunk_12": [
      "Flash Attention",
      "Approximate Attention",
      "Linformer",
      "GPU",
      "A100 GPU",
      "GPU Memory Hierarchy"
    ],
    "flash_attention.pdf_chunk_13": [
      "A100 GPU",
      "High Bandwidth Memory (HBM)",
      "On-chip SRAM",
      "Compute-bound operations",
      "Memory-bound operations",
      "FlashAttention"
    ],
    "flash_attention.pdf_chunk_14": [
      "Compute-bound",
      "Memory-bound",
      "Kernel fusion",
      "Arithmetic intensity"
    ],
    "flash_attention.pdf_chunk_15": [
      "Kernel Fusion",
      "Memory-Bound Operations",
      "FlashAttention"
    ],
    "flash_attention.pdf_chunk_16": [
      "FlashAttention",
      "GPT-2",
      "Attention Mechanism",
      "Standard Attention Implementation",
      "Memory Complexity"
    ],
    "flash_attention.pdf_chunk_17": [
      "Flash Attention",
      "Standard Attention Implementation",
      "FLOPs",
      "HBM",
      "Masking",
      "Dropout",
      "Softmax"
    ],
    "flash_attention.pdf_chunk_18": [
      "Flash Attention",
      "IO Complexity",
      "HBM",
      "Block-Sparse Attention",
      "Wall-Clock Time"
    ],
    "flash_attention.pdf_chunk_19": [
      "Tiling",
      "Recomputation",
      "Attention Output",
      "FlashAttention",
      "Sub-quadratic HBM accesses",
      "Softmax"
    ],
    "flash_attention.pdf_chunk_20": [
      "softmax with scaling",
      "numerical stability",
      "softmax",
      "max",
      "exponential function"
    ],
    "flash_attention.pdf_chunk_21": [
      "FlashAttention",
      "Softmax",
      "Recomputation",
      "Attention Mechanism",
      "Q-K-V"
    ],
    "flash_attention.pdf_chunk_22": [
      "Selective Gradient Checkpointing",
      "Kernel Fusion",
      "FlashAttention",
      "FLOPs",
      "CUDA",
      "HBM"
    ],
    "flash_attention.pdf_chunk_23": [
      "Flash Attention",
      "HBM",
      "on-chip SRAM",
      "block sizes",
      "Q-K-V",
      "S",
      "m",
      "\u2113"
    ],
    "flash_attention.pdf_chunk_24": [
      "FlashAttention",
      "IO Complexity",
      "FLOPs",
      "HBM",
      "Softmax",
      "Algebraic Aggregation"
    ],
    "flash_attention.pdf_chunk_25": [
      "Flash Attention",
      "Standard Attention",
      "GPT-2",
      "GFLOPs",
      "HBM R/W (GB)",
      "Runtime (ms)",
      "A100 GPU",
      "Block-Sparse Flash Attention"
    ],
    "flash_attention.pdf_chunk_26": [
      "Flash Attention",
      "block-sparse Flash Attention",
      "HBM accesses",
      "SRAM",
      "sequence length (N)",
      "head dimension (d)",
      "size of SRAM (M)"
    ],
    "flash_attention.pdf_chunk_27": [
      "Flash Attention",
      "HBM accesses",
      "Exact Attention",
      "Lower-bound",
      "N",
      "d",
      "M"
    ],
    "flash_attention.pdf_chunk_28": [
      "FlashAttention",
      "HBM accesses",
      "FLOP count",
      "block size",
      "parameterized complexity",
      "streaming algorithms"
    ],
    "flash_attention.pdf_chunk_29": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "IO complexity",
      "Mask matrix (~M)",
      "Q, K, V",
      "S",
      "P",
      "O"
    ],
    "flash_attention.pdf_chunk_30": [
      "Block-sparse Flash Attention",
      "IO complexity",
      "Block sparsity mask",
      "\u0398(\ud835\udc41\ud835\udc51)",
      "Sparsity",
      "Butterfly sparsity"
    ],
    "flash_attention.pdf_chunk_31": [
      "Flash Attention",
      "BERT",
      "GPT-2",
      "Megatron",
      "LRA benchmark",
      "Training Speed",
      "Attention Runtime",
      "Memory Benchmarks"
    ],
    "flash_attention.pdf_chunk_32": [
      "GPT-2",
      "Megatron",
      "Long-Range Arena (LRA)",
      "Perplexity",
      "Path-X",
      "Path-256",
      "Flash Attention"
    ],
    "flash_attention.pdf_chunk_33": [
      "Flash Attention",
      "Block-sparse Flash Attention",
      "BERT",
      "Wikipedia",
      "Training Speed",
      "Memory Footprint",
      "Runtime"
    ],
    "flash_attention.pdf_chunk_34": [
      "Flash Attention",
      "BERT-large",
      "GPT-2",
      "Open Webtext",
      "Training Speed",
      "Nvidia MLPerf 1.1",
      "Hugging Face",
      "Megatron-LM"
    ],
    "flash_attention.pdf_chunk_35": [
      "Flash Attention",
      "GPT-2 small",
      "GPT-2 medium",
      "Huggingface",
      "Megatron-LM",
      "Open Web Text",
      "Perplexity",
      "Transformer"
    ],
    "flash_attention.pdf_chunk_36": [
      "GPT-2 medium",
      "Flash Attention",
      "Block-sparse Flash Attention",
      "Long-Range Arena (LRA)",
      "Speedup",
      "Transformer"
    ],
    "flash_attention.pdf_chunk_37": [
      "Transformer",
      "Flash Attention",
      "Block-sparse Flash Attention",
      "Linformer",
      "Linear Attention",
      "Performer",
      "Local Attention",
      "Reformer",
      "Smyrf",
      "GPT-2",
      "Perplexity",
      "Megatron-LM"
    ],
    "flash_attention.pdf_chunk_38": [
      "GPT-2",
      "Megatron-LM",
      "Flash Attention",
      "MIMIC-III",
      "ECt HR",
      "Perplexity",
      "Transformer"
    ],
    "flash_attention.pdf_chunk_39": [
      "FlashAttention",
      "LRA accuracy",
      "unit patient discharge summaries",
      "ECt HR"
    ],
    "flash_attention.pdf_chunk_40": [
      "FlashAttention",
      "Block-Sparse Flash Attention",
      "PyTorch Attention",
      "Megatron Attention",
      "Linformer Attention",
      "OpenAI Sparse Attention",
      "RoBERTa",
      "MIMIC",
      "European Court of Human Rights (ECtHR)",
      "Memory Footprint",
      "Runtime"
    ],
    "flash_attention.pdf_chunk_41": [
      "RoBERTa",
      "MIMIC-III",
      "ECt HR",
      "micro F1",
      "Flash Attention",
      "Linformer",
      "Performer",
      "Local Attention",
      "Reformer",
      "SMYRF"
    ],
    "flash_attention.pdf_chunk_42": [
      "Transformer",
      "Linformer",
      "Linear Attention",
      "Performer",
      "Local Attention",
      "Reformer",
      "SMYRF",
      "Flash Attention",
      "Block-sparse Flash Attention",
      "Path-X",
      "Path-256",
      "Performance"
    ],
    "flash_attention.pdf_chunk_43": [
      "Flash Attention",
      "Block-sparse Flash Attention",
      "Path-64",
      "Path-X",
      "Path-256",
      "Accuracy",
      "Transformer",
      "A100 GPU"
    ],
    "flash_attention.pdf_chunk_44": [
      "Flash Attention",
      "Block-sparse Flash Attention",
      "Approximate Attention",
      "Sparse Attention",
      "Runtime",
      "PyTorch"
    ],
    "flash_attention.pdf_chunk_45": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "Memory Footprint",
      "Linformer",
      "A100 GPU",
      "Exact Attention",
      "Approximate Attention",
      "Sparse Attention"
    ],
    "flash_attention.pdf_chunk_46": [
      "IO-aware implementations",
      "CUDA kernel",
      "Attention",
      "Deep Network",
      "High-level language",
      "Halide",
      "FlashAttention"
    ],
    "flash_attention.pdf_chunk_47": [
      "IO-Aware Deep Learning",
      "IO-aware implementation of attention",
      "Transformer",
      "Multi-GPU IO-Aware Methods",
      "FMHA"
    ],
    "flash_attention.pdf_chunk_48": [
      "FlashAttention",
      "FMHA",
      "CUDA",
      "NIH",
      "NSF",
      "ARL",
      "ONR"
    ],
    "flash_attention.pdf_chunk_49": [
      "FlashAttention",
      "Exact Attention",
      "IO-Aware Architecture",
      "ONR",
      "Department of Defense (DoD)",
      "Stanford DAWN project"
    ],
    "flash_attention.pdf_chunk_50": [
      "FlashAttention",
      "NSF grant CCF-1763481",
      "Lambda Networks",
      "Longformer"
    ],
    "flash_attention.pdf_chunk_51": [
      "FlashAttention",
      "Attention Mechanism",
      "Transformer"
    ],
    "flash_attention.pdf_chunk_52": [
      "FlashAttention",
      "Attention Mechanism",
      "Memory Efficiency",
      "Speed"
    ],
    "flash_attention.pdf_chunk_53": [
      "FlashAttention",
      "Transformer",
      "Sparse Attention",
      "Low-Rank Attention",
      "Performer",
      "NeurIPS",
      "ICLR"
    ],
    "flash_attention.pdf_chunk_54": [
      "FlashAttention",
      "Transformer",
      "Transformer-XL",
      "Long Document Classification"
    ],
    "flash_attention.pdf_chunk_55": [
      "FlashAttention",
      "Butterfly Factorizations",
      "Kaleidoscope",
      "Pixelated Butterfly",
      "Monarch"
    ],
    "flash_attention.pdf_chunk_56": [
      "FlashAttention",
      "BERT",
      "Structured Matrices",
      "Asymmetric Clustering",
      "Memory Efficiency",
      "Deep Bidirectional Transformers"
    ],
    "flash_attention.pdf_chunk_57": [
      "FlashAttention",
      "BERT",
      "Layer-wise Optimal Brain Surgeon",
      "Transformers for Image Recognition",
      "Structured Matrices",
      "Fast Geometric Learning"
    ],
    "flash_attention.pdf_chunk_58": [
      "FlashAttention",
      "IO-Awareness",
      "Lottery Ticket Hypothesis",
      "Parameterized Complexity Theory",
      "International Conference on Learning Representations",
      "International Conference on Machine Learning"
    ],
    "flash_attention.pdf_chunk_59": [
      "FlashAttention",
      "OpenWebText Corpus",
      "State-Space Models"
    ],
    "flash_attention.pdf_chunk_60": [
      "FlashAttention",
      "Hippo",
      "Structured State Space Layers",
      "Memory Efficiency",
      "Speed"
    ],
    "flash_attention.pdf_chunk_61": [
      "FlashAttention",
      "Transformer",
      "Memory hierarchy",
      "Deep compression",
      "Structured state spaces"
    ],
    "flash_attention.pdf_chunk_62": [
      "FlashAttention",
      "Ampere GPU",
      "NVIDIA Volta GPU",
      "Graphcore IPU",
      "Data Movement Optimization"
    ],
    "flash_attention.pdf_chunk_63": [
      "FlashAttention",
      "Tensor Processing Unit (TPU)",
      "MIMIC-III",
      "Transformer"
    ],
    "flash_attention.pdf_chunk_64": [
      "FlashAttention",
      "Transformer",
      "RNN",
      "Memory Efficiency",
      "Speed"
    ],
    "flash_attention.pdf_chunk_65": [
      "Reformer",
      "Albert",
      "Runtime Neural Pruning",
      "Deep Learning Compiler",
      "Performers"
    ],
    "flash_attention.pdf_chunk_66": [
      "FlashAttention",
      "Roberta",
      "Luna",
      "MLPerf"
    ],
    "flash_attention.pdf_chunk_67": [
      "FlashAttention",
      "Nvidia Tesla V100",
      "Nvidia A100",
      "Nvidia H100",
      "Mlperf",
      "Online normalizer calculation for softmax"
    ],
    "flash_attention.pdf_chunk_68": [
      "FlashAttention",
      "Transformer",
      "PyTorch",
      "Self-attention does not need O(n^2) memory",
      "Language models are unsupervised multitask learners",
      "Annual Meeting of the Association for Computational Linguistics"
    ],
    "flash_attention.pdf_chunk_69": [
      "FlashAttention",
      "Attention Mechanism",
      "Compressive Transformers",
      "International Conference on Learning Representations (ICLR)"
    ],
    "flash_attention.pdf_chunk_70": [
      "FlashAttention",
      "Transformer",
      "Attention Mechanism",
      "Large-scale Matrix Completion",
      "Sparse Computation"
    ],
    "flash_attention.pdf_chunk_71": [
      "FlashAttention",
      "Routing Transformers",
      "Movement Pruning",
      "Megatron-LM",
      "Structured Transforms"
    ],
    "flash_attention.pdf_chunk_72": [
      "FlashAttention",
      "Transformer",
      "Long Range Arena",
      "Adaptive Attention Span"
    ],
    "flash_attention.pdf_chunk_73": [
      "Attention",
      "FlashAttention",
      "DeepNet",
      "Linformer",
      "Roo\ufb02ine Model",
      "Data Locality Optimization"
    ],
    "flash_attention.pdf_chunk_74": [
      "FlashAttention",
      "Transformer",
      "Optimal space lower bounds",
      "Lightweight and dynamic convolutions"
    ],
    "flash_attention.pdf_chunk_75": [
      "FlashAttention",
      "Nystr\u00f6mformer",
      "Tokens-to-token vit",
      "Big Bird",
      "ICLR",
      "AAAI Conference on Artificial Intelligence",
      "IEEE/CVF International Conference on Computer Vision",
      "Advances in Neural Information Processing Systems"
    ],
    "flash_attention.pdf_chunk_76": [
      "FlashAttention",
      "Big Bird",
      "Attention Free Transformer",
      "Long-Short Transformer"
    ],
    "flash_attention.pdf_chunk_77": [
      "IO-Aware Runtime Optimization",
      "I/O Complexity",
      "Memory Hierarchies",
      "Working Set Model",
      "Data Locality",
      "Roo\ufb02ine Model",
      "Scalability",
      "Structured Matrices",
      "Computational Complexity"
    ],
    "flash_attention.pdf_chunk_78": [
      "Structured Matrices",
      "Block-Sparse Attention",
      "Butterfly Matrices",
      "Toeplitz-like Matrices",
      "Low-Displacement Rank Matrices",
      "Quasi-Separable Matrices",
      "Fast Transforms"
    ],
    "flash_attention.pdf_chunk_79": [
      "Flash Attention",
      "Sparse Training",
      "Lottery Tickets Hypothesis",
      "Butterfly Matrices",
      "Hardware Lottery"
    ],
    "flash_attention.pdf_chunk_80": [
      "Flash Attention",
      "Reformer",
      "Smyrf",
      "Performer",
      "Longformer",
      "Big Bird",
      "Scatterbrain",
      "Long-short transformer",
      "Combiner",
      "Transformer",
      "Long-range Arena"
    ],
    "flash_attention.pdf_chunk_81": [
      "Transformer",
      "Transformer-XL",
      "Compressive Transformer",
      "Hi PPO",
      "S 4",
      "Lambda Networks",
      "AFT",
      "FLASH"
    ],
    "flash_attention.pdf_chunk_82": [
      "Flash Attention",
      "Lambda Networks",
      "AFT",
      "FLASH"
    ],
    "flash_attention.pdf_chunk_83": [
      "Softmax normalization",
      "Attention mechanism",
      "Memory footprint",
      "FlashAttention",
      "HBM accesses"
    ],
    "flash_attention.pdf_chunk_84": [
      "Attention Mechanism",
      "Softmax",
      "FlashAttention",
      "Memory Efficiency",
      "Gradient Checkpointing"
    ],
    "flash_attention.pdf_chunk_85": [
      "Gradient Checkpointing",
      "Backward Pass",
      "Loss Function",
      "Input Gradients",
      "Output Gradient",
      "Attention Mechanism",
      "Reverse-mode Autodiff",
      "Softmax Function",
      "Jacobian"
    ],
    "flash_attention.pdf_chunk_86": [
      "FlashAttention",
      "Softmax",
      "Jacobian",
      "Memory Efficiency",
      "Speed"
    ],
    "flash_attention.pdf_chunk_87": [
      "FlashAttention",
      "Pointwise Multiplication",
      "Memory Complexity",
      "Attention Mechanism",
      "Q, K, V",
      "d",
      "n"
    ],
    "flash_attention.pdf_chunk_88": [
      "Flash Attention",
      "Attention Output",
      "Input Sequences",
      "Softmax Scaling",
      "Masking Function",
      "Dropout",
      "Algorithm 2"
    ],
    "flash_attention.pdf_chunk_89": [
      "Flash Attention",
      "HBM",
      "on-chip SRAM",
      "softmax scaling constant",
      "masking function",
      "dropout probability",
      "Attention Mechanism"
    ],
    "flash_attention.pdf_chunk_90": [
      "FlashAttention",
      "Attention Mechanism",
      "Transformer",
      "Memory Efficiency",
      "Computational Speed",
      "Input Sequences"
    ],
    "flash_attention.pdf_chunk_91": [
      "FlashAttention",
      "Standard Attention Backward Pass",
      "Q-K-V",
      "P",
      "HBM",
      "Dropout Mask",
      "Pseudo-Random Number Generator"
    ],
    "flash_attention.pdf_chunk_92": [
      "FlashAttention",
      "Dropout Mask",
      "Memory Usage",
      "Softmax Gradient",
      "Eq. (4)",
      "Dot Product"
    ],
    "flash_attention.pdf_chunk_93": [
      "Flash Attention",
      "Flash Attention Backward Pass",
      "Q-K-V-O",
      "\u2113-m",
      "softmax scaling constant (\u03c4)",
      "dropout probability (p_drop)",
      "pseudo-random number generator state (R)",
      "on-chip SRAM"
    ],
    "flash_attention.pdf_chunk_94": [
      "FlashAttention",
      "HBM",
      "SRAM",
      "dropout mask",
      "S",
      "P"
    ],
    "flash_attention.pdf_chunk_95": [
      "FlashAttention",
      "FLOPs",
      "Attention Mechanism",
      "IO-complexity",
      "Backward Pass",
      "Forward Pass",
      "N",
      "d",
      "M"
    ],
    "flash_attention.pdf_chunk_96": [
      "Flash Attention",
      "Standard Attention",
      "IO-complexity",
      "SRAM",
      "HBM accesses",
      "Theorem 2",
      "Theorem 5",
      "N",
      "d",
      "M"
    ],
    "flash_attention.pdf_chunk_97": [
      "Flash Attention",
      "Rabe and Staats Algorithm",
      "Tiling",
      "Softmax Scaling",
      "Memory Footprint",
      "Memory Accesses"
    ],
    "flash_attention.pdf_chunk_98": [
      "Flash Attention",
      "Standard Attention",
      "Rabe and Staats",
      "Memory Requirement",
      "Runtime"
    ],
    "flash_attention.pdf_chunk_99": [
      "FlashAttention",
      "Gradient Checkpointing",
      "Rabe and Staats",
      "Algorithm 1",
      "FLOPs"
    ],
    "flash_attention.pdf_chunk_100": [
      "FlashAttention",
      "FLOPs",
      "Q",
      "K",
      "V",
      "softmax",
      "HBM"
    ],
    "flash_attention.pdf_chunk_101": [
      "FlashAttention",
      "Algorithm 1",
      "rowmax",
      "rowsum",
      "HBM",
      "S",
      "V"
    ],
    "flash_attention.pdf_chunk_102": [
      "FlashAttention",
      "Row-max",
      "Slice",
      "Softmax",
      "Attention Mechanism"
    ],
    "flash_attention.pdf_chunk_103": [
      "FlashAttention",
      "IO complexity",
      "Attention",
      "HBM",
      "S",
      "P",
      "Q",
      "K",
      "V",
      "Algorithm 0"
    ],
    "flash_attention.pdf_chunk_104": [
      "FlashAttention",
      "IO complexity",
      "HBM",
      "softmax",
      "global memory",
      "on-chip memory",
      "N",
      "d",
      "M",
      "T_c",
      "B_c",
      "B_r"
    ],
    "flash_attention.pdf_chunk_105": [
      "Block Sizes",
      "On-chip Memory",
      "FlashAttention"
    ],
    "flash_attention.pdf_chunk_106": [
      "FlashAttention",
      "IO complexity",
      "Attention Mechanism",
      "Exact Attention",
      "HBM (High Bandwidth Memory)",
      "HBM accesses",
      "Proposition 3",
      "Theorem 5"
    ],
    "flash_attention.pdf_chunk_107": [
      "Flash Attention",
      "IO Complexity",
      "HBM Accesses",
      "Standard Attention",
      "Theorem 2",
      "N",
      "d",
      "Tc",
      "Bc",
      "Br"
    ],
    "flash_attention.pdf_chunk_108": [
      "FlashAttention",
      "HBM accesses",
      "Theorem 2",
      "Block sizes",
      "N",
      "M",
      "d",
      "Tc"
    ],
    "flash_attention.pdf_chunk_109": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "softmax scaling constant",
      "masking function",
      "dropout probability",
      "Matrices Q, K, V",
      "block sizes"
    ],
    "flash_attention.pdf_chunk_110": [
      "FlashAttention",
      "Block-sparse Flash Attention",
      "IO-complexity",
      "Attention Mechanism"
    ],
    "flash_attention.pdf_chunk_111": [
      "Flash Attention",
      "IO-complexity",
      "block-sparsity mask",
      "Multi-GPU Attention",
      "Large Language Models"
    ],
    "flash_attention.pdf_chunk_112": [
      "FlashAttention",
      "MLP layers",
      "Sparse weight matrices",
      "IO-awareness",
      "Attention matrix",
      "Kernel machine learning",
      "Low-rank matrix"
    ],
    "flash_attention.pdf_chunk_113": [
      "Flash Attention",
      "BERT-large",
      "LAMB",
      "FLOPs",
      "Ke Ops",
      "Kernel Machine Learning",
      "Attention Matrix",
      "Kernel Matrix"
    ],
    "flash_attention.pdf_chunk_114": [
      "LAMB optimizer",
      "validation accuracy",
      "GPT-2",
      "MLPerf 1.1",
      "Megatron-LM",
      "FP 16 precision",
      "wall-clock run-time",
      "Apex AMP",
      "A100-80 GB GPUs"
    ],
    "flash_attention.pdf_chunk_115": [
      "GPT-2",
      "Gradient Accumulation",
      "Adam W",
      "Openwebtext",
      "Mixed-Precision Training",
      "Huggingface Transformers",
      "Nvidia\u2019s Megatron-LM"
    ],
    "flash_attention.pdf_chunk_116": [
      "GPT-2 small",
      "GPT-2 medium",
      "Flash Attention",
      "MIMIC-III",
      "ECt HR",
      "validation perplexity"
    ],
    "flash_attention.pdf_chunk_117": [
      "GPT-2-small",
      "GPT-2-medium",
      "Flash Attention",
      "Hugging Face",
      "Long-range arena",
      "Validation perplexity"
    ],
    "flash_attention.pdf_chunk_118": [
      "Mixed-Precision Training",
      "Performer",
      "Local Attention",
      "Path-64",
      "Path-X",
      "Path-256",
      "Validation Accuracy",
      "Wallclock-Time Speedup"
    ],
    "flash_attention.pdf_chunk_119": [
      "FlashAttention",
      "Path-X",
      "Apex FMHA",
      "val accuracy",
      "MLPerf",
      "BERT"
    ],
    "flash_attention.pdf_chunk_120": [
      "Flash Attention",
      "FMHA",
      "BERT-large",
      "A100-SXM",
      "Runtime"
    ],
    "flash_attention.pdf_chunk_121": [
      "Tiling",
      "Recomputation",
      "FlashAttention",
      "FMHA",
      "Turing GPUs",
      "Ampere GPUs",
      "Performance",
      "Memory Footprint"
    ],
    "flash_attention.pdf_chunk_122": [
      "FlashAttention",
      "Speedup",
      "FMHA",
      "A100 GPU",
      "PyTorch"
    ],
    "flash_attention.pdf_chunk_123": [
      "A100",
      "RTX 3090",
      "T4",
      "FlashAttention",
      "Speedup",
      "Attention Mechanism"
    ],
    "flash_attention.pdf_chunk_124": [
      "FlashAttention",
      "RTX 3090",
      "A100",
      "T4",
      "IO complexity",
      "speedup"
    ],
    "flash_attention.pdf_chunk_125": [
      "FlashAttention",
      "PyTorch",
      "Hugging Face",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer Attention",
      "Smyrf",
      "Long Short Former (LSFormer)",
      "Block-Sparse Attention",
      "Longformer",
      "Big Bird Attention",
      "A100",
      "Speedup"
    ],
    "flash_attention.pdf_chunk_126": [
      "FlashAttention",
      "A100 GPU",
      "Runtime",
      "Memory Footprint",
      "Dropout",
      "Masking",
      "Sequence Length",
      "Batch Size",
      "Attention Heads",
      "Attention Dimension"
    ],
    "flash_attention.pdf_chunk_127": [
      "FlashAttention",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "Forward pass runtime"
    ],
    "flash_attention.pdf_chunk_128": [
      "Flash Attention",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Timing Results",
      "Dropout",
      "Masking"
    ],
    "flash_attention.pdf_chunk_129": [
      "FlashAttention",
      "Megatron",
      "Block-Sparse",
      "Longformer",
      "Big Bird",
      "Memory Usage",
      "FP 16",
      "FP 32"
    ],
    "flash_attention.pdf_chunk_130": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird"
    ],
    "flash_attention.pdf_chunk_131": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Runtime"
    ],
    "flash_attention.pdf_chunk_132": [
      "Flash Attention",
      "Block Sparse Flash Attention",
      "Longformer",
      "Big Bird",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Forward pass runtime"
    ],
    "flash_attention.pdf_chunk_133": [
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Backward pass runtime"
    ],
    "flash_attention.pdf_chunk_134": [
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Flash Attention",
      "Block-Sparse Flash Attention"
    ],
    "flash_attention.pdf_chunk_135": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird"
    ],
    "flash_attention.pdf_chunk_136": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Forward pass runtime"
    ],
    "flash_attention.pdf_chunk_137": [
      "Flash Attention",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Backward pass runtime"
    ],
    "flash_attention.pdf_chunk_138": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer"
    ],
    "flash_attention.pdf_chunk_139": [
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Flash Attention",
      "Block-Sparse Flash Attention"
    ],
    "flash_attention.pdf_chunk_140": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird"
    ],
    "flash_attention.pdf_chunk_141": [
      "Flash Attention",
      "Block-Sparse Flash Attention",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Backward pass runtime"
    ],
    "flash_attention.pdf_chunk_142": [
      "Flash Attention",
      "Block Sparse Flash Attention",
      "Longformer",
      "Big Bird",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Runtime"
    ],
    "flash_attention.pdf_chunk_143": [
      "Flash Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Block-Sparse Flash Attention",
      "Memory usage",
      "Py Torch Attention",
      "Megatron",
      "Reformer",
      "Local Attention"
    ],
    "flash_attention.pdf_chunk_144": [
      "Reformer",
      "Local Attention",
      "Linformer",
      "Smyrf",
      "LSformer",
      "Block Sparse",
      "Longformer",
      "Big Bird",
      "Flash Attention",
      "Block-Sparse Flash Attention"
    ],
    "bert.pdf_chunk_0": [
      "BERT",
      "Transformer",
      "Pre-training",
      "Fine-tuning",
      "Question Answering",
      "Language Inference"
    ],
    "bert.pdf_chunk_1": [
      "BERT",
      "GLUE score",
      "Multi NLI accuracy",
      "SQuAD v1.1",
      "SQuAD v1.1 Test F1",
      "SQuAD v2.0",
      "SQuAD v2.0 Test F1",
      "Language model pre-training",
      "Natural Language Inference",
      "Paraphrasing"
    ],
    "bert.pdf_chunk_2": [
      "Natural Language Inference",
      "Paraphrasing",
      "Named Entity Recognition",
      "Question Answering",
      "ELMo",
      "Generative Pre-trained Transformer (Open AI GPT)",
      "BERT"
    ],
    "bert.pdf_chunk_3": [
      "BERT",
      "Open AI GPT",
      "Fine-tuning",
      "Left-to-right architecture",
      "Unidirectional language models",
      "Self-attention layers",
      "Transformer"
    ],
    "bert.pdf_chunk_4": [
      "BERT",
      "Transformer",
      "Masked Language Model (MLM)",
      "Cloze task",
      "Question Answering"
    ],
    "bert.pdf_chunk_5": [
      "BERT",
      "Masked Language Model (MLM)",
      "Next Sentence Prediction",
      "Unidirectional Language Models",
      "Peters et al. (2018)"
    ],
    "bert.pdf_chunk_6": [
      "BERT",
      "Pre-training",
      "Bidirectional Transformers",
      "NLP tasks",
      "General Language Representations",
      "Word2Vec",
      "GloVe"
    ],
    "bert.pdf_chunk_7": [
      "Pre-trained word embeddings",
      "Left-to-right language modeling",
      "Ranking candidate next sentences",
      "BERT",
      "Word2Vec",
      "GloVe",
      "Sentence embeddings",
      "Paragraph embeddings"
    ],
    "bert.pdf_chunk_8": [
      "BERT",
      "ELMo",
      "Denoising Autoencoder",
      "Next Sentence Prediction",
      "NLP Benchmarks",
      "State of the Art",
      "Contextual Word Embeddings",
      "Question Answering",
      "Sentiment Analysis",
      "Named Entity Recognition"
    ],
    "bert.pdf_chunk_9": [
      "BERT",
      "ELMo",
      "Cloze Task",
      "LSTM",
      "SQuAD",
      "Sentiment Analysis",
      "Named Entity Recognition",
      "Sentence/Document Encoders"
    ],
    "bert.pdf_chunk_10": [
      "BERT",
      "Open AI GPT",
      "GLUE benchmark",
      "Pre-training",
      "Fine-tuning"
    ],
    "bert.pdf_chunk_11": [
      "BERT",
      "Transformer",
      "Masked Language Model (MLM)",
      "Next Sentence Prediction (NSP)",
      "SQuAD",
      "MNLI"
    ],
    "bert.pdf_chunk_12": [
      "BERT",
      "Transfer Learning",
      "ImageNet",
      "Natural Language Inference",
      "Machine Translation",
      "Deep Bidirectional Transformers"
    ],
    "bert.pdf_chunk_13": [
      "BERT",
      "Transformer",
      "Pre-training",
      "Fine-tuning",
      "Downstream tasks"
    ],
    "bert.pdf_chunk_14": [
      "BERTBASE",
      "BERTLARGE",
      "Open AI GPT",
      "Transformer",
      "Bidirectional Self-Attention",
      "Constrained Self-Attention"
    ],
    "bert.pdf_chunk_15": [
      "BERT",
      "GPT",
      "Bidirectional Self-Attention",
      "Constrained Self-Attention",
      "Transformer"
    ],
    "bert.pdf_chunk_16": [
      "BERT",
      "Word Piece embeddings",
      "Deep Bidirectional Transformers",
      "[CLS]",
      "[SEP]",
      "30,000 token vocabulary"
    ],
    "bert.pdf_chunk_17": [
      "BERT",
      "Masked LM",
      "Transformer",
      "[SEP]",
      "Input Embedding",
      "Hidden Vector"
    ],
    "bert.pdf_chunk_18": [
      "BERT",
      "Masked LM",
      "Transformer encoder",
      "Transformer decoder",
      "Bidirectional conditioning",
      "Conditional language models"
    ],
    "bert.pdf_chunk_19": [
      "BERT",
      "Masked Language Model (MLM)",
      "Cloze task",
      "Denoising auto-encoders",
      "Word Piece tokens"
    ],
    "bert.pdf_chunk_20": [
      "BERT",
      "Masked Language Modeling",
      "Next Sentence Prediction (NSP)",
      "Question Answering (QA)",
      "Natural Language Inference (NLI)",
      "Cross Entropy Loss"
    ],
    "bert.pdf_chunk_21": [
      "Next Sentence Prediction (NSP)",
      "Binarized Next Sentence Prediction",
      "BERT",
      "Accuracy",
      "Question Answering (QA)",
      "Natural Language Inference (NLI)"
    ],
    "bert.pdf_chunk_22": [
      "BERT",
      "Books Corpus",
      "English Wikipedia",
      "Next Sentence Prediction (NSP)",
      "Representation Learning"
    ],
    "bert.pdf_chunk_23": [
      "BERT",
      "Books Corpus",
      "English Wikipedia",
      "Billion Word Benchmark",
      "Fine-tuning",
      "Self-attention mechanism",
      "Bidirectional cross attention"
    ],
    "bert.pdf_chunk_24": [
      "BERT",
      "Self-Attention Mechanism",
      "Bidirectional Cross Attention",
      "Paraphrasing",
      "Entailment",
      "Question Answering",
      "Text Classification",
      "Sequence Tagging",
      "[CLS] Representation"
    ],
    "bert.pdf_chunk_25": [
      "BERT",
      "Fine-tuning",
      "GLUE",
      "CLS representation",
      "Deep Bidirectional Transformers"
    ],
    "bert.pdf_chunk_26": [
      "BERT",
      "GLUE",
      "F1 score",
      "Transformer",
      "Fine-tuning"
    ],
    "bert.pdf_chunk_27": [
      "BERT",
      "Open AI GPT",
      "Bi LSTM+ELMo+Attn",
      "GLUE",
      "F1 Score",
      "Spearman Correlation",
      "Accuracy",
      "Transformer",
      "MNLI",
      "QQP",
      "QNLI",
      "SST-2",
      "CoLA",
      "STS-B",
      "MRPC",
      "RTE"
    ],
    "bert.pdf_chunk_28": [
      "BERTBASE",
      "BERTLARGE",
      "Open AI GPT",
      "GLUE",
      "accuracy",
      "fine-tuning",
      "Transformer"
    ],
    "bert.pdf_chunk_29": [
      "BERT",
      "BERTBASE",
      "BERTLARGE",
      "Open AI GPT",
      "GLUE",
      "MNLI",
      "SQuAD v1.1",
      "Accuracy",
      "Transformer"
    ],
    "bert.pdf_chunk_30": [
      "BERT",
      "GLUE",
      "Question Answering",
      "Softmax",
      "Deep Bidirectional Transformers"
    ],
    "bert.pdf_chunk_31": [
      "BERT",
      "SQuAD",
      "Trivia QA",
      "F1 Score",
      "Fine-tuning"
    ],
    "bert.pdf_chunk_32": [
      "BERT",
      "Trivia QA",
      "SQuAD",
      "F1 score",
      "QANet"
    ],
    "bert.pdf_chunk_33": [
      "BERT",
      "BERTBASE",
      "BERTLARGE",
      "nlnet",
      "QANet",
      "EM",
      "F1",
      "SQuAD 1.1",
      "SQuAD 2.0"
    ],
    "bert.pdf_chunk_34": [
      "BERT",
      "SQuAD 2.0",
      "F1 Score",
      "BERTLARGE",
      "Trivia QA"
    ],
    "bert.pdf_chunk_35": [
      "BERT",
      "Trivia QA",
      "F1 Score",
      "ESIM",
      "OpenAI GPT",
      "BERTBASE",
      "BERTLARGE",
      "Accuracy"
    ],
    "bert.pdf_chunk_36": [
      "BERT",
      "SWAG",
      "F1 Score",
      "Fine-tuning",
      "Transformer"
    ],
    "bert.pdf_chunk_37": [
      "BERT",
      "ESIM+ELMo",
      "Open AI GPT",
      "Accuracy Improvement",
      "Fine-tuning",
      "Deep Bidirectional Transformers"
    ],
    "bert.pdf_chunk_38": [
      "BERT",
      "Masked Language Model (MLM)",
      "Next Sentence Prediction (NSP)",
      "BERTBASE",
      "MNLI",
      "QNLI",
      "MRPC",
      "SST-2",
      "Accuracy",
      "F1 Score"
    ],
    "bert.pdf_chunk_39": [
      "BERT",
      "Open AI GPT",
      "Masked Language Modeling (MLM)",
      "Next Sentence Prediction (NSP)",
      "QNLI",
      "MNLI",
      "SQuAD 1.1",
      "LTR & No NSP"
    ],
    "bert.pdf_chunk_40": [
      "BERT",
      "BiLSTM",
      "SQuAD",
      "MRPC",
      "GLUE",
      "LTR",
      "NSP",
      "MLM"
    ],
    "bert.pdf_chunk_41": [
      "BERT",
      "ELMo",
      "GLUE",
      "Dev Set accuracy",
      "Fine-tuning",
      "Bidirectional Model",
      "MRPC"
    ],
    "bert.pdf_chunk_42": [
      "BERT BASE",
      "BERT LARGE",
      "Transformer",
      "MRPC",
      "accuracy",
      "Transformer (Vaswani et al. 2017)",
      "Transformer (Al-Rfou et al. 2018)"
    ],
    "bert.pdf_chunk_43": [
      "BERT",
      "Pre-training",
      "LM perplexity",
      "Model size",
      "Machine Translation",
      "Language Modeling",
      "Peters et al. (2018)"
    ],
    "bert.pdf_chunk_44": [
      "BERT",
      "Fine-tuning",
      "Feature-based Approach",
      "bi-LM",
      "Hidden Dimension Size"
    ],
    "bert.pdf_chunk_45": [
      "BERT",
      "CoNLL-2003",
      "Feature-based approach",
      "Transformer encoder",
      "Accuracy",
      "Tagging task"
    ],
    "bert.pdf_chunk_46": [
      "BERT",
      "Fine-tuning",
      "MNLI",
      "MRPC",
      "SST-2",
      "F1 Score",
      "Transformer"
    ],
    "bert.pdf_chunk_47": [
      "BERT",
      "CoNLL-2003",
      "F1 Score",
      "Fine-tuning",
      "Feature-based approach",
      "Bi LSTM"
    ],
    "bert.pdf_chunk_48": [
      "BERT",
      "Deep Bidirectional Architecture",
      "Fine-tuning",
      "Feature-based approaches",
      "F1 Score",
      "BERT LARGE",
      "Transfer Learning",
      "Unsupervised Pre-training",
      "NLP tasks"
    ],
    "bert.pdf_chunk_49": [
      "BERT",
      "Contextual string embeddings",
      "Character-level language modeling",
      "PASCAL recognizing textual entailment challenge",
      "Domain adaptation"
    ],
    "bert.pdf_chunk_50": [
      "BERT",
      "Transformer",
      "Semeval-2017",
      "Annotated Corpus",
      "Class-based n-gram models"
    ],
    "bert.pdf_chunk_51": [
      "BERT",
      "Pre-training",
      "Transformer",
      "Quora question pairs",
      "One billion word benchmark"
    ],
    "bert.pdf_chunk_52": [
      "BERT",
      "Deep Neural Networks",
      "Multitask Learning",
      "Universal Sentence Representations",
      "Natural Language Inference Data",
      "ImageNet",
      "Corpus of Sentential Paraphrases"
    ],
    "bert.pdf_chunk_53": [
      "BERT",
      "Transformer",
      "ImageNet",
      "Paraphrasing",
      "MaskGAN",
      "Gaussian Error Linear Units (GELUs)",
      "Distributed Representations",
      "Universal Language Model"
    ],
    "bert.pdf_chunk_54": [
      "BERT",
      "Transformer",
      "Universal Language Model Fine-tuning",
      "Reinforced Mnemonic Reader",
      "Discourse-based Objectives"
    ],
    "bert.pdf_chunk_55": [
      "BERT",
      "TriviaQA",
      "Skip-thought vectors",
      "Distributed representations",
      "Winograd Schema Challenge",
      "Sentence representations framework"
    ],
    "bert.pdf_chunk_56": [
      "BERT",
      "Transformer",
      "Contextualized word vectors",
      "Bidirectional LSTM",
      "Distributed representations",
      "Hierarchical distributed language model",
      "Decomposable attention"
    ],
    "bert.pdf_chunk_57": [
      "BERT",
      "Transformer",
      "Bidirectional Language Model",
      "GloVe",
      "Decomposable Attention Model",
      "Semi-supervised Sequence Tagging",
      "Contextualized Word Representations"
    ],
    "bert.pdf_chunk_58": [
      "BERT",
      "Transformer",
      "SQuAD",
      "Unsupervised Learning",
      "Bidirectional Attention Flow",
      "Semantic Compositionality"
    ],
    "bert.pdf_chunk_59": [
      "BERT",
      "Transformer",
      "Pre-training",
      "Sentiment Treebank",
      "Readability",
      "Named Entity Recognition"
    ],
    "bert.pdf_chunk_60": [
      "BERT",
      "Transformer",
      "Attention Mechanism",
      "GLUE"
    ],
    "bert.pdf_chunk_61": [
      "BERT",
      "Deep Bidirectional Transformers",
      "Multi-granularity hierarchical attention fusion networks",
      "A broad-coverage challenge corpus for sentence understanding through inference",
      "Google's Neural Machine Translation System"
    ],
    "bert.pdf_chunk_62": [
      "BERT",
      "QANet",
      "SWAG",
      "Neural Machine Translation",
      "Deep Bidirectional Transformers"
    ],
    "bert.pdf_chunk_63": [
      "BERT",
      "Transformer",
      "Pre-training",
      "Ablation Study",
      "Training Steps",
      "Masking Procedures"
    ],
    "bert.pdf_chunk_64": [
      "BERT",
      "Masked LM",
      "Masking Procedure",
      "Pre-training Tasks"
    ],
    "bert.pdf_chunk_65": [
      "BERT",
      "Masked Language Modeling",
      "Transformer",
      "Language Understanding Capability"
    ],
    "bert.pdf_chunk_66": [
      "BERT",
      "Open AI GPT",
      "ELMo",
      "Bidirectional Transformer",
      "Left-to-Right Transformer",
      "Fine-tuning",
      "Feature-based approach",
      "MLM (Masked Language Model)"
    ],
    "bert.pdf_chunk_67": [
      "BERT",
      "Masked Language Model (MLM)",
      "Next Sentence Prediction",
      "Bidirectional Transformers",
      "Corpus"
    ],
    "bert.pdf_chunk_68": [
      "BERT",
      "Next Sentence Prediction",
      "3.3 billion word corpus",
      "Batch Size",
      "Deep Bidirectional Transformers",
      "Word Piece Tokenization",
      "Learning Rate",
      "Dropout",
      "GELU"
    ],
    "bert.pdf_chunk_69": [
      "BERT",
      "BERT BASE",
      "BERT LARGE",
      "Dropout",
      "GELU Activation",
      "Learning Rate Warmup",
      "Linear Decay",
      "Cloud TPU",
      "Masked LM Likelihood",
      "Next Sentence Prediction Likelihood"
    ],
    "bert.pdf_chunk_70": [
      "BERT",
      "Fine-tuning",
      "Deep Bidirectional Transformers",
      "Dropout Probability",
      "Positional Embeddings"
    ],
    "bert.pdf_chunk_71": [
      "BERT",
      "ELMo",
      "Open AI GPT",
      "100k+ labeled training examples",
      "Fine-tuning",
      "Transformer"
    ],
    "bert.pdf_chunk_72": [
      "BERT",
      "Open AI GPT",
      "Books Corpus",
      "Wikipedia",
      "Pre-training",
      "Transformer",
      "[SEP]",
      "[CLS]"
    ],
    "bert.pdf_chunk_73": [
      "BERT",
      "GPT",
      "Fine-tuning",
      "Bidirectional Transformers",
      "Learning Rate",
      "Development Set"
    ],
    "bert.pdf_chunk_74": [
      "BERT",
      "GLUE",
      "MNLI",
      "Contextual Representation",
      "[CLS]",
      "[SEP]"
    ],
    "bert.pdf_chunk_75": [
      "MNLI",
      "QQP",
      "QNLI",
      "Stanford Question Answering Dataset"
    ],
    "bert.pdf_chunk_76": [
      "BERT",
      "SST-2",
      "CoLA",
      "Fine-tuning"
    ],
    "bert.pdf_chunk_77": [
      "STS-B",
      "MRPC",
      "RTE",
      "WNLI"
    ],
    "bert.pdf_chunk_78": [
      "WNLI",
      "GLUE",
      "Open AI GPT",
      "RTE",
      "MNLI",
      "65.1 baseline accuracy"
    ],
    "bert.pdf_chunk_79": [
      "BERT",
      "MNLI",
      "accuracy",
      "MLM",
      "LTR"
    ],
    "bert.pdf_chunk_80": [
      "BERT",
      "Masked Language Model (MLM)",
      "MNLI",
      "Dev Accuracy",
      "Fine-tuning",
      "Feature-based approaches",
      "NER"
    ],
    "bert.pdf_chunk_81": [
      "BERT",
      "Fine-tuning",
      "Feature-based approach",
      "MNLI",
      "NER",
      "Dev Set Results",
      "Masking Strategies"
    ],
    "bert.pdf_chunk_82": [
      "BERT",
      "Masked Language Modeling (MLM)",
      "Feature-based approach",
      "Dev set results",
      "Named Entity Recognition (NER)",
      "MASK strategy",
      "RND strategy"
    ],
    "attention_is_all_you_need.pdf_chunk_0": [
      "Transformer",
      "Attention Mechanism",
      "Sequence Transduction Models"
    ],
    "attention_is_all_you_need.pdf_chunk_1": [
      "Transformer",
      "WMT 2014 English-to-German",
      "WMT 2014 English-to-French",
      "BLEU",
      "Attention Mechanism",
      "Network Architecture"
    ],
    "attention_is_all_you_need.pdf_chunk_2": [
      "Transformer",
      "Self-Attention",
      "Scaled Dot-Product Attention",
      "Multi-Head Attention",
      "Position Representation",
      "Tensor2Tensor"
    ],
    "attention_is_all_you_need.pdf_chunk_3": [
      "Attention Mechanism",
      "Transformer",
      "Tensor2Tensor"
    ],
    "attention_is_all_you_need.pdf_chunk_4": [
      "Recurrent Neural Networks",
      "Long Short-Term Memory (LSTM)",
      "Gated Recurrent Neural Networks",
      "Sequence Modeling",
      "Transduction",
      "Encoder-Decoder Architecture",
      "Memory Constraints"
    ],
    "attention_is_all_you_need.pdf_chunk_5": [
      "Transformer",
      "Attention Mechanism",
      "Factorization Tricks",
      "Conditional Computation",
      "Recurrent Network"
    ],
    "attention_is_all_you_need.pdf_chunk_6": [
      "Transformer",
      "Extended Neural GPU",
      "Byte Net",
      "Conv S2S",
      "Translation Quality",
      "Convolutional Neural Networks"
    ],
    "attention_is_all_you_need.pdf_chunk_7": [
      "Transformer",
      "Self-attention",
      "Multi-Head Attention",
      "End-to-end memory networks",
      "Reading comprehension",
      "Abstractive summarization",
      "Textual entailment",
      "Learning task-independent sentence representations"
    ],
    "attention_is_all_you_need.pdf_chunk_8": [
      "Transformer",
      "Self-Attention",
      "Encoder-Decoder",
      "Language Modeling",
      "Question Answering"
    ],
    "attention_is_all_you_need.pdf_chunk_9": [
      "Transformer",
      "Auto-regressive generation",
      "Continuous representations",
      "Output sequence"
    ],
    "attention_is_all_you_need.pdf_chunk_10": [
      "Transformer",
      "Self-Attention",
      "Multi-Head Self-Attention",
      "Encoder-Decoder Architecture",
      "Feed-Forward Network",
      "Residual Connection",
      "Layer Normalization"
    ],
    "attention_is_all_you_need.pdf_chunk_11": [
      "Transformer",
      "Multi-Head Attention",
      "Encoder-Decoder",
      "Residual Connections",
      "Layer Normalization",
      "Masking"
    ],
    "attention_is_all_you_need.pdf_chunk_12": [
      "Attention Function",
      "Transformer"
    ],
    "attention_is_all_you_need.pdf_chunk_13": [
      "Scaled Dot-Product Attention",
      "Multi-Head Attention",
      "Transformer"
    ],
    "attention_is_all_you_need.pdf_chunk_14": [
      "Attention Mechanism",
      "Dot-product Attention",
      "Additive Attention",
      "dk"
    ],
    "attention_is_all_you_need.pdf_chunk_15": [
      "Multi-Head Attention",
      "Dot Product",
      "Softmax Function",
      "Transformer"
    ],
    "attention_is_all_you_need.pdf_chunk_16": [
      "Transformer",
      "Multi-head attention",
      "Encoder-decoder attention",
      "WQ, WK, WV, WO"
    ],
    "attention_is_all_you_need.pdf_chunk_17": [
      "Transformer",
      "Multi-head Attention",
      "Encoder-Decoder",
      "Self-Attention",
      "Sequence-to-Sequence Models"
    ],
    "attention_is_all_you_need.pdf_chunk_18": [
      "Transformer",
      "Self-Attention",
      "Encoder-Decoder",
      "Scaled Dot-Product Attention",
      "Position-wise Feed-Forward Networks",
      "ReLU"
    ],
    "attention_is_all_you_need.pdf_chunk_19": [
      "Transformer",
      "Attention Mechanism",
      "Feed Forward Network (FFN)",
      "Next-token probabilities",
      "Learned Embeddings",
      "Dimensionality"
    ],
    "attention_is_all_you_need.pdf_chunk_20": [
      "Self-Attention",
      "Positional Encoding",
      "Restricted Self-Attention",
      "Transformer",
      "Complexity per Layer",
      "Maximum Path Length"
    ],
    "attention_is_all_you_need.pdf_chunk_21": [
      "Positional Encodings",
      "Sine and Cosine Functions",
      "Transformer",
      "Encoder-Decoder Stacks"
    ],
    "attention_is_all_you_need.pdf_chunk_22": [
      "Self-Attention",
      "Transformer",
      "Positional Embeddings",
      "Computational Complexity",
      "Parallelization"
    ],
    "attention_is_all_you_need.pdf_chunk_23": [
      "Long-range dependencies",
      "Computational complexity",
      "Path length",
      "Self-attention"
    ],
    "attention_is_all_you_need.pdf_chunk_24": [
      "Self-Attention Layer",
      "Recurrent Layer"
    ],
    "attention_is_all_you_need.pdf_chunk_25": [
      "Self-Attention",
      "Convolutional Layer",
      "Dilated Convolutions",
      "Transformer",
      "Maximum Path Length",
      "Word-Piece",
      "Byte-Pair"
    ],
    "attention_is_all_you_need.pdf_chunk_26": [
      "Self-Attention",
      "Separable Convolutions",
      "Transformer",
      "Point-wise Feed-Forward Layer",
      "Complexity"
    ],
    "attention_is_all_you_need.pdf_chunk_27": [
      "WMT 2014 English-German",
      "WMT 2014 English-French",
      "Byte-Pair Encoding",
      "Transformer",
      "NVIDIA P 100 GPUs"
    ],
    "attention_is_all_you_need.pdf_chunk_28": [
      "Transformer",
      "Adam optimizer",
      "Training Steps",
      "Learning Rate Scheduling",
      "Regularization",
      "NVIDIA P100 GPUs"
    ],
    "attention_is_all_you_need.pdf_chunk_29": [
      "Transformer",
      "BLEU",
      "newstest 2014",
      "Byte Net",
      "Deep-Att + Pos Unk",
      "GNMT + RL",
      "Conv S2S",
      "MoE"
    ],
    "attention_is_all_you_need.pdf_chunk_30": [
      "Transformer",
      "Label Smoothing",
      "WMT 2014 English-to-German",
      "BLEU",
      "Encoder-Decoder",
      "Transformer (big)"
    ],
    "attention_is_all_you_need.pdf_chunk_31": [
      "Transformer",
      "WMT 2014 English-to-French",
      "BLEU score",
      "beam search",
      "big model",
      "dropout rate",
      "length penalty"
    ],
    "attention_is_all_you_need.pdf_chunk_32": [
      "Transformer",
      "Translation Quality",
      "Training Costs",
      "K80",
      "K40",
      "M40",
      "P100",
      "English-to-German Translation Dataset"
    ],
    "attention_is_all_you_need.pdf_chunk_33": [
      "Transformer",
      "newstest 2013",
      "PPL",
      "BLEU",
      "Transformer architecture variations"
    ],
    "attention_is_all_you_need.pdf_chunk_34": [
      "Attention Mechanism",
      "Beam Search",
      "newstest 2013",
      "BLEU",
      "Transformer",
      "Single-Head Attention",
      "Multi-Head Attention",
      "Dropout"
    ],
    "attention_is_all_you_need.pdf_chunk_35": [
      "Transformer",
      "Dropout",
      "Wall Street Journal (WSJ)",
      "Penn Treebank",
      "Berkley Parser corpora",
      "state-of-the-art results",
      "4-layer transformer"
    ],
    "attention_is_all_you_need.pdf_chunk_36": [
      "Penn Treebank",
      "Berkley Parser corpora",
      "Vocabulary Size",
      "Dropout",
      "Attention",
      "Residual Connection",
      "English-to-German base translation model"
    ],
    "attention_is_all_you_need.pdf_chunk_37": [
      "Transformer",
      "WSJ",
      "F1 Score",
      "Discriminative",
      "Semi-supervised",
      "Multi-task",
      "Generative"
    ],
    "attention_is_all_you_need.pdf_chunk_38": [
      "Transformer",
      "Recurrent Neural Network Grammar",
      "Berkeley-Parser",
      "WSJ",
      "WMT 2014 English-to-German",
      "WMT 2014 English-to-French",
      "multi-headed self-attention"
    ],
    "attention_is_all_you_need.pdf_chunk_39": [
      "Transformer",
      "WMT 2014 English-to-German",
      "WMT 2014 English-to-French",
      "Attention Mechanism",
      "Efficient Handling of Large Inputs",
      "Less Sequential Generation"
    ],
    "attention_is_all_you_need.pdf_chunk_40": [
      "Transformer",
      "Attention Mechanism",
      "Self-Attention",
      "BLEU Score"
    ],
    "attention_is_all_you_need.pdf_chunk_41": [
      "Transformer",
      "Self-Attention",
      "Encoder-Decoder",
      "BERT",
      "BLEU"
    ],
    "attention_is_all_you_need.pdf_chunk_42": [
      "Transformer",
      "Self-Attention",
      "Sequence to Sequence Learning"
    ],
    "attention_is_all_you_need.pdf_chunk_43": [
      "Transformer",
      "Self-Attention",
      "Neural Machine Translation",
      "Language Modeling"
    ],
    "attention_is_all_you_need.pdf_chunk_44": [
      "Transformer",
      "Self-Attention",
      "Multi-Head Attention",
      "BLEU",
      "Encoder-Decoder"
    ],
    "attention_is_all_you_need.pdf_chunk_45": [
      "Transformer",
      "Penn Treebank",
      "Self-training",
      "Decomposable Attention Model",
      "Deep Reinforced Model",
      "Tree Annotation"
    ],
    "attention_is_all_you_need.pdf_chunk_46": [
      "Transformer"
    ],
    "attention_is_all_you_need.pdf_chunk_47": [
      "Transformer",
      "Self-Attention",
      "Attention Mechanism",
      "Machine Translation Datasets"
    ],
    "attention_is_all_you_need.pdf_chunk_48": [
      "Transformer",
      "Self-Attention",
      "WMT",
      "BLEU"
    ],
    "attention_is_all_you_need.pdf_chunk_49": [
      "Transformer",
      "Attention Mechanism",
      "Encoder-Decoder Architecture"
    ],
    "attention_is_all_you_need.pdf_chunk_50": [
      "Transformer",
      "Attention Mechanism",
      "Input-Input Layer",
      "Anaphora Resolution"
    ],
    "attention_is_all_you_need.pdf_chunk_51": [
      "Transformer",
      "Self-Attention",
      "Encoder-Decoder",
      "Attention Heads"
    ],
    "llama.pdf_chunk_0": [
      "LLaMA",
      "GPT-3",
      "Chinchilla",
      "PaLM",
      "Publicly Available Datasets",
      "Benchmarks"
    ],
    "llama.pdf_chunk_1": [
      "Large Language Models (LLMs)",
      "Few-shot learning",
      "LLaMA",
      "Massive corpora of texts",
      "Scaling laws",
      "Hoffmann et al. (2022)",
      "Kaplan et al. (2020)",
      "Brown et al. (2020)",
      "Chowdhery et al. (2022)",
      "Rae et al. (2021)"
    ],
    "llama.pdf_chunk_2": [
      "LLaMA",
      "10 B model",
      "7 B model",
      "200 B tokens",
      "1 T tokens",
      "Inference Budget",
      "Training Compute Budget"
    ],
    "llama.pdf_chunk_3": [
      "LLaMA",
      "GPT-3",
      "Chinchilla",
      "PaLM-540B",
      "Publicly Available Data"
    ],
    "llama.pdf_chunk_4": [
      "LLaMA",
      "Chinchilla",
      "Pa LM",
      "GPT-3",
      "OPT",
      "GPT-Neo X",
      "BLOOM",
      "GLM",
      "Transformer",
      "Standard Benchmarks",
      "Biases and Toxicity"
    ],
    "llama.pdf_chunk_5": [
      "LLaMA",
      "Chinchilla scaling laws",
      "Common Crawl",
      "CCNet pipeline",
      "Pre-training Data"
    ],
    "llama.pdf_chunk_6": [
      "Common Crawl",
      "C4",
      "Deduplication",
      "Language Identification",
      "Quality Filtering",
      "n-gram Language Model",
      "Linear Classifier",
      "Wikipedia"
    ],
    "llama.pdf_chunk_7": [
      "LLaMA",
      "Common Crawl",
      "Github",
      "Wikipedia",
      "Books",
      "Ar Xiv",
      "Stack Exchange",
      "Quality Filtering",
      "Heuristics"
    ],
    "llama.pdf_chunk_8": [
      "Common Crawl",
      "C",
      "Github",
      "Wikipedia",
      "Books",
      "Ar Xiv",
      "Stack Exchange",
      "Gutenberg Project",
      "Books 3",
      "Sampling Proportion",
      "Disk Size",
      "LLaMA"
    ],
    "llama.pdf_chunk_9": [
      "The Pile",
      "arXiv",
      "Stack Exchange",
      "Deduplication"
    ],
    "llama.pdf_chunk_10": [
      "Byte-Pair Encoding (BPE)",
      "LLaMA",
      "28 largest websites",
      "SentencePiece",
      "HTML tags",
      "UTF-8 characters"
    ],
    "llama.pdf_chunk_11": [
      "LLaMA",
      "Transformer",
      "Pre-normalization",
      "Wikipedia",
      "Books",
      "Tokens"
    ],
    "llama.pdf_chunk_12": [
      "Pre-normalization",
      "Swi GLU activation function",
      "Rotary Embeddings",
      "AdamW",
      "LLaMA",
      "GPT-3",
      "PaLM",
      "GPTNeo"
    ],
    "llama.pdf_chunk_13": [
      "LLaMA-7B",
      "LLaMA-13B",
      "LLaMA-33B",
      "LLaMA-65B",
      "AdamW optimizer",
      "Training loss",
      "Cosine learning rate schedule",
      "Weight decay",
      "Gradient clipping",
      "1.4T tokens",
      "1.0T tokens"
    ],
    "llama.pdf_chunk_14": [
      "LLaMA",
      "Causal Multi-Head Attention",
      "xformers",
      "Checkpointing",
      "Backward Pass Optimization",
      "Rabe and Staats (2021)",
      "Dao et al. (2022)"
    ],
    "llama.pdf_chunk_15": [
      "Checkpointing",
      "Transformer",
      "PyTorch"
    ],
    "llama.pdf_chunk_16": [
      "LLaMA",
      "GPT-3",
      "Gopher",
      "Chinchilla",
      "Pa LM",
      "Bool Q",
      "PIQA",
      "SIQA",
      "Hella Swag",
      "Wino",
      "Grande",
      "ARC-e",
      "ARC-c",
      "OBQA",
      "Zero-shot performance",
      "Model and sequence parallelism"
    ],
    "llama.pdf_chunk_17": [
      "LLaMA",
      "GPT-3",
      "Gopher",
      "Chinchilla",
      "1.4 T tokens dataset",
      "tokens/sec/GPU",
      "zero-shot",
      "few-shot"
    ],
    "llama.pdf_chunk_18": [
      "LLaMA",
      "GPT-3",
      "Gopher",
      "Chinchilla",
      "PaLM",
      "OPT",
      "GPT-J",
      "GPT-Neo",
      "OPT-IML",
      "Flan-PaLM",
      "Open Book QA",
      "Bool Q",
      "Likelihood"
    ],
    "llama.pdf_chunk_19": [
      "LLaMA",
      "GPT-3",
      "Gopher",
      "Chinchilla",
      "PaLM",
      "Open Book QA",
      "Bool Q",
      "Natural Questions",
      "PIQA",
      "SIQA",
      "Likelihood Normalization"
    ],
    "llama.pdf_chunk_20": [
      "Hella Swag",
      "Wino Grande",
      "ARC easy and challenge",
      "Open Book QA",
      "LLaMA-65B",
      "Chinchilla-70B",
      "PaLM-540B",
      "LLaMA-13B",
      "GPT-3",
      "Natural Questions",
      "Trivia QA"
    ],
    "llama.pdf_chunk_21": [
      "LLaMA-7B",
      "LLaMA-13B",
      "LLaMA-33B",
      "LLaMA-65B",
      "GPT-3",
      "Chinchilla",
      "Natural Questions",
      "Trivia QA",
      "Exact Match Performance",
      "V100 GPU"
    ],
    "llama.pdf_chunk_22": [
      "LLaMA",
      "Chinchilla",
      "GPT-3",
      "PaLM",
      "RACE",
      "Zero-shot accuracy",
      "Exact match performance"
    ],
    "llama.pdf_chunk_23": [
      "LLaMA-13B",
      "LLaMA-65B",
      "GPT-3",
      "Minerva",
      "PaLM",
      "MATH",
      "GSM 8k",
      "maj 1@k"
    ],
    "llama.pdf_chunk_24": [
      "LLaMA-65B",
      "Minerva-62B",
      "GSM 8k",
      "Human Eval",
      "MBPP",
      "Majority Voting"
    ],
    "llama.pdf_chunk_25": [
      "LLaMA",
      "Minerva",
      "PaLM",
      "LaMDA",
      "MATH",
      "GSM 8k",
      "pass@1"
    ],
    "llama.pdf_chunk_26": [
      "LLaMA",
      "PaLM",
      "LaMDA",
      "PaLM-Coder",
      "pass@1",
      "pass@100",
      "pass@80",
      "Human Eval",
      "MBPP"
    ],
    "llama.pdf_chunk_27": [
      "Pa LM",
      "Pa LM-Coder",
      "LLaMA",
      "Human Eval",
      "MBPP",
      "pass@1",
      "pass@100"
    ],
    "llama.pdf_chunk_28": [
      "LLaMA-65B",
      "Chinchilla-70B",
      "PaLM-540B",
      "Human Eval",
      "MBPP",
      "MMLU",
      "pass@ score",
      "zero-shot",
      "3-shot prompts",
      "5-shot setting",
      "pre-training data"
    ],
    "llama.pdf_chunk_29": [
      "LLaMA",
      "Gopher",
      "Chinchilla",
      "Pa LM",
      "GPT-3",
      "ArXiv",
      "Gutenberg",
      "Books 3",
      "training perplexity",
      "SIQA",
      "Wino Grande"
    ],
    "llama.pdf_chunk_30": [
      "LLaMA",
      "GPT-Neo X",
      "GPT-3",
      "Gopher",
      "Chinchilla",
      "PaLM",
      "MMLU",
      "Wino Grande"
    ],
    "llama.pdf_chunk_31": [
      "LLaMA-65B",
      "LLaMA-I",
      "MMLU",
      "OPT-30B",
      "GLM-120B",
      "PaLM-62B",
      "Chinchilla-70B",
      "OPT-IML-Max-30B",
      "Flan-T5-XXL-11B",
      "Flan-PaLM-62B",
      "Flan-PaLM-cont-62B"
    ],
    "llama.pdf_chunk_32": [
      "LLaMA-I",
      "OPT-IML",
      "Flan-PaLM",
      "GPT code-davinci-002",
      "MMLU",
      "Performance on MMLU",
      "Instruction Fine-tuning",
      "Bias, Toxicity and Misinformation"
    ],
    "llama.pdf_chunk_33": [
      "LLaMA-65 B",
      "Web Data",
      "Toxic Content Production Benchmark",
      "Stereotypes Detection Benchmark",
      "Bias in Language Models",
      "Toxic Content Generation"
    ],
    "llama.pdf_chunk_34": [
      "LLaMA",
      "Real Toxicity Prompts",
      "Accuracy",
      "Trivia QA",
      "Hella Swag",
      "Natural Questions",
      "SIQA",
      "Wino Grande",
      "PIQA",
      "Chinchilla"
    ],
    "llama.pdf_chunk_35": [
      "Real Toxicity Prompts",
      "toxicity score",
      "Perspective API",
      "Chinchilla"
    ],
    "llama.pdf_chunk_36": [
      "LLaMA",
      "Real Toxicity Prompts",
      "Perplexity",
      "Toxicity",
      "Greedy Decoder",
      "Sampling Strategy"
    ],
    "llama.pdf_chunk_37": [
      "LLaMA",
      "Chinchilla",
      "Gopher",
      "Zhang et al. (2022)",
      "Hoffmann et al. (2022)"
    ],
    "llama.pdf_chunk_38": [
      "LLaMA",
      "GPT-3",
      "OPT",
      "Crow S-Pairs",
      "Perplexity",
      "Bias"
    ],
    "llama.pdf_chunk_39": [
      "LLaMA",
      "GPT-3",
      "OPT-175B",
      "Common Crawl",
      "Wino Gender",
      "Perplexity",
      "Bias"
    ],
    "llama.pdf_chunk_40": [
      "Wino Gender dataset",
      "Co-reference resolution",
      "Perplexity",
      "Societal biases",
      "her/her/she",
      "his/him/he",
      "their/them/someone"
    ],
    "llama.pdf_chunk_41": [
      "LLaMA",
      "Wino Gender",
      "co-reference scores",
      "gender bias",
      "her/her/she",
      "his/him/he",
      "their/them/someone"
    ],
    "llama.pdf_chunk_42": [
      "LLaMA-65B",
      "Wino Gender dataset",
      "Truthful QA",
      "Societal Biases",
      "Literal Truth"
    ],
    "llama.pdf_chunk_43": [
      "Literal Truth",
      "Misformation",
      "Adversarial Questions Benchmark"
    ],
    "llama.pdf_chunk_44": [
      "LLaMA",
      "GPT-3",
      "Co-reference resolution accuracy",
      "Truthful QA",
      "Wino Gender"
    ],
    "llama.pdf_chunk_45": [
      "LLaMA",
      "GPT-3",
      "truthful models",
      "informative",
      "carbon footprint",
      "energy consumption estimation",
      "Power Usage Effectiveness (PUE)"
    ],
    "llama.pdf_chunk_46": [
      "BLOOM",
      "OPT",
      "Power Usage Effectiveness (PUE)",
      "Carbon Emission",
      "Carbon Intensity Factor"
    ],
    "llama.pdf_chunk_47": [
      "LLaMA",
      "OPT",
      "BLOOM",
      "t CO2 eq",
      "A100",
      "next token prediction"
    ],
    "llama.pdf_chunk_48": [
      "next token prediction",
      "language modeling",
      "n-gram models",
      "smoothing techniques",
      "neural networks"
    ],
    "llama.pdf_chunk_49": [
      "LLaMA-7",
      "LLaMA-13",
      "LLaMA-33",
      "LLaMA-65",
      "OPT-175B",
      "BLOOM-175B",
      "Carbon Emission",
      "Power Consumption",
      "NVLink"
    ],
    "llama.pdf_chunk_50": [
      "LLaMA",
      "Feed Forward Models",
      "Recurrent Neural Networks",
      "LSTMs",
      "Transformer Networks",
      "Common Crawl",
      "Stupid Backoff",
      "Kneser-Ney Smoothing",
      "Tokens",
      "n-grams"
    ],
    "llama.pdf_chunk_51": [
      "Kneser-Ney smoothing",
      "Common Crawl",
      "5-gram model",
      "One Billion Word benchmark",
      "LSTM",
      "BERT",
      "GPT-2",
      "Megatron-LM",
      "T5",
      "GPT-3",
      "Jurassic-1",
      "Megatron-Turing NLG"
    ],
    "llama.pdf_chunk_52": [
      "GPT-3",
      "Jurassic-1",
      "Megatron-Turing NLG",
      "Gopher",
      "Chinchilla",
      "PaLM",
      "OPT",
      "GLM",
      "Power Laws",
      "Learning Rate Schedule"
    ],
    "llama.pdf_chunk_53": [
      "LLaMA-13B",
      "LLaMA-65B",
      "GPT-3",
      "Chinchilla-70B",
      "PaLM-540B",
      "Publicly available data",
      "Finetuning on instructions",
      "Toxicity",
      "Bias"
    ],
    "llama.pdf_chunk_54": [
      "LLaMA",
      "Finetuning",
      "Scaling",
      "Pretraining Corpora"
    ],
    "llama.pdf_chunk_55": [
      "LLaMA",
      "xformers team",
      "data deduplication",
      "AI infra team",
      "training stability",
      "evaluation team",
      "data collection"
    ],
    "llama.pdf_chunk_56": [
      "LLaMA",
      "GPT-NeoX-20B",
      "Neural Probabilistic Language Model",
      "PIQA"
    ],
    "llama.pdf_chunk_57": [
      "LLaMA",
      "BERT",
      "Machine Translation",
      "Statistical Approach",
      "EMNLP-CoNLL"
    ],
    "llama.pdf_chunk_58": [
      "LLaMA",
      "GPT-3",
      "Common Crawl",
      "One Billion Word Benchmark"
    ],
    "llama.pdf_chunk_59": [
      "LLaMA",
      "GPT-3",
      "Self-supervised learning",
      "C4",
      "Perplexity"
    ],
    "llama.pdf_chunk_60": [
      "LLaMA",
      "GPT-3",
      "BERT",
      "Language Model Evaluation",
      "Code Dataset"
    ],
    "llama.pdf_chunk_61": [
      "LLaMA",
      "Palm",
      "Scaling language modeling"
    ],
    "llama.pdf_chunk_62": [
      "LLaMA",
      "BERT",
      "BoolQ",
      "ARC",
      "Instruction-Finetuning"
    ],
    "llama.pdf_chunk_63": [
      "LLaMA",
      "BERT",
      "Transformer-XL",
      "FlashAttention",
      "ARC",
      "Math Word Problems"
    ],
    "llama.pdf_chunk_64": [
      "LLaMA",
      "BERT",
      "Incoder",
      "The Pile"
    ],
    "llama.pdf_chunk_65": [
      "LLaMA",
      "Few-shot learning",
      "Realtoxicityprompts",
      "Recurrent Neural Networks",
      "Kneser-Ney smoothing"
    ],
    "llama.pdf_chunk_66": [
      "LLaMA",
      "BERT",
      "Math Dataset",
      "Massive Multitask Language Understanding",
      "Long Short-Term Memory (LSTM)"
    ],
    "llama.pdf_chunk_67": [
      "LLaMA",
      "Transformer",
      "TriviaQA",
      "Compute-optimal training",
      "Instruction meta learning"
    ],
    "llama.pdf_chunk_68": [
      "LLaMA",
      "Transformer",
      "Language Modeling",
      "Scaling Laws",
      "Activation Re-computation"
    ],
    "llama.pdf_chunk_69": [
      "LLaMA",
      "Transformer",
      "SentencePiece",
      "Natural Questions",
      "Quantifying Social Biases"
    ],
    "llama.pdf_chunk_70": [
      "LLaMA",
      "RACE",
      "TruthfulQA",
      "Jurassic-1",
      "Quantitative Reasoning"
    ],
    "llama.pdf_chunk_71": [
      "LLaMA",
      "TruthfulQA",
      "Decoupled Weight Decay Regularization",
      "Crow S-pairs",
      "Recurrent Neural Network",
      "Open Book Question Answering Dataset"
    ],
    "llama.pdf_chunk_72": [
      "LLaMA",
      "Crow S-pairs",
      "Codegen",
      "Human Feedback",
      "Self-attention"
    ],
    "llama.pdf_chunk_73": [
      "LLaMA",
      "GPT",
      "Self-attention",
      "Generative Pre-training",
      "Common Crawl",
      "Perplexity"
    ],
    "llama.pdf_chunk_74": [
      "LLaMA",
      "BERT",
      "Fine-tuning",
      "Transformer"
    ],
    "llama.pdf_chunk_75": [
      "LLaMA",
      "Gopher",
      "Transfer Learning",
      "Text-to-Text Transformer"
    ],
    "llama.pdf_chunk_76": [
      "LLaMA",
      "Transformer",
      "Winogrande",
      "SocialIQA",
      "Generalization Error",
      "Transfer Learning",
      "NAACL-HLT 2018"
    ],
    "llama.pdf_chunk_77": [
      "LLaMA",
      "Bloom",
      "Neural Machine Translation",
      "Subword Units",
      "Entropy",
      "Bias in Language Generation"
    ],
    "llama.pdf_chunk_78": [
      "LLaMA",
      "Megatron-LM",
      "Megatron-Turing NLG 530B",
      "Roformer",
      "Model Parallelism",
      "DeepSpeed"
    ],
    "llama.pdf_chunk_79": [
      "LLaMA",
      "Megatron-Turing NLG 530B",
      "Roformer"
    ],
    "llama.pdf_chunk_80": [
      "LLaMA",
      "Lamda",
      "Attention Mechanism",
      "GPT-J-6B"
    ],
    "llama.pdf_chunk_81": [
      "LLaMA",
      "Transformer"
    ],
    "llama.pdf_chunk_82": [
      "LLaMA",
      "Self-consistency",
      "Large Language Models",
      "CCNet",
      "Sustainable AI",
      "Hellaswag"
    ],
    "llama.pdf_chunk_83": [
      "LLaMA",
      "GLM-130B",
      "OPT",
      "Hellaswag",
      "Root Mean Square Layer Normalization"
    ],
    "llama.pdf_chunk_84": [
      "LLaMA",
      "Natural Questions",
      "Trivia QA",
      "GPT-3",
      "PaLM",
      "Exact Match",
      "Greedy Decoding"
    ],
    "llama.pdf_chunk_85": [
      "Natural Questions",
      "Trivia QA",
      "LLaMA",
      "1-shot setting"
    ],
    "llama.pdf_chunk_86": [
      "LLaMA",
      "GPT-3",
      "Gopher",
      "Chinchilla",
      "MMLU",
      "STEM",
      "Other",
      "Abstract Algebra",
      "Anatomy",
      "Astronomy",
      "Business Ethics",
      "Clinical Knowledge",
      "College Biology",
      "College Chemistry",
      "College Computer Science",
      "College Mathematics",
      "College Medicine",
      "College Physics",
      "Computer Security",
      "Conceptual Physics",
      "Econometrics"
    ],
    "llama.pdf_chunk_87": [
      "LLaMA",
      "STEM Scores",
      "Social Science Scores",
      "Humanities Scores",
      "High School Subjects"
    ],
    "llama.pdf_chunk_88": [
      "LLaMA",
      "Performance Scores",
      "High School Subjects",
      "Social Science",
      "STEM",
      "Humanities",
      "Other"
    ],
    "llama.pdf_chunk_89": [
      "LLaMA",
      "Performance Scores",
      "Machine Learning",
      "Humanities",
      "STEM",
      "Other"
    ],
    "llama.pdf_chunk_90": [
      "LLaMA",
      "Performance Scores",
      "Humanities",
      "Social Science",
      "STEM",
      "Professional Fields",
      "Others"
    ],
    "llama.pdf_chunk_91": [
      "LLaMA",
      "MMLU",
      "5-shot results",
      "STEM",
      "Social Science",
      "Others",
      "All"
    ],
    "llama.pdf_chunk_92": [
      "LLaMA-65B",
      "Fibonacci sequence",
      "Instruction finetuning",
      "Leonardo of Pisa",
      "Liber abaci"
    ],
    "llama.pdf_chunk_93": [
      "LLaMA",
      "Foundation Language Models"
    ],
    "llama.pdf_chunk_94": [
      "LLaMA"
    ],
    "llama.pdf_chunk_95": [
      "Deep Learning",
      "Yann Le Cun",
      "Deep Learning Album"
    ],
    "llama.pdf_chunk_96": [
      "LLaMA",
      "Deep Learning",
      "Le Cun"
    ],
    "llama.pdf_chunk_97": [
      "LLaMA",
      "AI",
      "Training",
      "Perception",
      "Network"
    ],
    "llama.pdf_chunk_98": [
      "LLaMA"
    ],
    "llama.pdf_chunk_99": [],
    "llama.pdf_chunk_100": [
      "LLaMA",
      "Telegraph",
      "Social Media",
      "Geek",
      "Character Limit"
    ],
    "llama.pdf_chunk_101": [
      "LLaMA-65B",
      "Instruction Dataset",
      "HTTP Request",
      "GET Request"
    ],
    "llama.pdf_chunk_102": [
      "LLaMA",
      "Fetch API",
      "Regular Expressions"
    ],
    "llama.pdf_chunk_103": [
      "LLaMA",
      "Regular Expressions",
      "Python"
    ],
    "llama.pdf_chunk_104": [
      "Sicilian Defense",
      "French Defense",
      "Caro-Kann Defense",
      "Ruy Lopez",
      "Italian Game",
      "Scotch Game"
    ],
    "llama.pdf_chunk_105": [
      "Sicilian Defense",
      "French Defense",
      "Caro-Kann Defense",
      "Ruy Lopez",
      "Italian Game",
      "Scotch Game",
      "Open Games"
    ],
    "llama.pdf_chunk_106": [
      "LLaMA",
      "Scotch Game",
      "Italian Game"
    ],
    "llama.pdf_chunk_107": [
      "LLaMA"
    ],
    "llama.pdf_chunk_108": [
      "LLaMA"
    ],
    "llama.pdf_chunk_109": [
      "LLaMA"
    ],
    "llama.pdf_chunk_110": [
      "LLaMA"
    ],
    "llama.pdf_chunk_111": [
      "LLaMA"
    ],
    "llama.pdf_chunk_112": [
      "LLaMA"
    ],
    "llama.pdf_chunk_113": [
      "Language Models",
      "Ethical Implications",
      "Legal and Policy Constraints"
    ],
    "llama.pdf_chunk_114": [],
    "llama.pdf_chunk_115": [
      "LLaMA",
      "Theory of Relativity",
      "Law of Photons"
    ],
    "llama.pdf_chunk_116": [
      "Theory of Relativity",
      "Law of Photons",
      "Quantum Mechanics",
      "E = mc^2"
    ],
    "llama.pdf_chunk_117": [
      "LLaMA"
    ],
    "llama.pdf_chunk_118": []
  },
  "extraction_timestamp": "2025-11-11T16:52:48.185076"
}